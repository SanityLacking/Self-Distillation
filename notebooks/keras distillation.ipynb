{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "import tensorflow.keras.layers as layers\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "import os\n",
    "sys.path.append(\"..\") # Adds higher directory to python modules path.\n",
    "import branchingdnn as branching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Distiller(keras.Model):\n",
    "    def __init__(self, student, teacher):\n",
    "        super(Distiller, self).__init__()\n",
    "        self.teacher = teacher\n",
    "        self.student = student\n",
    "\n",
    "    def compile(\n",
    "        self,\n",
    "        optimizer,\n",
    "        metrics,\n",
    "        student_loss_fn,\n",
    "        distillation_loss_fn,\n",
    "        alpha=0.1,\n",
    "        temperature=3,\n",
    "    ):\n",
    "        \"\"\" Configure the distiller.\n",
    "\n",
    "        Args:\n",
    "            optimizer: Keras optimizer for the student weights\n",
    "            metrics: Keras metrics for evaluation\n",
    "            student_loss_fn: Loss function of difference between student\n",
    "                predictions and ground-truth\n",
    "            distillation_loss_fn: Loss function of difference between soft\n",
    "                student predictions and soft teacher predictions\n",
    "            alpha: weight to student_loss_fn and 1-alpha to distillation_loss_fn\n",
    "            temperature: Temperature for softening probability distributions.\n",
    "                Larger temperature gives softer distributions.\n",
    "        \"\"\"\n",
    "        super(Distiller, self).compile(optimizer=optimizer, metrics=metrics)\n",
    "        self.student_loss_fn = student_loss_fn\n",
    "        self.distillation_loss_fn = distillation_loss_fn\n",
    "        self.alpha = alpha\n",
    "        self.temperature = temperature\n",
    "\n",
    "    def train_step(self, data):\n",
    "        # Unpack data\n",
    "        x, y = data\n",
    "\n",
    "        # Forward pass of teacher\n",
    "        teacher_predictions = self.teacher(x, training=False)\n",
    "\n",
    "        with tf.GradientTape() as tape:\n",
    "            # Forward pass of student\n",
    "            student_predictions = self.student(x, training=True)\n",
    "\n",
    "            # Compute losses\n",
    "            student_loss = self.student_loss_fn(y, student_predictions)\n",
    "            distillation_loss = self.distillation_loss_fn(\n",
    "                tf.nn.softmax(teacher_predictions / self.temperature, axis=1),\n",
    "                tf.nn.softmax(student_predictions / self.temperature, axis=1),\n",
    "            )\n",
    "            loss = self.alpha * student_loss + (1 - self.alpha) * distillation_loss *100\n",
    "\n",
    "        # Compute gradients\n",
    "        trainable_vars = self.student.trainable_variables\n",
    "        gradients = tape.gradient(loss, trainable_vars)\n",
    "\n",
    "        # Update weights\n",
    "        self.optimizer.apply_gradients(zip(gradients, trainable_vars))\n",
    "\n",
    "        # Update the metrics configured in `compile()`.\n",
    "        self.compiled_metrics.update_state(y, student_predictions)\n",
    "\n",
    "        # Return a dict of performance\n",
    "        results = {m.name: m.result() for m in self.metrics}\n",
    "        results.update(\n",
    "            {\"student_loss\": student_loss, \"distillation_loss\": distillation_loss}\n",
    "        )\n",
    "        return results\n",
    "\n",
    "    def test_step(self, data):\n",
    "        # Unpack the data\n",
    "        x, y = data\n",
    "\n",
    "        # Compute predictions\n",
    "        y_prediction = self.student(x, training=False)\n",
    "\n",
    "        # Calculate the loss\n",
    "        student_loss = self.student_loss_fn(y, y_prediction)\n",
    "\n",
    "        # Update the metrics.\n",
    "        self.compiled_metrics.update_state(y, y_prediction)\n",
    "\n",
    "        # Return a dict of performance\n",
    "        results = {m.name: m.result() for m in self.metrics}\n",
    "        results.update({\"student_loss\": student_loss})\n",
    "        return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the teacher\n",
    "teacher = keras.Sequential(\n",
    "    [\n",
    "        keras.Input(shape=(28, 28, 1)),\n",
    "        layers.Conv2D(256, (3, 3), strides=(2, 2), padding=\"same\"),\n",
    "        layers.LeakyReLU(alpha=0.2),\n",
    "        layers.MaxPooling2D(pool_size=(2, 2), strides=(1, 1), padding=\"same\"),\n",
    "        layers.Conv2D(512, (3, 3), strides=(2, 2), padding=\"same\"),\n",
    "        layers.Flatten(),\n",
    "        layers.Dense(10),\n",
    "    ],\n",
    "    name=\"teacher\",\n",
    ")\n",
    "\n",
    "# Create the student\n",
    "student = keras.Sequential(\n",
    "    [\n",
    "        keras.Input(shape=(28, 28, 1)),\n",
    "        layers.Conv2D(16, (3, 3), strides=(2, 2), padding=\"same\"),\n",
    "        layers.LeakyReLU(alpha=0.2),\n",
    "        layers.MaxPooling2D(pool_size=(2, 2), strides=(1, 1), padding=\"same\"),\n",
    "        layers.Conv2D(32, (3, 3), strides=(2, 2), padding=\"same\"),\n",
    "        layers.Flatten(),\n",
    "        layers.Dense(10),\n",
    "    ],\n",
    "    name=\"student\",\n",
    ")\n",
    "\n",
    "# Clone student for later comparison\n",
    "student_scratch = keras.models.clone_model(student)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the train and test dataset.\n",
    "batch_size = 64\n",
    "(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()\n",
    "\n",
    "# Normalize data\n",
    "x_train = x_train.astype(\"float32\") / 255.0\n",
    "x_train = np.reshape(x_train, (-1, 28, 28, 1))\n",
    "\n",
    "x_test = x_test.astype(\"float32\") / 255.0\n",
    "x_test = np.reshape(x_test, (-1, 28, 28, 1))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "augment Dataset\n",
      "targetsis : False\n",
      "trainSize 45000\n",
      "testSize 10000\n"
     ]
    }
   ],
   "source": [
    "dataset = branching.dataset.prepare.dataset(tf.keras.datasets.cifar10.load_data(),32,5000,22500,(227,227), include_targets=False, categorical=True)\n",
    "train_ds, test_ds, validation_ds = dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "312/312 [==============================] - 8s 13ms/step - loss: 0.6905 - accuracy: 0.7940\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.6905297040939331, 0.7939703464508057]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_teacher = tf.keras.models.load_model(\"models/alexNetv6_logits_teacher.hdf5\")\n",
    "model_teacher.evaluate(test_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "1875/1875 [==============================] - 13s 7ms/step - loss: 0.2488 - sparse_categorical_accuracy: 0.9225\n",
      "Epoch 2/5\n",
      "1875/1875 [==============================] - 13s 7ms/step - loss: 0.0855 - sparse_categorical_accuracy: 0.9742\n",
      "Epoch 3/5\n",
      "1875/1875 [==============================] - 13s 7ms/step - loss: 0.0799 - sparse_categorical_accuracy: 0.9757\n",
      "Epoch 4/5\n",
      "1875/1875 [==============================] - 13s 7ms/step - loss: 0.0616 - sparse_categorical_accuracy: 0.9819\n",
      "Epoch 5/5\n",
      "1875/1875 [==============================] - 13s 7ms/step - loss: 0.0651 - sparse_categorical_accuracy: 0.9811\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.1105 - sparse_categorical_accuracy: 0.9715\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.11047188937664032, 0.9714999794960022]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# # Train teacher as usual\n",
    "# teacher.compile(\n",
    "#     optimizer=keras.optimizers.Adam(),\n",
    "#     loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "#     metrics=[keras.metrics.SparseCategoricalAccuracy()],\n",
    "# )\n",
    "\n",
    "# # Train and evaluate teacher on data.\n",
    "# teacher.fit(x_train, y_train, epochs=5)\n",
    "# teacher.evaluate(x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 1s 2ms/step - sparse_categorical_accuracy: 0.9799 - student_loss: 0.0693\n",
      "results:  [0.9799000024795532, 0.0016497487667948008]\n",
      "313/313 [==============================] - 1s 2ms/step - sparse_categorical_accuracy: 0.9763 - student_loss: 0.0824\n",
      "results:  [0.9763000011444092, 0.0008576142136007547]\n",
      "313/313 [==============================] - 1s 2ms/step - sparse_categorical_accuracy: 0.9764 - student_loss: 0.0852\n",
      "results:  [0.9764000177383423, 0.002193244406953454]\n",
      "313/313 [==============================] - 1s 2ms/step - sparse_categorical_accuracy: 0.9770 - student_loss: 0.0826\n",
      "results:  [0.9769999980926514, 9.805313311517239e-05]\n",
      "313/313 [==============================] - 1s 2ms/step - sparse_categorical_accuracy: 0.9754 - student_loss: 0.0827\n",
      "results:  [0.9753999710083008, 0.0012869059573858976]\n",
      "313/313 [==============================] - 1s 2ms/step - sparse_categorical_accuracy: 0.9770 - student_loss: 0.0774\n",
      "results:  [0.9769999980926514, 3.976887819590047e-05]\n"
     ]
    }
   ],
   "source": [
    "# Initialize and compile distiller\n",
    "for i in range(6):\n",
    "    student = keras.Sequential(\n",
    "        [\n",
    "            keras.Input(shape=(227, 227, 1)),\n",
    "            layers.Conv2D(16, (3, 3), strides=(2, 2), padding=\"same\"),\n",
    "            layers.LeakyReLU(alpha=0.2),\n",
    "            layers.MaxPooling2D(pool_size=(2, 2), strides=(1, 1), padding=\"same\"),\n",
    "            layers.Conv2D(32, (3, 3), strides=(2, 2), padding=\"same\"),\n",
    "            layers.Flatten(),\n",
    "            layers.Dense(10),\n",
    "        ],\n",
    "        name=\"student\",\n",
    "    )\n",
    "\n",
    "#     print(\"alpha: \",(i+1)/10)\n",
    "    distiller = Distiller(student=student, teacher=model_teacher)\n",
    "    distiller.compile(\n",
    "        optimizer=keras.optimizers.Adam(),\n",
    "        metrics=[keras.metrics.SparseCategoricalAccuracy()],\n",
    "        student_loss_fn=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "        distillation_loss_fn=keras.losses.KLDivergence(),\n",
    "        alpha=0.3,\n",
    "        temperature=10,\n",
    "    )\n",
    "    # Distill teacher to student\n",
    "    distiller.fit(x_train, y_train, epochs=3, verbose=0)\n",
    "    # Evaluate student on test dataset\n",
    "    print(\"results: \",distiller.evaluate(x_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the student model with the teacher model supplying additional loss signals.\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/6\n",
      "1406/1406 [==============================] - 101s 61ms/step - categorical_accuracy: 0.2491 - student_loss: 2.5345 - distillation_loss: 4.0446e-04\n",
      "Epoch 2/6\n",
      "1406/1406 [==============================] - 100s 61ms/step - categorical_accuracy: 0.3771 - student_loss: 1.7384 - distillation_loss: 3.4249e-04\n",
      "Epoch 3/6\n",
      "1406/1406 [==============================] - 100s 61ms/step - categorical_accuracy: 0.4553 - student_loss: 1.5115 - distillation_loss: 2.9602e-04\n",
      "Epoch 4/6\n",
      "1406/1406 [==============================] - 102s 62ms/step - categorical_accuracy: 0.5425 - student_loss: 1.3019 - distillation_loss: 2.5275e-04\n",
      "Epoch 5/6\n",
      "1406/1406 [==============================] - 101s 62ms/step - categorical_accuracy: 0.6116 - student_loss: 1.1355 - distillation_loss: 2.1807e-04\n",
      "Epoch 6/6\n",
      "1406/1406 [==============================] - 101s 62ms/step - categorical_accuracy: 0.6629 - student_loss: 0.9868 - distillation_loss: 1.8802e-04\n",
      "312/312 [==============================] - 4s 13ms/step - categorical_accuracy: 0.5899 - student_loss: 1.1856\n",
      "results:  [0.5899438858032227, 1.2809326648712158]\n"
     ]
    }
   ],
   "source": [
    "# Initialize and compile distiller\n",
    "# for i in range(3):\n",
    "\n",
    "inputs = keras.Input(shape=(227,227,3))\n",
    "x = keras.layers.Conv2D(filters=96, kernel_size=(11,11), strides=(4,4), activation='relu', input_shape=(227,227,3))(inputs)\n",
    "x = keras.layers.BatchNormalization()(x)\n",
    "x = keras.layers.MaxPool2D(pool_size=(3,3), strides=(2,2))(x)\n",
    "x = keras.layers.Conv2D(filters=256, kernel_size=(5,5), strides=(1,1), activation='relu', padding=\"same\")(x)\n",
    "x = keras.layers.BatchNormalization()(x)\n",
    "x = keras.layers.MaxPool2D(pool_size=(3,3), strides=(2,2))(x)\n",
    "x = keras.layers.Conv2D(filters=384, kernel_size=(3,3), strides=(1,1), activation='relu', padding=\"same\")(x)\n",
    "x = keras.layers.BatchNormalization()(x)\n",
    "x = keras.layers.Conv2D(filters=384, kernel_size=(1,1), strides=(1,1), activation='relu', padding=\"same\")(x)\n",
    "x = keras.layers.BatchNormalization()(x)\n",
    "x = keras.layers.Conv2D(filters=256, kernel_size=(1,1), strides=(1,1), activation='relu', padding=\"same\")(x)\n",
    "x = keras.layers.BatchNormalization()(x)\n",
    "x = keras.layers.MaxPool2D(pool_size=(3,3), strides=(2,2))(x)\n",
    "x = keras.layers.Flatten()(x)\n",
    "x = keras.layers.Dense(4096, activation='relu')(x)\n",
    "x = keras.layers.Dropout(0.5)(x)\n",
    "x = keras.layers.Dense(4096, activation='relu')(x)\n",
    "x = keras.layers.Dropout(0.5)(x)\n",
    "x = keras.layers.Dense(10, activation='softmax')(x)\n",
    "student = keras.Model(inputs=(inputs), outputs=[x], name=\"alexnet\")\n",
    "\n",
    "# student = keras.Sequential(\n",
    "#         [\n",
    "#             keras.Input(shape=(227, 227, 3)),\n",
    "#             layers.Conv2D(16, (3, 3), strides=(2, 2), padding=\"same\"),\n",
    "#             layers.LeakyReLU(alpha=0.2),\n",
    "#             layers.MaxPooling2D(pool_size=(2, 2), strides=(1, 1), padding=\"same\"),\n",
    "#             layers.Conv2D(32, (3, 3), strides=(2, 2), padding=\"same\"),\n",
    "#             layers.Flatten(),\n",
    "#             layers.Dense(10),\n",
    "#         ],\n",
    "#         name=\"student\",\n",
    "#     )\n",
    "\n",
    "distiller = Distiller(student=student, teacher=model_teacher)\n",
    "distiller.compile(\n",
    "    optimizer=keras.optimizers.Adam(),\n",
    "    metrics=[keras.metrics.CategoricalAccuracy()],\n",
    "    student_loss_fn=keras.losses.CategoricalCrossentropy(from_logits=True),\n",
    "    distillation_loss_fn=keras.losses.KLDivergence(),\n",
    "    alpha=0.1,\n",
    "    temperature=10,\n",
    ")\n",
    "# Distill teacher to student\n",
    "distiller.fit(train_ds, epochs=6,verbose=1)\n",
    "# Evaluate student on test dataset\n",
    "print(\"results: \",distiller.evaluate(test_ds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1\n",
      "Epoch 1/12\n"
     ]
    },
    {
     "ename": "ResourceExhaustedError",
     "evalue": " OOM when allocating tensor with shape[69984,4096] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n\t [[node gradient_tape/alexnet/dense_3/MatMul_1 (defined at <ipython-input-2-51b0ad79b8c8>:56) ]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n [Op:__inference_train_function_138672]\n\nErrors may have originated from an input operation.\nInput Source operations connected to node gradient_tape/alexnet/dense_3/MatMul_1:\n alexnet/flatten_1/Reshape (defined at <ipython-input-2-51b0ad79b8c8>:44)\n\nFunction call stack:\ntrain_function\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-28a198f75019>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     48\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdistiller\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0malpha\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     49\u001b[0m \u001b[1;31m# Distill teacher to student\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 50\u001b[1;33m \u001b[0mdistiller\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_ds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m12\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     51\u001b[0m \u001b[1;31m# Evaluate student on test dataset\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"results: \"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdistiller\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_ds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\sanity\\appdata\\local\\conda\\conda\\envs\\py36\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1098\u001b[0m                 _r=1):\n\u001b[0;32m   1099\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1100\u001b[1;33m               \u001b[0mtmp_logs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1101\u001b[0m               \u001b[1;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1102\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\sanity\\appdata\\local\\conda\\conda\\envs\\py36\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    826\u001b[0m     \u001b[0mtracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    827\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mtrace\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTrace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mtm\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 828\u001b[1;33m       \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    829\u001b[0m       \u001b[0mcompiler\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"xla\"\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_experimental_compile\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;34m\"nonXla\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    830\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\sanity\\appdata\\local\\conda\\conda\\envs\\py36\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    886\u001b[0m         \u001b[1;31m# Lifting succeeded, so variables are initialized and we can run the\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    887\u001b[0m         \u001b[1;31m# stateless function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 888\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    889\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    890\u001b[0m       \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfiltered_flat_args\u001b[0m \u001b[1;33m=\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\sanity\\appdata\\local\\conda\\conda\\envs\\py36\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2941\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[0;32m   2942\u001b[0m     return graph_function._call_flat(\n\u001b[1;32m-> 2943\u001b[1;33m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0m\u001b[0;32m   2944\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2945\u001b[0m   \u001b[1;33m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\sanity\\appdata\\local\\conda\\conda\\envs\\py36\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1917\u001b[0m       \u001b[1;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1918\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[1;32m-> 1919\u001b[1;33m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[0;32m   1920\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[0;32m   1921\u001b[0m         \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\sanity\\appdata\\local\\conda\\conda\\envs\\py36\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    558\u001b[0m               \u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    559\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 560\u001b[1;33m               ctx=ctx)\n\u001b[0m\u001b[0;32m    561\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    562\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[1;32mc:\\users\\sanity\\appdata\\local\\conda\\conda\\envs\\py36\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[1;32m---> 60\u001b[1;33m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[0;32m     61\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mResourceExhaustedError\u001b[0m:  OOM when allocating tensor with shape[69984,4096] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n\t [[node gradient_tape/alexnet/dense_3/MatMul_1 (defined at <ipython-input-2-51b0ad79b8c8>:56) ]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n [Op:__inference_train_function_138672]\n\nErrors may have originated from an input operation.\nInput Source operations connected to node gradient_tape/alexnet/dense_3/MatMul_1:\n alexnet/flatten_1/Reshape (defined at <ipython-input-2-51b0ad79b8c8>:44)\n\nFunction call stack:\ntrain_function\n"
     ]
    }
   ],
   "source": [
    "# Initialize and compile distiller\n",
    "# for i in range(3):\n",
    "\n",
    "inputs = keras.Input(shape=(227,227,3))\n",
    "x = keras.layers.Conv2D(filters=96, kernel_size=(11,11), strides=(4,4), activation='relu', input_shape=(227,227,3))(inputs)\n",
    "x = keras.layers.BatchNormalization()(x)\n",
    "x = keras.layers.MaxPool2D(pool_size=(3,3), strides=(2,2))(x)\n",
    "# x = keras.layers.Conv2D(filters=256, kernel_size=(5,5), strides=(1,1), activation='relu', padding=\"same\")(x)\n",
    "# x = keras.layers.BatchNormalization()(x)\n",
    "# x = keras.layers.MaxPool2D(pool_size=(3,3), strides=(2,2))(x)\n",
    "# x = keras.layers.Conv2D(filters=384, kernel_size=(3,3), strides=(1,1), activation='relu', padding=\"same\")(x)\n",
    "# x = keras.layers.BatchNormalization()(x)\n",
    "# x = keras.layers.Conv2D(filters=384, kernel_size=(1,1), strides=(1,1), activation='relu', padding=\"same\")(x)\n",
    "# x = keras.layers.BatchNormalization()(x)\n",
    "# x = keras.layers.Conv2D(filters=256, kernel_size=(1,1), strides=(1,1), activation='relu', padding=\"same\")(x)\n",
    "# x = keras.layers.BatchNormalization()(x)\n",
    "# x = keras.layers.MaxPool2D(pool_size=(3,3), strides=(2,2))(x)\n",
    "x = keras.layers.Flatten()(x)\n",
    "# x = keras.layers.Dense(124, activation='relu')(x)\n",
    "# x = keras.layers.Dense(64, activation='relu')(x)\n",
    "x = keras.layers.Dense(4096, activation='relu')(x)\n",
    "x = keras.layers.Dense(4096, activation='relu')(x)\n",
    "x = keras.layers.Dense(10, activation='softmax')(x)\n",
    "student = keras.Model(inputs=(inputs), outputs=[x], name=\"alexnet\")\n",
    "\n",
    "# student = keras.Sequential(\n",
    "#         [\n",
    "#             keras.Input(shape=(227, 227, 3)),\n",
    "#             layers.Conv2D(16, (3, 3), strides=(2, 2), padding=\"same\"),\n",
    "#             layers.LeakyReLU(alpha=0.2),\n",
    "#             layers.MaxPooling2D(pool_size=(2, 2), strides=(1, 1), padding=\"same\"),\n",
    "#             layers.Conv2D(32, (3, 3), strides=(2, 2), padding=\"same\"),\n",
    "#             layers.Flatten(),\n",
    "#             layers.Dense(10),\n",
    "#         ],\n",
    "#         name=\"student\",\n",
    "#     )\n",
    "\n",
    "distiller = Distiller(student=student, teacher=model_teacher)\n",
    "distiller.compile(\n",
    "    optimizer=keras.optimizers.Adam(),\n",
    "    metrics=[keras.metrics.CategoricalAccuracy()],\n",
    "    student_loss_fn=keras.losses.CategoricalCrossentropy(from_logits=True),\n",
    "    distillation_loss_fn=keras.losses.KLDivergence(),\n",
    "    alpha=.1,\n",
    "    temperature=10,\n",
    ")\n",
    "print(distiller.alpha)\n",
    "# Distill teacher to student\n",
    "distiller.fit(train_ds, epochs=12,verbose=1)\n",
    "# Evaluate student on test dataset\n",
    "print(\"results: \",distiller.evaluate(test_ds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Results from running:  <br>\n",
    "1 [0.5782251358032227, 1.4982712268829346] <br>\n",
    "2 [0.5210336446762085, 1.799507737159729] <br>\n",
    "3 [0.5548878312110901, 1.3219722509384155] <br>\n",
    "4 [0.5767227411270142, 1.3181259632110596] <br>\n",
    "<br>\n",
    "5 [0.5558894276618958, 1.3667815923690796] <br>\n",
    "6 [0.5221354365348816, 1.5140984058380127] <br>\n",
    "7 [0.5203325152397156, 1.4471924304962158] <br>\n",
    "8 [0.5422676205635071, 1.8327810764312744] <br>\n",
    "\n",
    "\n",
    "12 epochs <br>\n",
    "0.59,\n",
    "0.58, 0.588"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now train the same student model without the teacher\n",
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/6\n",
      "1406/1406 [==============================] - 87s 51ms/step - loss: 4.5256 - categorical_accuracy: 0.2705\n",
      "Epoch 2/6\n",
      "1406/1406 [==============================] - 86s 51ms/step - loss: 1.7853 - categorical_accuracy: 0.3737\n",
      "Epoch 3/6\n",
      "1406/1406 [==============================] - 86s 51ms/step - loss: 1.5966 - categorical_accuracy: 0.4447\n",
      "Epoch 4/6\n",
      "1406/1406 [==============================] - 87s 52ms/step - loss: 1.4099 - categorical_accuracy: 0.5100\n",
      "Epoch 5/6\n",
      "1406/1406 [==============================] - 87s 52ms/step - loss: 1.2739 - categorical_accuracy: 0.5648\n",
      "Epoch 6/6\n",
      "1406/1406 [==============================] - 87s 52ms/step - loss: 1.0581 - categorical_accuracy: 0.6434\n",
      "312/312 [==============================] - 4s 13ms/step - loss: 1.0825 - categorical_accuracy: 0.6264\n",
      "results:  [1.0825001001358032, 0.6264022588729858]\n"
     ]
    }
   ],
   "source": [
    "# Train student as doen usually\n",
    "# for i in range(6):\n",
    "inputs = keras.Input(shape=(227,227,3))\n",
    "x = keras.layers.Conv2D(filters=96, kernel_size=(11,11), strides=(4,4), activation='relu', input_shape=(227,227,3))(inputs)\n",
    "x = keras.layers.BatchNormalization()(x)\n",
    "x = keras.layers.MaxPool2D(pool_size=(3,3), strides=(2,2))(x)\n",
    "x = keras.layers.Conv2D(filters=256, kernel_size=(5,5), strides=(1,1), activation='relu', padding=\"same\")(x)\n",
    "x = keras.layers.BatchNormalization()(x)\n",
    "x = keras.layers.MaxPool2D(pool_size=(3,3), strides=(2,2))(x)\n",
    "x = keras.layers.Conv2D(filters=384, kernel_size=(3,3), strides=(1,1), activation='relu', padding=\"same\")(x)\n",
    "x = keras.layers.BatchNormalization()(x)\n",
    "x = keras.layers.Conv2D(filters=384, kernel_size=(1,1), strides=(1,1), activation='relu', padding=\"same\")(x)\n",
    "x = keras.layers.BatchNormalization()(x)\n",
    "x = keras.layers.Conv2D(filters=256, kernel_size=(1,1), strides=(1,1), activation='relu', padding=\"same\")(x)\n",
    "x = keras.layers.BatchNormalization()(x)\n",
    "x = keras.layers.MaxPool2D(pool_size=(3,3), strides=(2,2))(x)\n",
    "x = keras.layers.Flatten()(x)\n",
    "x = keras.layers.Dense(4096, activation='relu')(x)\n",
    "x = keras.layers.Dropout(0.5)(x)\n",
    "x = keras.layers.Dense(4096, activation='relu')(x)\n",
    "x = keras.layers.Dropout(0.5)(x)\n",
    "x = keras.layers.Dense(10, activation='softmax')(x)\n",
    "student_scratch = keras.Model(inputs=(inputs), outputs=[x], name=\"alexnet\")\n",
    "\n",
    "# student_scratch = keras.Sequential(\n",
    "#     [\n",
    "#         keras.Input(shape=(227, 227, 3)),\n",
    "#         layers.Conv2D(16, (3, 3), strides=(2, 2), padding=\"same\"),\n",
    "#         layers.LeakyReLU(alpha=0.2),\n",
    "#         layers.MaxPooling2D(pool_size=(2, 2), strides=(1, 1), padding=\"same\"),\n",
    "#         layers.Conv2D(32, (3, 3), strides=(2, 2), padding=\"same\"),\n",
    "#         layers.Flatten(),\n",
    "#         layers.Dense(10),\n",
    "#     ],\n",
    "#     name=\"student\",\n",
    "# )\n",
    "student_scratch.compile(\n",
    "    optimizer=keras.optimizers.Adam(),\n",
    "    loss=keras.losses.CategoricalCrossentropy(from_logits=True),\n",
    "    metrics=[keras.metrics.CategoricalAccuracy()],\n",
    ")\n",
    "\n",
    "# Train and evaluate student trained from scratch.\n",
    "student_scratch.fit(train_ds, epochs=6,verbose=1)\n",
    "#     student_scratch.evaluate(x_test, y_test)\n",
    "print(\"results: \",student_scratch.evaluate(test_ds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Results from running: <br>\n",
    "1 [0.5782251358032227, 1.4982712268829346] <br>\n",
    "2 [1.342477798461914, 0.5252403616905212] <br>\n",
    "3 0.5658053159713745 <br>\n",
    "4 0.5724158883094788 <br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "312/312 [==============================] - 4s 13ms/step - loss: 1.6139 - categorical_accuracy: 0.3922\n",
      "results:  [1.6138664484024048, 0.39222756028175354]\n"
     ]
    }
   ],
   "source": [
    "print(\"results: \",student_scratch.evaluate(test_ds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "1875/1875 [==============================] - 5s 3ms/step - loss: 0.4604 - sparse_categorical_accuracy: 0.8590\n",
      "Epoch 2/3\n",
      "1875/1875 [==============================] - 5s 3ms/step - loss: 0.1038 - sparse_categorical_accuracy: 0.9688\n",
      "Epoch 3/3\n",
      "1875/1875 [==============================] - 5s 3ms/step - loss: 0.0825 - sparse_categorical_accuracy: 0.9750\n",
      "313/313 [==============================] - 1s 2ms/step - loss: 0.0666 - sparse_categorical_accuracy: 0.9784\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.06662680953741074, 0.9783999919891357]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "student_2 = keras.Sequential(\n",
    "    [\n",
    "        keras.Input(shape=(28, 28, 1)),\n",
    "        layers.Conv2D(16, (3, 3), strides=(2, 2), padding=\"same\"),\n",
    "        layers.LeakyReLU(alpha=0.2),\n",
    "        layers.MaxPooling2D(pool_size=(2, 2), strides=(1, 1), padding=\"same\"),\n",
    "        layers.Conv2D(32, (3, 3), strides=(2, 2), padding=\"same\"),\n",
    "        layers.Flatten(),\n",
    "        layers.Dense(10),\n",
    "    ],\n",
    "    name=\"student\",\n",
    ")\n",
    "student_2.compile(\n",
    "    optimizer=keras.optimizers.Adam(),\n",
    "    loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "    metrics=[keras.metrics.SparseCategoricalAccuracy()],\n",
    ")\n",
    "# Train and evaluate student trained from scratch.\n",
    "student_2.fit(x_train, y_train, epochs=3)\n",
    "student_2.evaluate(x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
