{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Self Distillation for Keras\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# os.environ['CUDA_VISIBLE_DEVICES'] = '-1'\n",
    "import tensorflow as tf\n",
    "os.environ['TF_DETERMINISTIC_OPS'] = '1'\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "import os\n",
    "import keras\n",
    "\n",
    "sys.path.append(\"..\") # Adds higher directory to python modules path.\n",
    "# import brevis\n",
    "# from brevis import branches\n",
    "# from brevis import evaluate\n",
    "# dataset = branching.dataset.prepare.dataset(tf.keras.datasets.cifar10.load_data(),64,5000,22500,(227,227),include_targets=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BranchModel(tf.keras.Model):\n",
    "    '''\n",
    "    Branched model sub-class. \n",
    "    Acts as a drop in replacement keras model class, with the additional functionality of adding branches to the model.\n",
    "            \n",
    "    '''\n",
    "    def __init__(self, inputs=None, outputs=None, name=\"\", model=None, transfer=True,custom_objects={}):\n",
    "        ## add default custom objects to the custom objects dictionary, this saves having to define them everytime.\n",
    "        custom_objects = {**branching.default_custom_objects,**custom_objects} \n",
    "        if inputs  is None and model is None and name is not \"\":\n",
    "            model = tf.keras.models.load_model(name,custom_objects=custom_objects)\n",
    "            self.saveLocation = name\n",
    "            super(BranchModel, self).__init__(inputs = model.inputs, outputs=model.outputs,name=model.name)            \n",
    "        elif model is None:\n",
    "            super(BranchModel, self).__init__(inputs = inputs, outputs=outputs,name=name)\n",
    "        elif model is not None:\n",
    "            super(BranchModel, self).__init__(inputs = model.inputs, outputs=model.outputs,name=name)\n",
    "        self.transfer = transfer\n",
    "        self.custom_objects = custom_objects\n",
    "        self.branch_active = False\n",
    " \n",
    "    \n",
    "class Distiller(keras.Model):\n",
    "    def __init__(self, student, teacher):\n",
    "        super(Distiller, self).__init__()\n",
    "        self.teacher = teacher\n",
    "        self.student = tf.keras.models.clone_model(student)\n",
    "    def compile(\n",
    "        self,\n",
    "        optimizer,\n",
    "        metrics,\n",
    "        student_loss_fn,\n",
    "        distillation_loss_fn,\n",
    "        alpha=0.1,\n",
    "        temperature=1,\n",
    "    ):\n",
    "        \"\"\" Configure the distiller.\n",
    "        Args:\n",
    "            optimizer: Keras optimizer for the student weights\n",
    "            metrics: Keras metrics for evaluation\n",
    "            student_loss_fn: Loss function of difference between student\n",
    "                predictions and ground-truth\n",
    "            distillation_loss_fn: Loss function of difference between soft\n",
    "                student predictions and soft teacher predictions\n",
    "            alpha: weight to student_loss_fn and 1-alpha to distillation_loss_fn\n",
    "            temperature: Temperature for softening probability distributions.\n",
    "                Larger temperature gives softer distributions.\n",
    "        \"\"\"\n",
    "        super(Distiller, self).compile(optimizer=optimizer, metrics=metrics)\n",
    "        self.student_loss_fn = student_loss_fn\n",
    "        self.distillation_loss_fn = distillation_loss_fn\n",
    "        self.alpha = alpha\n",
    "        self.temperature = temperature\n",
    "\n",
    "    def train_step(self, data):\n",
    "        # Unpack data\n",
    "        x, y = data\n",
    "        # Forward pass of teacher\n",
    "        teacher_predictions = self.teacher(x, training=False)\n",
    "\n",
    "        with tf.GradientTape() as tape:\n",
    "            # Forward pass of student\n",
    "            student_predictions = self.student(x, training=True)\n",
    "\n",
    "            # Compute losses\n",
    "            student_loss = self.student_loss_fn(y, student_predictions)\n",
    "            distillation_loss = self.distillation_loss_fn(\n",
    "                (teacher_predictions / self.temperature),\n",
    "                (student_predictions / self.temperature),\n",
    "            )\n",
    "#             student_loss = student_loss #* self.alpha\n",
    "#             distillation_loss = (distillation_loss) #* (1 - self.alpha)\n",
    "            loss = distillation_loss\n",
    "            #loss=student_loss +distillation_loss\n",
    "        # Compute gradients\n",
    "        trainable_vars = self.student.trainable_variables\n",
    "        gradients = tape.gradient(loss, trainable_vars)\n",
    "\n",
    "        # Update weights\n",
    "        self.optimizer.apply_gradients(zip(gradients, trainable_vars))\n",
    "\n",
    "        # Update the metrics configured in `compile()`.\n",
    "        self.compiled_metrics.update_state(y, student_predictions)\n",
    "\n",
    "        # Return a dict of performance\n",
    "        results = {m.name: m.result() for m in self.metrics}\n",
    "        results.update(\n",
    "            {\"loss\":loss,\"student_loss\": student_loss, \"distillation_loss\": distillation_loss}\n",
    "        )\n",
    "        return results\n",
    "\n",
    "    def test_step(self, data):\n",
    "        # Unpack the data\n",
    "        x, y = data\n",
    "        # Compute predictions\n",
    "        y_prediction = self.student(x, training=False)\n",
    "\n",
    "        # Calculate the loss\n",
    "        student_loss = self.student_loss_fn(y, y_prediction)\n",
    "\n",
    "        # Update the metrics.\n",
    "        self.compiled_metrics.update_state(y, y_prediction)\n",
    "\n",
    "        # Return a dict of performance\n",
    "        results = {m.name: m.result() for m in self.metrics}\n",
    "        results.update({\"student_loss\": student_loss})\n",
    "        return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import layers, models\n",
    "\n",
    "def summarize_keras_trainable_variables(model, message):\n",
    "    s = sum(map(lambda x: x.sum(), model.get_weights()))\n",
    "    print(\"summary of trainable variables %s: %.13f\" % (message, s))\n",
    "    return s\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_ds batch count 1406\n",
      "validation_ds batch count 156\n",
      "test_ds batch count 312\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# tf.debugging.experimental.enable_dump_debug_info(logdir, tensor_debug_mode=\"FULL_HEALTH\", circular_buffer_size=-1)\n",
    "(train_images, train_labels), (test_images, test_labels) = keras.datasets.cifar10.load_data()\n",
    "\n",
    "CLASS_NAMES= ['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n",
    "\n",
    "# import csv\n",
    "# with open('results/altTrain_labels.csv', newline='') as f:\n",
    "    # reader = csv.reader(f,quoting=csv.QUOTE_NONNUMERIC)\n",
    "    # alt_trainLabels = list(reader)\n",
    "# with open('results/altTest_labels.csv', newline='') as f:\n",
    "    # reader = csv.reader(f,quoting=csv.QUOTE_NONNUMERIC)\n",
    "    # alt_testLabels = list(reader)\n",
    "\n",
    "# altTraining = tf.data.Dataset.from_tensor_slices((train_images,alt_trainLabels))\n",
    "\n",
    "# validation_images, validation_labels = train_images[:5000], alt_trainLabels[:5000]\n",
    "# train_ds = tf.data.Dataset.from_tensor_slices((train_images, alt_trainLabels))\n",
    "# test_ds = tf.data.Dataset.from_tensor_slices((test_images, alt_testLabels))\n",
    "train_labels = tf.keras.utils.to_categorical(train_labels,10)\n",
    "test_labels = tf.keras.utils.to_categorical(test_labels,10)\n",
    "\n",
    "###normal method\n",
    "validation_images, validation_labels = train_images[:5000], train_labels[:5000] #get the first 5k training samples as validation set\n",
    "train_images, train_labels = train_images[5000:], train_labels[5000:] # now remove the validation set from the training set.\n",
    "train_ds = tf.data.Dataset.from_tensor_slices((train_images, train_labels))\n",
    "test_ds = tf.data.Dataset.from_tensor_slices((test_images, test_labels))\n",
    "validation_ds = tf.data.Dataset.from_tensor_slices((validation_images, validation_labels))\n",
    "\n",
    "def augment_images(image, label):\n",
    "    # Normalize images to have a mean of 0 and standard deviation of 1\n",
    "    # image = tf.image.per_image_standardization(image)\n",
    "    # Resize images from 32x32 to 277x277\n",
    "    image = tf.image.resize(image, (227,227))\n",
    "    return image, label\n",
    "\n",
    "train_ds_size = len(list(train_ds))\n",
    "test_ds_size = len(list(test_ds))\n",
    "validation_ds_size = len(list(validation_ds))\n",
    "\n",
    "train_ds = (train_ds\n",
    "                  .map(augment_images)\n",
    "                  .shuffle(buffer_size=train_ds_size,seed=42,reshuffle_each_iteration=False)\n",
    "                  .batch(batch_size=32, drop_remainder=True))\n",
    "\n",
    "test_ds = (test_ds\n",
    "                  .map(augment_images)\n",
    "                #   .shuffle(buffer_size=train_ds_size)\n",
    "                  .batch(batch_size=32, drop_remainder=True))\n",
    "\n",
    "validation_ds = (validation_ds\n",
    "                  .map(augment_images)\n",
    "                #   .shuffle(buffer_size=validation_ds_size)\n",
    "                  .batch(batch_size=32, drop_remainder=True))\n",
    "print(\"train_ds batch count\", len(train_ds))\n",
    "print(\"validation_ds batch count\", len(validation_ds))\n",
    "print(\"test_ds batch count\", len(test_ds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"alexnet\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_2 (InputLayer)        [(None, 227, 227, 3)]     0         \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 55, 55, 96)        34944     \n",
      "                                                                 \n",
      " batch_normalization (BatchN  (None, 55, 55, 96)       384       \n",
      " ormalization)                                                   \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2D  (None, 27, 27, 96)       0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 27, 27, 256)       614656    \n",
      "                                                                 \n",
      " batch_normalization_1 (Batc  (None, 27, 27, 256)      1024      \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPooling  (None, 13, 13, 256)      0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_3 (Conv2D)           (None, 13, 13, 384)       885120    \n",
      "                                                                 \n",
      " batch_normalization_2 (Batc  (None, 13, 13, 384)      1536      \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " conv2d_4 (Conv2D)           (None, 13, 13, 384)       147840    \n",
      "                                                                 \n",
      " batch_normalization_3 (Batc  (None, 13, 13, 384)      1536      \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " conv2d_5 (Conv2D)           (None, 13, 13, 256)       98560     \n",
      "                                                                 \n",
      " batch_normalization_4 (Batc  (None, 13, 13, 256)      1024      \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " max_pooling2d_2 (MaxPooling  (None, 6, 6, 256)        0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 9216)              0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 4096)              37752832  \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 4096)              0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 4096)              16781312  \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 4096)              0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 10)                40970     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 56,361,738\n",
      "Trainable params: 56,358,986\n",
      "Non-trainable params: 2,752\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_teacher = tf.keras.models.load_model(\"models/alexNetv6_logits_teacher.hdf5\")\n",
    "model_teacher.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "312/312 [==============================] - 5s 12ms/step - loss: 0.6905 - accuracy: 0.7940\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.6905280947685242, 0.7939703464508057]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_teacher.evaluate(test_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First, train the student model without the teacher input to get a baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"alexnet\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_3 (InputLayer)        [(None, 227, 227, 3)]     0         \n",
      "                                                                 \n",
      " conv2d_5 (Conv2D)           (None, 55, 55, 96)        34944     \n",
      "                                                                 \n",
      " batch_normalization_5 (Batc  (None, 55, 55, 96)       384       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " max_pooling2d_3 (MaxPooling  (None, 27, 27, 96)       0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_6 (Conv2D)           (None, 27, 27, 256)       614656    \n",
      "                                                                 \n",
      " batch_normalization_6 (Batc  (None, 27, 27, 256)      1024      \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " max_pooling2d_4 (MaxPooling  (None, 13, 13, 256)      0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_7 (Conv2D)           (None, 13, 13, 384)       885120    \n",
      "                                                                 \n",
      " batch_normalization_7 (Batc  (None, 13, 13, 384)      1536      \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " conv2d_8 (Conv2D)           (None, 13, 13, 384)       147840    \n",
      "                                                                 \n",
      " batch_normalization_8 (Batc  (None, 13, 13, 384)      1536      \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " conv2d_9 (Conv2D)           (None, 13, 13, 256)       98560     \n",
      "                                                                 \n",
      " batch_normalization_9 (Batc  (None, 13, 13, 256)      1024      \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " max_pooling2d_5 (MaxPooling  (None, 6, 6, 256)        0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " flatten_1 (Flatten)         (None, 9216)              0         \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 4096)              37752832  \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 4096)              0         \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 4096)              16781312  \n",
      "                                                                 \n",
      " dropout_3 (Dropout)         (None, 4096)              0         \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 10)                40970     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 56,361,738\n",
      "Trainable params: 56,358,986\n",
      "Non-trainable params: 2,752\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "seed = 66\n",
    "# random.seed(seed)\n",
    "tf.random.set_seed(seed)\n",
    "np.random.seed(seed)\n",
    "## keep setting the seed so that it doesn't matter what order you complete cells in. \n",
    "student_model = tf.keras.models.load_model(\"models/alexNetv6_logits_student.hdf5\")\n",
    "student_model.summary()\n",
    "# summarize_keras_trainable_variables(student_model,\"before training\")\n",
    "# student_model.fit(train_ds, validation_data = validation_ds, epochs=9)\n",
    "# summarize_keras_trainable_variables(student_model,\"after training\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "312/312 [==============================] - 4s 12ms/step - loss: 0.7575 - accuracy: 0.7361\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.7575096487998962, 0.7360777258872986]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "student_model.evaluate(test_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "summary of trainable variables before training: 2688.9488805532455\n",
      "Epoch 1/3\n",
      "1406/1406 [==============================] - 92s 57ms/step - loss: 1.7787 - accuracy: 0.3966 - val_loss: 1.4048 - val_accuracy: 0.5000\n",
      "Epoch 2/3\n",
      "1406/1406 [==============================] - 82s 53ms/step - loss: 1.3243 - accuracy: 0.5234 - val_loss: 1.1543 - val_accuracy: 0.5998\n",
      "Epoch 3/3\n",
      "1406/1406 [==============================] - 82s 53ms/step - loss: 1.1354 - accuracy: 0.5913 - val_loss: 1.0041 - val_accuracy: 0.6478\n",
      "summary of trainable variables after training: 9552084.5724323093891\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "9552084.57243231"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seed = 66\n",
    "# random.seed(seed)\n",
    "tf.random.set_seed(seed)\n",
    "np.random.seed(seed)\n",
    "## keep setting the seed so that it doesn't matter what order you complete cells in. \n",
    "student_model = tf.keras.models.load_model(\"models/alexNetv6_logits_student.hdf5\")\n",
    "summarize_keras_trainable_variables(student_model,\"before training\")\n",
    "student_model.fit(train_ds, validation_data = validation_ds, epochs=3)\n",
    "summarize_keras_trainable_variables(student_model,\"after training\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "312/312 [==============================] - 4s 12ms/step - loss: 1.0213 - accuracy: 0.6436\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1.0212757587432861, 0.643629789352417]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "student_model.evaluate(test_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "summary of trainable variables before training: 2688.9488805532455\n",
      "Epoch 1/3\n",
      "1406/1406 [==============================] - 101s 51ms/step - loss: 1.7787 - accuracy: 0.3966 - val_loss: 1.4048 - val_accuracy: 0.5000\n",
      "Epoch 2/3\n",
      "1406/1406 [==============================] - 81s 53ms/step - loss: 1.3243 - accuracy: 0.5234 - val_loss: 1.1543 - val_accuracy: 0.5998\n",
      "Epoch 3/3\n",
      "1406/1406 [==============================] - 103s 66ms/step - loss: 1.1354 - accuracy: 0.5913 - val_loss: 1.0041 - val_accuracy: 0.6478\n",
      "summary of trainable variables after training: 9552084.5724323093891\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "9552084.57243231"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seed = 66\n",
    "# random.seed(seed)\n",
    "tf.random.set_seed(seed)\n",
    "np.random.seed(seed)\n",
    "## keep setting the seed so that it doesn't matter what order you complete cells in. \n",
    "student_model = tf.keras.models.load_model(\"models/alexNetv6_logits_student.hdf5\")\n",
    "summarize_keras_trainable_variables(student_model,\"before training\")\n",
    "student_model.fit(train_ds, validation_data = validation_ds, epochs=3)\n",
    "summarize_keras_trainable_variables(student_model,\"after training\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "312/312 [==============================] - 4s 11ms/step - loss: 1.0213 - accuracy: 0.6436\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1.0212757587432861, 0.643629789352417]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "student_model.evaluate(test_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next, train the student with the teacher model input as well to see the difference the teacher made. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 66\n",
    "# random.seed(seed)\n",
    "tf.random.set_seed(seed)\n",
    "np.random.seed(seed)\n",
    "def createDistiller(alpha=1):\n",
    "    loaded_student = tf.keras.models.load_model(\"models/alexNetv6_logits_student.hdf5\")\n",
    "    summarize_keras_trainable_variables(loaded_student,\"before compiling in distiller\")\n",
    "    distiller = Distiller(student=loaded_student, teacher=model_teacher)\n",
    "    distiller.compile(\n",
    "        optimizer=tf.optimizers.SGD(lr=0.001,momentum=0.9),\n",
    "        metrics=[keras.metrics.CategoricalAccuracy()],\n",
    "        student_loss_fn=keras.losses.CategoricalCrossentropy(from_logits=False),\n",
    "\n",
    "        distillation_loss_fn=keras.losses.KLDivergence(),\n",
    "        alpha=alpha,\n",
    "        temperature=1,\n",
    "    )\n",
    "    return distiller\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "summary of trainable variables before compiling in distiller: 2688.9488805532455\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Sanity\\AppData\\Local\\conda\\conda\\envs\\py36\\lib\\site-packages\\keras\\optimizer_v2\\optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "summary of trainable variables before training: 2684.4909174442291\n",
      "Epoch 1/3\n",
      "1406/1406 [==============================] - 117s 74ms/step - categorical_accuracy: 0.4011 - loss: 1.5589 - student_loss: 1.7577 - distillation_loss: 1.5589 - val_categorical_accuracy: 0.4545 - val_student_loss: 1.8797\n",
      "Epoch 2/3\n",
      "1406/1406 [==============================] - 121s 80ms/step - categorical_accuracy: 0.5406 - loss: 1.0852 - student_loss: 1.2874 - distillation_loss: 1.0852 - val_categorical_accuracy: 0.6056 - val_student_loss: 1.5514\n",
      "Epoch 3/3\n",
      "1406/1406 [==============================] - 114s 75ms/step - categorical_accuracy: 0.6140 - loss: 0.8843 - student_loss: 1.0866 - distillation_loss: 0.8843 - val_categorical_accuracy: 0.6721 - val_student_loss: 1.5438\n",
      "312/312 [==============================] - 4s 11ms/step - categorical_accuracy: 0.6655 - student_loss: 0.9662\n"
     ]
    }
   ],
   "source": [
    "seed = 66\n",
    "# random.seed(seed)\n",
    "tf.random.set_seed(seed)\n",
    "np.random.seed(seed)\n",
    "distiller = createDistiller(1)\n",
    "summarize_keras_trainable_variables(distiller.student,\"before training\")\n",
    "# Distill teacher to student\n",
    "distiller.fit(train_ds, validation_data = validation_ds,epochs=3,verbose=1)\n",
    "# Evaluate student on test dataset\n",
    "res = distiller.evaluate(test_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "summary of trainable variables before compiling in distiller: 2688.9488805532455\n",
      "summary of trainable variables before training: 2684.4909174442291\n",
      "Epoch 1/3\n",
      "1406/1406 [==============================] - 101s 64ms/step - categorical_accuracy: 0.4110 - loss: 1.5378 - student_loss: 1.7385 - distillation_loss: 1.5378 - val_categorical_accuracy: 0.4698 - val_student_loss: 1.8240\n",
      "Epoch 2/3\n",
      "1406/1406 [==============================] - 100s 64ms/step - categorical_accuracy: 0.5558 - loss: 1.0442 - student_loss: 1.2479 - distillation_loss: 1.0442 - val_categorical_accuracy: 0.5649 - val_student_loss: 1.5232\n",
      "Epoch 3/3\n",
      "1406/1406 [==============================] - 127s 83ms/step - categorical_accuracy: 0.6262 - loss: 0.8506 - student_loss: 1.0549 - distillation_loss: 0.8506 - val_categorical_accuracy: 0.6635 - val_student_loss: 1.5830\n",
      "312/312 [==============================] - 4s 12ms/step - categorical_accuracy: 0.6655 - student_loss: 0.9412\n"
     ]
    }
   ],
   "source": [
    "seed = 66\n",
    "# random.seed(seed)\n",
    "tf.random.set_seed(seed)\n",
    "np.random.seed(seed)\n",
    "distiller = createDistiller(1)\n",
    "summarize_keras_trainable_variables(distiller.student,\"before training\")\n",
    "# Distill teacher to student\n",
    "distiller.fit(train_ds, validation_data = validation_ds,epochs=3,verbose=1)\n",
    "# Evaluate student on test dataset\n",
    "res = distiller.evaluate(test_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "summary of trainable variables before compiling in distiller: 2688.9488805532455\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Sanity\\AppData\\Local\\conda\\conda\\envs\\py36\\lib\\site-packages\\keras\\optimizer_v2\\optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "summary of trainable variables before training: 2684.4909174442291\n",
      "Epoch 1/9\n",
      "351/351 [==============================] - 124s 263ms/step - categorical_accuracy: 0.2968 - student_loss: 0.2443 - distillation_loss: 0.3866 - val_categorical_accuracy: 0.4323 - val_student_loss: 1.6946\n",
      "Epoch 2/9\n",
      "351/351 [==============================] - 89s 222ms/step - categorical_accuracy: 0.4244 - student_loss: 0.1675 - distillation_loss: 0.2981 - val_categorical_accuracy: 0.4914 - val_student_loss: 1.5332\n",
      "Epoch 3/9\n",
      "351/351 [==============================] - 96s 237ms/step - categorical_accuracy: 0.4865 - student_loss: 0.1473 - distillation_loss: 0.2653 - val_categorical_accuracy: 0.5407 - val_student_loss: 1.4427\n",
      "Epoch 4/9\n",
      "351/351 [==============================] - 89s 220ms/step - categorical_accuracy: 0.5312 - student_loss: 0.1350 - distillation_loss: 0.2419 - val_categorical_accuracy: 0.4559 - val_student_loss: 1.7602\n",
      "Epoch 5/9\n",
      "351/351 [==============================] - 80s 207ms/step - categorical_accuracy: 0.5654 - student_loss: 0.1251 - distillation_loss: 0.2227 - val_categorical_accuracy: 0.6136 - val_student_loss: 1.2668\n",
      "Epoch 6/9\n",
      "351/351 [==============================] - 80s 205ms/step - categorical_accuracy: 0.5933 - student_loss: 0.1177 - distillation_loss: 0.2089 - val_categorical_accuracy: 0.6044 - val_student_loss: 1.3350\n",
      "Epoch 7/9\n",
      "351/351 [==============================] - 81s 208ms/step - categorical_accuracy: 0.6164 - student_loss: 0.1112 - distillation_loss: 0.1962 - val_categorical_accuracy: 0.6342 - val_student_loss: 1.2468\n",
      "Epoch 8/9\n",
      "351/351 [==============================] - 83s 207ms/step - categorical_accuracy: 0.6342 - student_loss: 0.1057 - distillation_loss: 0.1851 - val_categorical_accuracy: 0.6494 - val_student_loss: 1.1680\n",
      "Epoch 9/9\n",
      "351/351 [==============================] - 148s 203ms/step - categorical_accuracy: 0.6566 - student_loss: 0.1003 - distillation_loss: 0.1740 - val_categorical_accuracy: 0.6366 - val_student_loss: 1.2129\n",
      "78/78 [==============================] - 3s 39ms/step - categorical_accuracy: 0.6296 - student_loss: 1.0641\n",
      "results:  [0.6296073794364929, 1.137315273284912]\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'i' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-4296f97d2aef>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[0mres\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdistiller\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_ds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"results: \"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mres\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m \u001b[0mresults\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mres\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     16\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_seed\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mseed\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'i' is not defined"
     ]
    }
   ],
   "source": [
    "results = {}\n",
    "results[0.1] = \"test\"\n",
    "# for i in range(10):\n",
    "seed = 66\n",
    "# random.seed(seed)\n",
    "tf.random.set_seed(seed)\n",
    "np.random.seed(seed)\n",
    "distiller = createDistiller(0.1)\n",
    "summarize_keras_trainable_variables(distiller.student,\"before training\")\n",
    "# Distill teacher to student\n",
    "distiller.fit(train_ds, validation_data = validation_ds,epochs=9,verbose=1)\n",
    "# Evaluate student on test dataset\n",
    "res = distiller.evaluate(test_ds)\n",
    "print(\"results: \",res)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "summary of trainable variables before compiling in distiller: 2688.9488805532455\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Sanity\\AppData\\Local\\conda\\conda\\envs\\py36\\lib\\site-packages\\keras\\optimizer_v2\\optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "summary of trainable variables before training: 2684.4909174442291\n",
      "Epoch 1/9\n",
      "1406/1406 [==============================] - 103s 65ms/step - categorical_accuracy: 0.3953 - student_loss: 0.8973 - distillation_loss: 0.1748 - val_categorical_accuracy: 0.4946 - val_student_loss: 1.7729\n",
      "Epoch 2/9\n",
      "1406/1406 [==============================] - 101s 65ms/step - categorical_accuracy: 0.5283 - student_loss: 0.6598 - distillation_loss: 0.1342 - val_categorical_accuracy: 0.5897 - val_student_loss: 1.5725\n",
      "Epoch 3/9\n",
      "1406/1406 [==============================] - 109s 69ms/step - categorical_accuracy: 0.5977 - student_loss: 0.5637 - distillation_loss: 0.1136 - val_categorical_accuracy: 0.6210 - val_student_loss: 1.6123\n",
      "Epoch 4/9\n",
      "1406/1406 [==============================] - 100s 65ms/step - categorical_accuracy: 0.6509 - student_loss: 0.4930 - distillation_loss: 0.0983 - val_categorical_accuracy: 0.5873 - val_student_loss: 1.7226\n",
      "Epoch 5/9\n",
      "1406/1406 [==============================] - 104s 69ms/step - categorical_accuracy: 0.6930 - student_loss: 0.4369 - distillation_loss: 0.0857 - val_categorical_accuracy: 0.6997 - val_student_loss: 1.3866\n",
      "Epoch 6/9\n",
      "1406/1406 [==============================] - 109s 70ms/step - categorical_accuracy: 0.7268 - student_loss: 0.3912 - distillation_loss: 0.0758 - val_categorical_accuracy: 0.7280 - val_student_loss: 1.3901\n",
      "Epoch 7/9\n",
      "1406/1406 [==============================] - 100s 65ms/step - categorical_accuracy: 0.7538 - student_loss: 0.3515 - distillation_loss: 0.0676 - val_categorical_accuracy: 0.7292 - val_student_loss: 1.3652\n",
      "Epoch 8/9\n",
      "1406/1406 [==============================] - 100s 66ms/step - categorical_accuracy: 0.7783 - student_loss: 0.3188 - distillation_loss: 0.0605 - val_categorical_accuracy: 0.7284 - val_student_loss: 1.4569\n",
      "Epoch 9/9\n",
      "1406/1406 [==============================] - 101s 65ms/step - categorical_accuracy: 0.7993 - student_loss: 0.2880 - distillation_loss: 0.0542 - val_categorical_accuracy: 0.7550 - val_student_loss: 1.3230\n",
      "312/312 [==============================] - 4s 13ms/step - categorical_accuracy: 0.7476 - student_loss: 0.7384\n",
      "results:  [0.7475961446762085, 1.0419151782989502]\n",
      "summary of trainable variables before compiling in distiller: 2688.9488805532455\n",
      "summary of trainable variables before training: 2684.4909174442291\n",
      "Epoch 1/9\n",
      "1406/1406 [==============================] - 102s 65ms/step - categorical_accuracy: 0.3941 - student_loss: 1.0756 - distillation_loss: 0.1399 - val_categorical_accuracy: 0.5310 - val_student_loss: 1.6153\n",
      "Epoch 2/9\n",
      "1406/1406 [==============================] - 100s 64ms/step - categorical_accuracy: 0.5300 - student_loss: 0.7844 - distillation_loss: 0.1069 - val_categorical_accuracy: 0.5938 - val_student_loss: 1.8170\n",
      "Epoch 3/9\n",
      "1406/1406 [==============================] - 104s 68ms/step - categorical_accuracy: 0.6014 - student_loss: 0.6721 - distillation_loss: 0.0908 - val_categorical_accuracy: 0.6118 - val_student_loss: 1.6058\n",
      "Epoch 4/9\n",
      "1406/1406 [==============================] - 98s 65ms/step - categorical_accuracy: 0.6550 - student_loss: 0.5865 - distillation_loss: 0.0780 - val_categorical_accuracy: 0.6062 - val_student_loss: 1.6798\n",
      "Epoch 5/9\n",
      "1406/1406 [==============================] - 99s 65ms/step - categorical_accuracy: 0.6968 - student_loss: 0.5185 - distillation_loss: 0.0681 - val_categorical_accuracy: 0.6569 - val_student_loss: 1.5653\n",
      "Epoch 6/9\n",
      "1406/1406 [==============================] - 113s 74ms/step - categorical_accuracy: 0.7293 - student_loss: 0.4617 - distillation_loss: 0.0601 - val_categorical_accuracy: 0.7025 - val_student_loss: 1.4068\n",
      "Epoch 7/9\n",
      "1406/1406 [==============================] - 107s 68ms/step - categorical_accuracy: 0.7606 - student_loss: 0.4141 - distillation_loss: 0.0529 - val_categorical_accuracy: 0.7258 - val_student_loss: 1.2081\n",
      "Epoch 8/9\n",
      "1406/1406 [==============================] - 106s 69ms/step - categorical_accuracy: 0.7843 - student_loss: 0.3690 - distillation_loss: 0.0471 - val_categorical_accuracy: 0.7282 - val_student_loss: 1.3934\n",
      "Epoch 9/9\n",
      "1406/1406 [==============================] - 105s 69ms/step - categorical_accuracy: 0.8063 - student_loss: 0.3340 - distillation_loss: 0.0424 - val_categorical_accuracy: 0.7454 - val_student_loss: 1.4467\n",
      "312/312 [==============================] - 4s 13ms/step - categorical_accuracy: 0.7395 - student_loss: 0.7749\n",
      "results:  [0.7394831776618958, 1.1406097412109375]\n",
      "summary of trainable variables before compiling in distiller: 2688.9488805532455\n",
      "summary of trainable variables before training: 2684.4909174442291\n",
      "Epoch 1/9\n",
      "1406/1406 [==============================] - 109s 69ms/step - categorical_accuracy: 0.3948 - student_loss: 1.2564 - distillation_loss: 0.1049 - val_categorical_accuracy: 0.5260 - val_student_loss: 1.8429\n",
      "Epoch 2/9\n",
      "1406/1406 [==============================] - 108s 69ms/step - categorical_accuracy: 0.5350 - student_loss: 0.9112 - distillation_loss: 0.0797 - val_categorical_accuracy: 0.6054 - val_student_loss: 1.5317\n",
      "Epoch 3/9\n",
      "1406/1406 [==============================] - 108s 71ms/step - categorical_accuracy: 0.6057 - student_loss: 0.7782 - distillation_loss: 0.0673 - val_categorical_accuracy: 0.6424 - val_student_loss: 1.4658\n",
      "Epoch 4/9\n",
      "1406/1406 [==============================] - 105s 67ms/step - categorical_accuracy: 0.6578 - student_loss: 0.6758 - distillation_loss: 0.0579 - val_categorical_accuracy: 0.6979 - val_student_loss: 1.3753\n",
      "Epoch 5/9\n",
      "1406/1406 [==============================] - 109s 71ms/step - categorical_accuracy: 0.7020 - student_loss: 0.5970 - distillation_loss: 0.0505 - val_categorical_accuracy: 0.7175 - val_student_loss: 1.3387\n",
      "Epoch 6/9\n",
      "1406/1406 [==============================] - 104s 68ms/step - categorical_accuracy: 0.7310 - student_loss: 0.5300 - distillation_loss: 0.0444 - val_categorical_accuracy: 0.7188 - val_student_loss: 1.3682\n",
      "Epoch 7/9\n",
      "1406/1406 [==============================] - 102s 67ms/step - categorical_accuracy: 0.7648 - student_loss: 0.4672 - distillation_loss: 0.0387 - val_categorical_accuracy: 0.7288 - val_student_loss: 1.2701\n",
      "Epoch 8/9\n",
      "1406/1406 [==============================] - 110s 72ms/step - categorical_accuracy: 0.7896 - student_loss: 0.4199 - distillation_loss: 0.0346 - val_categorical_accuracy: 0.7242 - val_student_loss: 1.4400\n",
      "Epoch 9/9\n",
      "1406/1406 [==============================] - 105s 68ms/step - categorical_accuracy: 0.8126 - student_loss: 0.3738 - distillation_loss: 0.0308 - val_categorical_accuracy: 0.7240 - val_student_loss: 1.3909\n",
      "312/312 [==============================] - 4s 13ms/step - categorical_accuracy: 0.7182 - student_loss: 0.8477\n",
      "results:  [0.7182492017745972, 1.1012463569641113]\n",
      "summary of trainable variables before compiling in distiller: 2688.9488805532455\n",
      "summary of trainable variables before training: 2684.4909174442291\n",
      "Epoch 1/9\n",
      "1406/1406 [==============================] - 107s 70ms/step - categorical_accuracy: 0.4013 - student_loss: 1.4143 - distillation_loss: 0.0694 - val_categorical_accuracy: 0.5246 - val_student_loss: 1.8258\n",
      "Epoch 2/9\n",
      "1406/1406 [==============================] - 109s 70ms/step - categorical_accuracy: 0.5354 - student_loss: 1.0406 - distillation_loss: 0.0530 - val_categorical_accuracy: 0.5747 - val_student_loss: 1.7718\n",
      "Epoch 3/9\n",
      " 393/1406 [=======>......................] - ETA: 1:02 - categorical_accuracy: 0.5866 - student_loss: 0.9235 - distillation_loss: 0.0471"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-15-12dc41d9a423>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     34\u001b[0m \u001b[0msummarize_keras_trainable_variables\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdistiller\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstudent\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\"before training\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     35\u001b[0m \u001b[1;31m# Distill teacher to student\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 36\u001b[1;33m \u001b[0mdistiller\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_ds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvalidation_ds\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m9\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     37\u001b[0m \u001b[1;31m# Evaluate student on test dataset\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     38\u001b[0m \u001b[0mres\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdistiller\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_ds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\conda\\conda\\envs\\py36\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1182\u001b[0m                 _r=1):\n\u001b[0;32m   1183\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1184\u001b[1;33m               \u001b[0mtmp_logs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1185\u001b[0m               \u001b[1;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1186\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    883\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    884\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 885\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    886\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    887\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    915\u001b[0m       \u001b[1;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    916\u001b[0m       \u001b[1;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 917\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=not-callable\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    918\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    919\u001b[0m       \u001b[1;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   3038\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[0;32m   3039\u001b[0m     return graph_function._call_flat(\n\u001b[1;32m-> 3040\u001b[1;33m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0m\u001b[0;32m   3041\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3042\u001b[0m   \u001b[1;33m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1962\u001b[0m       \u001b[1;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1963\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[1;32m-> 1964\u001b[1;33m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[0;32m   1965\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[0;32m   1966\u001b[0m         \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    594\u001b[0m               \u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    595\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 596\u001b[1;33m               ctx=ctx)\n\u001b[0m\u001b[0;32m    597\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    598\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[1;32m---> 60\u001b[1;33m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[0;32m     61\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "tf.random.set_seed(seed)\n",
    "np.random.seed(seed)\n",
    "distiller = createDistiller(0.5)\n",
    "summarize_keras_trainable_variables(distiller.student,\"before training\")\n",
    "# Distill teacher to student\n",
    "distiller.fit(train_ds, validation_data = validation_ds,epochs=9,verbose=1)\n",
    "# Evaluate student on test dataset\n",
    "res = distiller.evaluate(test_ds)\n",
    "print(\"results: \",res)\n",
    "\n",
    "tf.random.set_seed(seed)\n",
    "np.random.seed(seed)\n",
    "distiller = createDistiller(0.6)\n",
    "summarize_keras_trainable_variables(distiller.student,\"before training\")\n",
    "# Distill teacher to student\n",
    "distiller.fit(train_ds, validation_data = validation_ds,epochs=9,verbose=1)\n",
    "# Evaluate student on test dataset\n",
    "res = distiller.evaluate(test_ds)\n",
    "print(\"results: \",res)\n",
    "\n",
    "tf.random.set_seed(seed)\n",
    "np.random.seed(seed)\n",
    "distiller = createDistiller(0.7)\n",
    "summarize_keras_trainable_variables(distiller.student,\"before training\")\n",
    "# Distill teacher to student\n",
    "distiller.fit(train_ds, validation_data = validation_ds,epochs=9,verbose=1)\n",
    "# Evaluate student on test dataset\n",
    "res = distiller.evaluate(test_ds)\n",
    "print(\"results: \",res)\n",
    "\n",
    "tf.random.set_seed(seed)\n",
    "np.random.seed(seed)\n",
    "distiller = createDistiller(0.8)\n",
    "summarize_keras_trainable_variables(distiller.student,\"before training\")\n",
    "# Distill teacher to student\n",
    "distiller.fit(train_ds, validation_data = validation_ds,epochs=9,verbose=1)\n",
    "# Evaluate student on test dataset\n",
    "res = distiller.evaluate(test_ds)\n",
    "print(\"results: \",res)\n",
    "\n",
    "tf.random.set_seed(seed)\n",
    "np.random.seed(seed)\n",
    "distiller = createDistiller(1)\n",
    "summarize_keras_trainable_variables(distiller.student,\"before training\")\n",
    "# Distill teacher to student\n",
    "distiller.fit(train_ds, validation_data = validation_ds,epochs=9,verbose=1)\n",
    "# Evaluate student on test dataset\n",
    "res = distiller.evaluate(test_ds)\n",
    "print(\"results: \",res)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "summary of trainable variables before compiling in distiller: 2688.9488805532455\n",
      "1406/1406 [==============================] - 85s 58ms/step - loss: 1.7808 - accuracy: 0.3975 - val_loss: 1.3747 - val_accuracy: 0.5082\n",
      "312/312 [==============================] - 4s 13ms/step - loss: 1.3680 - accuracy: 0.5146\n",
      "summary of trainable variables before compiling in distiller: 2688.9488805532455\n",
      "1406/1406 [==============================] - 85s 57ms/step - loss: 1.7978 - accuracy: 0.3899 - val_loss: 1.3804 - val_accuracy: 0.5022\n",
      "312/312 [==============================] - 4s 13ms/step - loss: 1.3786 - accuracy: 0.5082\n",
      "summary of trainable variables before compiling in distiller: 2688.9488805532455\n",
      "1406/1406 [==============================] - 84s 57ms/step - loss: 1.7687 - accuracy: 0.4037 - val_loss: 1.4553 - val_accuracy: 0.4906\n",
      "312/312 [==============================] - 4s 13ms/step - loss: 1.4670 - accuracy: 0.4779\n"
     ]
    }
   ],
   "source": [
    "for i in range(3):\n",
    "    seed = 66\n",
    "    # random.seed(seed)\n",
    "    tf.random.set_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    distiller = createDistiller(0.1)\n",
    "#     summarize_keras_trainable_variables(distiller.student,\"before training\")\n",
    "    # Distill teacher to student\n",
    "    tf.random.set_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    model = tf.keras.models.load_model(\"models/alexNetv6_logits_student.hdf5\")\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=tf.optimizers.SGD(lr=0.001,momentum=0.9), metrics=['accuracy'])\n",
    "    model.fit(train_ds, validation_data=validation_ds, epochs=1,verbose=1)\n",
    "    model.evaluate(test_ds)\n",
    "#     distiller.fit(train_ds, validation_data = validation_ds,epochs=1,verbose=1)\n",
    "    # Evaluate student on test dataset\n",
    "#     res = distiller.evaluate(test_ds)\n",
    "#     print(\"results: \",res)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "summary of trainable variables before training: 2931.7305606603622\n",
      "351/351 [==============================] - 180s 314ms/step - loss: 1.9348 - accuracy: 0.3622 - val_loss: 1.5802 - val_accuracy: 0.4387\n",
      "summary of trainable variables after training: 2272256.2266048272140\n",
      "78/78 [==============================] - 3s 42ms/step - loss: 1.5863 - accuracy: 0.4361\n",
      "[1.5863159894943237, 0.43609777092933655]\n",
      "summary of trainable variables before training: 2931.7305606603622\n",
      "351/351 [==============================] - 80s 195ms/step - loss: 1.9348 - accuracy: 0.3622 - val_loss: 1.5802 - val_accuracy: 0.4387\n",
      "summary of trainable variables after training: 2272256.2266048272140\n",
      "78/78 [==============================] - 3s 43ms/step - loss: 1.5863 - accuracy: 0.4361\n",
      "[1.5863159894943237, 0.43609777092933655]\n",
      "summary of trainable variables before training: 2931.7305606603622\n",
      "351/351 [==============================] - 81s 203ms/step - loss: 1.9348 - accuracy: 0.3622 - val_loss: 1.5802 - val_accuracy: 0.4387\n",
      "summary of trainable variables after training: 2272256.2266048272140\n",
      "78/78 [==============================] - 3s 44ms/step - loss: 1.5863 - accuracy: 0.4361\n",
      "[1.5863159894943237, 0.43609777092933655]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# tf.debugging.experimental.enable_dump_debug_info(logdir, tensor_debug_mode=\"FULL_HEALTH\", circular_buffer_size=-1)\n",
    "(train_images, train_labels), (test_images, test_labels) = keras.datasets.cifar10.load_data()\n",
    "\n",
    "CLASS_NAMES= ['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n",
    "\n",
    "# import csv\n",
    "# with open('results/altTrain_labels.csv', newline='') as f:\n",
    "    # reader = csv.reader(f,quoting=csv.QUOTE_NONNUMERIC)\n",
    "    # alt_trainLabels = list(reader)\n",
    "# with open('results/altTest_labels.csv', newline='') as f:\n",
    "    # reader = csv.reader(f,quoting=csv.QUOTE_NONNUMERIC)\n",
    "    # alt_testLabels = list(reader)\n",
    "\n",
    "# altTraining = tf.data.Dataset.from_tensor_slices((train_images,alt_trainLabels))\n",
    "\n",
    "# validation_images, validation_labels = train_images[:5000], alt_trainLabels[:5000]\n",
    "# train_ds = tf.data.Dataset.from_tensor_slices((train_images, alt_trainLabels))\n",
    "# test_ds = tf.data.Dataset.from_tensor_slices((test_images, alt_testLabels))\n",
    "train_labels = tf.keras.utils.to_categorical(train_labels,10)\n",
    "test_labels = tf.keras.utils.to_categorical(test_labels,10)\n",
    "\n",
    "###normal method\n",
    "validation_images, validation_labels = train_images[:5000], train_labels[:5000] #get the first 5k training samples as validation set\n",
    "train_images, train_labels = train_images[5000:], train_labels[5000:] # now remove the validation set from the training set.\n",
    "train_ds = tf.data.Dataset.from_tensor_slices((train_images, train_labels))\n",
    "test_ds = tf.data.Dataset.from_tensor_slices((test_images, test_labels))\n",
    "validation_ds = tf.data.Dataset.from_tensor_slices((validation_images, validation_labels))\n",
    "\n",
    "def augment_images(image, label):\n",
    "    # Normalize images to have a mean of 0 and standard deviation of 1\n",
    "    # image = tf.image.per_image_standardization(image)\n",
    "    # Resize images from 32x32 to 277x277\n",
    "    image = tf.image.resize(image, (227,227))\n",
    "    return image, label\n",
    "\n",
    "train_ds_size = len(list(train_ds))\n",
    "test_ds_size = len(list(test_ds))\n",
    "validation_ds_size = len(list(validation_ds))\n",
    "\n",
    "train_ds = (train_ds\n",
    "                  .map(augment_images)\n",
    "                  .shuffle(buffer_size=train_ds_size,seed=42,reshuffle_each_iteration=False)\n",
    "                  .batch(batch_size=128, drop_remainder=True))\n",
    "\n",
    "test_ds = (test_ds\n",
    "                  .map(augment_images)\n",
    "                #   .shuffle(buffer_size=train_ds_size)\n",
    "                  .batch(batch_size=128, drop_remainder=True))\n",
    "\n",
    "validation_ds = (validation_ds\n",
    "                  .map(augment_images)\n",
    "                #   .shuffle(buffer_size=validation_ds_size)\n",
    "                  .batch(batch_size=128, drop_remainder=True))\n",
    "\n",
    "for i in range(3):\n",
    "    tf.random.set_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    model = tf.keras.models.load_model(\"distil_test.hdf5\")\n",
    "    summarize_keras_trainable_variables(model,\"before training\")\n",
    "    \n",
    "    model.fit(train_ds, shuffle=False,validation_data = validation_ds,epochs=1)\n",
    "    summarize_keras_trainable_variables(model,\"after training\")\n",
    "    results = model.evaluate(test_ds)\n",
    "    print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n",
      "0.01\n",
      "0.02\n",
      "0.03\n",
      "0.04\n",
      "0.05\n",
      "0.06\n",
      "0.07\n",
      "0.08\n",
      "0.09\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    print(i/100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 42\n",
    "# random.seed(seed)\n",
    "tf.random.set_seed(seed)\n",
    "np.random.seed(seed)\n",
    "## keep setting the seed so that it doesn't matter what order you complete cells in. \n",
    "student_model = tf.keras.models.load_model(\"models/alexNetv6_logits_student.hdf5\")\n",
    "summarize_keras_trainable_variables(student_model,\"before training\")\n",
    "student_model.fit(train_ds, validation_data = validation_ds, epochs=3)\n",
    "summarize_keras_trainable_variables(student_model,\"after training\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "Model: \"mnist_model_normal\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_3 (InputLayer)         [(None, 28, 28)]          0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 512)               401920    \n",
      "_________________________________________________________________\n",
      "output1 (Dense)              (None, 10)                5130      \n",
      "_________________________________________________________________\n",
      "softmax (Softmax)            (None, 10)                0         \n",
      "=================================================================\n",
      "Total params: 407,050\n",
      "Trainable params: 407,050\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "outputs =[]\n",
    "inputs = keras.Input(shape=(28,28,))\n",
    "x = layers.Flatten(input_shape=(28,28))(inputs)\n",
    "x = layers.Dense(512, activation=\"relu\")(x)\n",
    "#exit 1 The main branch exit is refered to as \"exit 1\" or \"main exit\" to avoid confusion when adding addtional exits\n",
    "output1 = layers.Dense(10, name=\"output1\")(x)\n",
    "softmax = layers.Softmax()(output1)\n",
    "\n",
    "outputs.append(softmax)\n",
    "print(len(outputs))\n",
    "model_student = keras.Model(inputs=inputs, outputs=outputs, name=\"mnist_model_normal\")\n",
    "model_student.summary()\n",
    "#visualize_model(model,\"mnist_normal\")\n",
    "print(len(model_student.outputs))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = keras.Input(shape=(227,227,3))\n",
    "# targets = keras.Input(shape=(10,))\n",
    "x = keras.layers.Conv2D(filters=96, kernel_size=(11,11), strides=(4,4), activation='relu', input_shape=(227,227,3))(inputs)\n",
    "x = keras.layers.BatchNormalization()(x)\n",
    "x = keras.layers.MaxPool2D(pool_size=(3,3), strides=(2,2))(x)\n",
    "x = keras.layers.Conv2D(filters=256, kernel_size=(5,5), strides=(1,1), activation='relu', padding=\"same\")(x)\n",
    "x = keras.layers.BatchNormalization()(x)\n",
    "x = keras.layers.MaxPool2D(pool_size=(3,3), strides=(2,2))(x)\n",
    "x = keras.layers.Conv2D(filters=384, kernel_size=(3,3), strides=(1,1), activation='relu', padding=\"same\")(x)\n",
    "x = keras.layers.BatchNormalization()(x)\n",
    "x = keras.layers.Conv2D(filters=384, kernel_size=(1,1), strides=(1,1), activation='relu', padding=\"same\")(x)\n",
    "x = keras.layers.BatchNormalization()(x)\n",
    "x = keras.layers.Conv2D(filters=256, kernel_size=(1,1), strides=(1,1), activation='relu', padding=\"same\")(x)\n",
    "x = keras.layers.BatchNormalization()(x)\n",
    "x = keras.layers.MaxPool2D(pool_size=(3,3), strides=(2,2))(x)\n",
    "x = keras.layers.Flatten()(x)\n",
    "x = keras.layers.Dense(4096, activation='relu')(x)\n",
    "x = keras.layers.Dropout(0.5)(x)\n",
    "\n",
    "# ### first branch\n",
    "# branchLayer = keras.layers.Flatten(name=tf.compat.v1.get_default_graph().unique_name(\"branch_flatten\"))(x)\n",
    "# branchLayer = keras.layers.Dense(124, activation=\"relu\",name=tf.compat.v1.get_default_graph().unique_name(\"branch124\"))(branchLayer)\n",
    "# branchLayer = keras.layers.Dense(64, activation=\"relu\",name=tf.compat.v1.get_default_graph().unique_name(\"branch64\"))(branchLayer)\n",
    "# branchLayer = keras.layers.Dense(10, name=tf.compat.v1.get_default_graph().unique_name(\"branch_output\"))(branchLayer)\n",
    "\n",
    "x = keras.layers.Dense(4096, activation='relu')(x)\n",
    "x = keras.layers.Dropout(0.5)(x)\n",
    "\n",
    "# ### second Branch\n",
    "# branchLayer2 = keras.layers.Flatten(name=tf.compat.v1.get_default_graph().unique_name(\"branch_flatten\"))(x)\n",
    "# branchLayer2 = keras.layers.Dense(10, name=tf.compat.v1.get_default_graph().unique_name(\"branch_output\"))(branchLayer2)\n",
    "\n",
    "x = keras.layers.Dense(10, activation='softmax')(x)\n",
    "\n",
    "# model = keras.Model(inputs=inputs, outputs=[x,branchLayer,branchLayer2], name=\"alexnet\")\n",
    "teacher = keras.Model(inputs=(inputs), outputs=[x], name=\"alexnet\")\n",
    "\n",
    "teacher.compile(loss='categorical_crossentropy', optimizer=tf.optimizers.SGD(lr=0.001,momentum=0.9), metrics=['accuracy'])\n",
    "teacher.save(\"models/alexNetv6_new_teacher.hdf5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summarize_keras_trainable_variables(student_model,\"before training\")\n",
    "teacher.fit(train_ds, validation_data = validation_ds, epochs=30)\n",
    "summarize_keras_trainable_variables(student_model,\"after training\")\n",
    "teacher.save(\"models/alexNetv6_new_teacher.hdf5\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py36",
   "language": "python",
   "name": "py36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
