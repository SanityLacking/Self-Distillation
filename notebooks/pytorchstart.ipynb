{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.distributions import Dirichlet\n",
    "import math\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 2, 3, 4, 5],\n",
      "        [0, 0, 1, 2, 3]])\n",
      "tensor([15,  6])\n",
      "tensor([[15, 15, 15, 15, 15],\n",
      "        [ 6,  6,  6,  6,  6]])\n"
     ]
    }
   ],
   "source": [
    "alpha = torch.tensor([[1,2,3,4,5],[0,0,1,2,3]])\n",
    "print(alpha)\n",
    "print(alpha.sum(1))\n",
    "print(alpha.sum(1).unsqueeze(-1).repeat(1,5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.2000, 6.1000, 1.1000, 1.1000, 1.1000])\n",
      "tensor(11.5703) tensor([1.1814, 6.1812, 1.4026, 1.4026, 1.4026])\n",
      "tensor(35.1154) tensor([6.2611, 8.7620, 6.6974, 6.6974, 6.6974])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def dirichlet_kl_divergence(\n",
    "    alphas, target_alphas, precision=None, target_precision=None, epsilon=1e-8\n",
    "):\n",
    "    \"\"\"\n",
    "    This function computes the Forward KL divergence between a model Dirichlet distribution\n",
    "    and a target Dirichlet distribution based on the concentration (alpha) parameters of each.\n",
    "\n",
    "    :param alphas: Tensor containing concentation parameters of model. Expected shape is batchsize X num_classes.\n",
    "    :param target_alphas: Tensor containing target concentation parameters. Expected shape is batchsize X num_classes.\n",
    "    :param precision: Optional argument. Can pass in precision of model. Expected shape is batchsize X 1\n",
    "    :param target_precision: Optional argument. Can pass in target precision. Expected shape is batchsize X 1\n",
    "    :param epsilon: Smoothing factor for numercal stability. Default value is 1e-8\n",
    "    :return: Tensor for Batchsize X 1 of forward KL divergences between target Dirichlet and model\n",
    "    \"\"\"\n",
    "    if not precision:\n",
    "        precision = torch.sum(alphas, dim=1, keepdim=True)\n",
    "    if not target_precision:\n",
    "        target_precision = torch.sum(target_alphas, dim=1, keepdim=True)\n",
    "    precision_term = torch.lgamma(target_precision) - torch.lgamma(precision)\n",
    "    assert torch.all(torch.isfinite(precision_term)).item()\n",
    "    alphas_term = torch.sum(\n",
    "        torch.lgamma(alphas + epsilon)\n",
    "        - torch.lgamma(target_alphas + epsilon)\n",
    "        + (target_alphas - alphas)\n",
    "        * (\n",
    "            torch.digamma(target_alphas + epsilon)\n",
    "            - torch.digamma(target_precision + epsilon)\n",
    "        ),\n",
    "        dim=1,\n",
    "        keepdim=True,\n",
    "    )\n",
    "    # print(alphas_term)\n",
    "    assert torch.all(torch.isfinite(alphas_term)).item()\n",
    "\n",
    "    cost = torch.squeeze(precision_term + alphas_term)\n",
    "    return cost\n",
    "# alpha = torch.tensor([[1,2,3,4,5]])\n",
    "alpha = torch.tensor([[0.1,0.1,6,0.1,0.1,]])\n",
    "beta = torch.tensor([[1,1,1,1,1]])\n",
    "\n",
    "alpha = torch.tensor([[0.1,0.1],[6,0.1],[0.1,1],[0.1,1],[0.1,1]])\n",
    "beta = torch.tensor([[1,1],[1,1],[1,1],[1,1],[1,1]])\n",
    "\n",
    "print(torch.sum(alpha, 1))\n",
    "\n",
    "print(dirichlet_kl_divergence(alpha,beta).sum(), dirichlet_kl_divergence(alpha,beta))\n",
    "print(dirichlet_kl_divergence(beta,alpha).sum(), dirichlet_kl_divergence(beta,alpha))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alpha tf.Tensor(\n",
      "[[  2.105171    2.105171 ]\n",
      " [404.4288      2.105171 ]\n",
      " [  2.105171    3.7182817]\n",
      " [  2.105171    3.7182817]\n",
      " [  2.105171    3.7182817]], shape=(5, 2), dtype=float32)\n",
      "tc tf.Tensor(\n",
      "[[  2.210342 ]\n",
      " [404.53397  ]\n",
      " [  3.8234527]\n",
      " [  3.8234527]\n",
      " [  3.8234527]], shape=(5, 1), dtype=float32)\n",
      "ta tf.Tensor(\n",
      "[[  1.         3.210342]\n",
      " [405.53397    1.      ]\n",
      " [  4.823453   1.      ]\n",
      " [  1.         4.823453]\n",
      " [  1.         4.823453]], shape=(5, 2), dtype=float32)\n",
      "forward\n",
      "tf.Tensor(7.450365, shape=(), dtype=float32) tf.Tensor([0.94614553 0.68737984 4.136518   0.84016085 0.84016085], shape=(5,), dtype=float32)\n",
      "reverse\n",
      "tf.Tensor(5.895544, shape=(), dtype=float32) tf.Tensor([0.7998429  0.49340296 3.2784505  0.66192377 0.66192377], shape=(5,), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf \n",
    "import numpy as np\n",
    "def _KL(alphas, target_alphas,  precision=None, target_precision=None, epsilon=1e-8):\n",
    "    # print(\"K:\",K)\n",
    "    # beta=tf.constant(np.ones((1,K)),dtype=tf.float32)\n",
    "    target_alphas = tf.cast(target_alphas,tf.float32)\n",
    "    alphas = tf.cast(alphas,tf.float32)\n",
    "    # print(target_alphas)\n",
    "    if not precision:\n",
    "        precision = tf.reduce_sum(alphas, axis=1, keepdims=True)\n",
    "    if not target_precision:\n",
    "        target_precision = tf.reduce_sum(target_alphas, axis=1, keepdims=True)\n",
    "    precision = tf.cast(precision,tf.float32)\n",
    "    target_precision = tf.cast(target_precision,tf.float32)\n",
    "    \n",
    "    precision_term = tf.compat.v1.lgamma(target_precision) - tf.compat.v1.lgamma(precision)\n",
    "    # assert torch.all(torch.isfinite(precision_term)).item()\n",
    "    alphas_term = tf.reduce_sum(\n",
    "        tf.compat.v1.lgamma(alphas + epsilon)\n",
    "        - tf.compat.v1.lgamma(target_alphas + epsilon)\n",
    "        + (target_alphas - alphas)\n",
    "        * (\n",
    "            tf.compat.v1.digamma(target_alphas + epsilon)\n",
    "            - tf.compat.v1.digamma(target_precision + epsilon)\n",
    "        ),\n",
    "        axis=1,\n",
    "        keepdims=True,\n",
    "    )\n",
    "    # print(alphas_term)\n",
    "    # assert torch.all(torch.isfinite(alphas_term)).item()\n",
    "\n",
    "    cost = tf.squeeze(precision_term + alphas_term)\n",
    "    return cost\n",
    "\n",
    "def KL(alpha,K):\n",
    "    # print(\"K:\",K)\n",
    "    beta=tf.constant(np.ones((1,K)),dtype=tf.float32)\n",
    "\n",
    "    S_alpha = tf.reduce_sum(alpha,axis=1,keepdims=True)\n",
    "    S_beta = tf.reduce_sum(beta,axis=1,keepdims=True)\n",
    "    lnB = tf.compat.v1.lgamma(S_alpha) - tf.reduce_sum(tf.compat.v1.lgamma(alpha),axis=1,keepdims=True)\n",
    "    lnB_uni = tf.reduce_sum(tf.compat.v1.lgamma(beta),axis=1,keepdims=True) - tf.compat.v1.lgamma(S_beta)\n",
    "    \n",
    "    dg0 = tf.compat.v1.digamma(S_alpha)\n",
    "    dg1 = tf.compat.v1.digamma(alpha)\n",
    "    # tf.print(\"alpha\",alpha.shape)\n",
    "    # tf.print(\"beta\",beta.shape)\n",
    "    kl = tf.reduce_sum((alpha - beta)*(dg1-dg0),axis=1,keepdims=True) + lnB + lnB_uni\n",
    "    # print(\"kl\", kl)\n",
    "    return kl\n",
    "\n",
    "\n",
    "logits = torch.tensor([[0.1,1]])\n",
    "label = torch.tensor([[0,1]])\n",
    "alphas = torch.exp(logits)\n",
    "\n",
    "target_alphas = torch.empty_like(alphas, requires_grad=False).fill_(\n",
    "    1\n",
    ")\n",
    "# print(\"ta\", target_alphas)\n",
    "target_concentration = torch.sum(alphas, 1,keepdims=True)\n",
    "# print(\"tc\", target_concentration)\n",
    "# target_alphas[torch.tensor([0]), label] = target_concentration\n",
    "target_alphas = target_alphas + (target_concentration * label)\n",
    "\n",
    "# #alphafix\n",
    "# alphas = alphas+1\n",
    "# print(\"alphas\",alphas)\n",
    "# print(\"ta\", target_alphas)\n",
    "# print(\"forward\")\n",
    "# print(dirichlet_kl_divergence(alphas, target_alphas).sum(), dirichlet_kl_divergence(alphas, target_alphas))\n",
    "# print(\"backward\")\n",
    "# print(dirichlet_kl_divergence(target_alphas, alphas).sum(), dirichlet_kl_divergence(target_alphas, alphas))\n",
    "\n",
    "\n",
    "logits = tf.Variable([[0.1,0.1],[6,0.1],[0.1,1],[0.1,1],[0.1,1]])\n",
    "alpha = tf.exp(logits)\n",
    "labels = tf.Variable([[0,1],[1,0],[1,0],[0,1],[0,1]],dtype=tf.float32)\n",
    "\n",
    "\n",
    "# logits = torch.tensor([[0.1,1]])\n",
    "# labels = torch.tensor([[0,1]])\n",
    "# alphas = torch.exp(logits)\n",
    "\n",
    "\n",
    "target_concentration = tf.reduce_sum(alpha,axis=1,keepdims=True)\n",
    "target_alphas = (tf.ones_like(alpha) + (target_concentration * labels))\n",
    "\n",
    "alpha = alpha + 1\n",
    "print(\"alpha\",alpha)\n",
    "print(\"tc\",target_concentration)\n",
    "print(\"ta\",target_alphas)\n",
    "# if tf.reduce_any(tf.math.is_nan(logits)):\n",
    "    # tf.print(\"NaN val in KLloss\",logits)\n",
    "# inverse_labels = tf.math.abs(labels - 1)\n",
    "\n",
    "# beta = target_alphas #+ inverse_labels\n",
    "\n",
    "# # beta=tf.Variable(tf.ones_like(alpha),dtype=tf.float32)\n",
    "print(\"forward\")\n",
    "print(tf.reduce_sum(_KL(alpha,target_alphas)),_KL(alpha,target_alphas))\n",
    "print(\"reverse\")\n",
    "print(tf.reduce_sum(_KL(target_alphas,alpha)),_KL(target_alphas,alpha))\n",
    "# print(\"Old\")\n",
    "# print(tf.reduce_sum(KL(alpha,2)),KL(alpha,2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alpha tf.Tensor(\n",
      "[[  2.105171    2.105171 ]\n",
      " [404.4288      2.105171 ]\n",
      " [  2.105171    3.7182817]\n",
      " [  2.105171    3.7182817]\n",
      " [  2.105171    3.7182817]], shape=(5, 2), dtype=float32)\n",
      "tf.Tensor(\n",
      "[[2.105171  1.       ]\n",
      " [1.        2.105171 ]\n",
      " [1.        3.7182817]\n",
      " [2.105171  1.       ]\n",
      " [2.105171  1.       ]], shape=(5, 2), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "logits = tf.Variable([[0.1,0.1],[6,0.1],[0.1,1],[0.1,1],[0.1,1]])\n",
    "labels = tf.Variable([[0,1],[1,0],[1,0],[0,1],[0,1]],dtype=tf.float32)\n",
    "\n",
    "evidence = tf.exp(logits)\n",
    "alpha = evidence + 1\n",
    "print(\"alpha\",alpha)\n",
    "p = labels\n",
    "S = tf.reduce_sum(alpha,axis=1,keepdims=True) \n",
    "E = alpha - 1\n",
    "m = alpha / S\n",
    "A = tf.reduce_sum((p-m)**2, axis=1, keepdims=True) \n",
    "B = tf.reduce_sum(alpha*(S-alpha)/(S*S*(S+1)), axis=1, keepdims=True) \n",
    "# tf.print(annealing_coef)\n",
    "# annealing_coef = \n",
    "\n",
    "alp = evidence*(1-p) + 1 \n",
    "print(alp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alphas tensor([[  2.1052,   2.1052],\n",
      "        [404.4288,   2.1052],\n",
      "        [  2.1052,   3.7183],\n",
      "        [  2.1052,   3.7183],\n",
      "        [  2.1052,   3.7183]])\n",
      "ta tensor([[  1.0000,   3.2103],\n",
      "        [405.5340,   1.0000],\n",
      "        [  4.8235,   1.0000],\n",
      "        [  1.0000,   4.8235],\n",
      "        [  1.0000,   4.8235]])\n",
      "forward\n",
      "tensor(7.4504) tensor([0.9461, 0.6874, 4.1365, 0.8402, 0.8402])\n",
      "backward\n",
      "tensor(5.8955) tensor([0.7998, 0.4934, 3.2785, 0.6619, 0.6619])\n"
     ]
    }
   ],
   "source": [
    "logits = torch.tensor([[0.1,0.1],[6,0.1],[0.1,1],[0.1,1],[0.1,1]])\n",
    "label = torch.tensor([[0,1],[1,0],[1,0],[0,1],[0,1]])\n",
    "\n",
    "\n",
    "# logits = torch.tensor([[0.1,20]])\n",
    "# label = torch.tensor([[0,1]])\n",
    "alphas = torch.exp(logits)\n",
    "\n",
    "target_alphas = torch.empty_like(alphas, requires_grad=False).fill_(\n",
    "    1\n",
    ")\n",
    "# print(\"ta\", target_alphas)\n",
    "target_concentration = torch.sum(alphas, 1,keepdims=True)\n",
    "# print(\"tc\", target_concentration)\n",
    "# target_alphas[torch.tensor([0]), label] = target_concentration\n",
    "target_alphas = target_alphas + (target_concentration * label)\n",
    "\n",
    "#alphafix\n",
    "alphas = alphas+1\n",
    "print(\"alphas\",alphas)\n",
    "print(\"ta\", target_alphas)\n",
    "print(\"forward\")\n",
    "print(dirichlet_kl_divergence(alphas, target_alphas).sum(), dirichlet_kl_divergence(alphas, target_alphas))\n",
    "print(\"backward\")\n",
    "print(dirichlet_kl_divergence(target_alphas, alphas).sum(), dirichlet_kl_divergence(target_alphas, alphas))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=float32, numpy=7.023075>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow_probability as tfp\n",
    "-tf.reduce_mean(tfp.distributions.Dirichlet(alpha).entropy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _KL_old(alpha, beta, K=5 ):\n",
    "    # print(\"K:\",K)\n",
    "    # beta=tf.constant(np.)),dtype=tf.float32)\n",
    "    beta = tf.ones_like(alpha)\n",
    "    # beta = tf.cast(beta,tf.float32)\n",
    "    print(\"beta\",beta)\n",
    "    alpha = tf.cast(alpha,tf.float32)\n",
    "    S_alpha = tf.reduce_sum(alpha,axis=1,keepdims=True)\n",
    "    S_beta = tf.reduce_sum(beta,axis=1,keepdims=True)\n",
    "    lnB = tf.compat.v1.lgamma(S_alpha) - tf.reduce_sum(tf.compat.v1.lgamma(alpha),axis=1,keepdims=True)\n",
    "    lnB_uni = tf.reduce_sum(tf.compat.v1.lgamma(beta),axis=1,keepdims=True) - tf.compat.v1.lgamma(S_beta)\n",
    "    \n",
    "    dg0 = tf.compat.v1.digamma(S_alpha)\n",
    "    dg1 = tf.compat.v1.digamma(alpha)\n",
    "    # tf.print(\"alpha\",alpha.shape)\n",
    "    # tf.print(\"beta\",beta.shape)\n",
    "    kl = tf.reduce_sum((alpha - beta)*(dg1-dg0),axis=1,keepdims=True) + lnB + lnB_uni\n",
    "    # print(\"kl\", kl)\n",
    "    return kl\n",
    "# alpha = tf.Variable([[0.1,0.1],[6,0.1],[0.1,1],[0.1,1],[0.1,1]])\n",
    "# beta = tf.Variable([[0,1],[0,1],[1,0],[0,1],[0,1]],dtype=tf.float32)\n",
    "alpha = tf.Variable([[0.1,0.1],[6,0.1],[0.1,1],[0.1,1],[0.1,1]])\n",
    "beta = tf.Variable([[0,1],[0,1],[1,0],[0,1],[0,1]],dtype=tf.float32)\n",
    "target_concentration = tf.reduce_sum(alpha,axis=1,keepdims=True)\n",
    "if tf.reduce_any(tf.math.is_nan(logits)):\n",
    "    tf.print(\"NaN val in KLloss\",logits)\n",
    "target_alphas = (tf.ones_like(logits)* target_concentration) * labels\n",
    "inverse_labels = tf.math.abs(labels - 1)\n",
    "\n",
    "beta = target_alphas #+ inverse_labels\n",
    "# beta=tf.constant(np.ones((1,2)),dtype=tf.float32)\n",
    "print(beta)\n",
    "print(tf.reduce_sum(_KL_old(alpha,beta,2)),_KL_old(alpha,beta,2))\n",
    "print(\"----\")\n",
    "print(tf.reduce_sum(_KL_old(beta,alpha,2)),_KL_old(beta,alpha,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([6.4000, 7.4000, 6.4000])\n",
      "tensor([0, 1])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "shape mismatch: value tensor of shape [3] cannot be broadcast to indexing result of shape [1]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_1116/2503149265.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlabel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlabel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m     \u001b[0mtarget_alphas\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mlabel\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtarget_concentration\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     14\u001b[0m \u001b[1;31m# target_alphas[torch.arange(len(label)),label] = target_concentration\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtarget_alphas\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: shape mismatch: value tensor of shape [3] cannot be broadcast to indexing result of shape [1]"
     ]
    }
   ],
   "source": [
    "alphas = torch.tensor([[0.1,0.1,6,0.1,0.1],[0.1,0.1,7,0.1,0.1],[0.1,0.1,6,0.1,0.1]])\n",
    "# beta = torch.tensor([[1,1,1,1,1]])\n",
    "target_concentration = torch.sum(alphas,1)\n",
    "print(target_concentration)\n",
    "label = torch.tensor([[1,0,0,0,0],[0,1,0,0,0],[0,0,1,0,0]])\n",
    "label = torch.tensor([[3],[0]])\n",
    "\n",
    "target_alphas = torch.empty_like(alphas, requires_grad=False).fill_(\n",
    "    1\n",
    ")\n",
    "print(torch.arange(len(label)))\n",
    "for i in torch.arange(len(label)):\n",
    "    target_alphas[i][label[i]] = target_concentration \n",
    "# target_alphas[torch.arange(len(label)),label] = target_concentration\n",
    "print(target_alphas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[1. 0.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [1. 0.]], shape=(5, 2), dtype=float32)\n",
      "tf.Tensor(\n",
      "[[1.  0.2]\n",
      " [1.  6.1]\n",
      " [1.1 1. ]\n",
      " [1.  1.1]\n",
      " [1.  1.1]], shape=(5, 2), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "alpha = tf.Variable([[0.1,0.1],[6,0.1],[0.1,1],[0.1,1],[0.1,1]])\n",
    "beta = tf.Variable([[0,1],[0,1],[1,0],[0,1],[0,1]],dtype=tf.float32)\n",
    "reverse_beta = tf.math.abs(beta - 1)\n",
    "\n",
    "# print(tf.reduce_sum(alpha,axis=1,keepdims=True))\n",
    "# print(tf.ones_like(alpha))\n",
    "target_concentration = (tf.ones_like(alpha)* tf.reduce_sum(alpha,axis=1,keepdims=True)) * beta\n",
    "# target_concentration = tf.fill(alpha.shape, tf.reduce_sum(alpha)) * beta\n",
    "print(reverse_beta)\n",
    "TS = tf.cast(reverse_beta,tf.float32) + tf.cast(target_concentration,tf.float32)\n",
    "print(TS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5, 2)\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "print(alpha.shape)\n",
    "print(alpha.shape[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.1000, 0.1000],\n",
      "        [6.0000, 0.1000],\n",
      "        [0.1000, 1.0000],\n",
      "        [0.1000, 1.0000],\n",
      "        [0.1000, 1.0000]])\n",
      "tensor([[0.1000, 0.1000],\n",
      "        [6.0000, 0.1000],\n",
      "        [0.1000, 1.0000],\n",
      "        [0.1000, 1.0000],\n",
      "        [0.1000, 1.0000]])\n",
      "tensor([0.1414, 6.0008, 1.0050, 1.0050, 1.0050])\n",
      "tensor([0.1414, 6.0008, 1.0050, 1.0050, 1.0050])\n"
     ]
    }
   ],
   "source": [
    "alpha = torch.tensor([[0.1,0.1],[6,0.1],[0.1,1],[0.1,1],[0.1,1]])\n",
    "print(alpha)\n",
    "print(alpha.flatten(start_dim=1))\n",
    "print(alpha.flatten(start_dim=1).norm(2,1))\n",
    "print(alpha.norm(2,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tf.Variable 'Variable:0' shape=(5, 2) dtype=float32, numpy=\n",
      "array([[0.1, 0.1],\n",
      "       [6. , 0.1],\n",
      "       [0.1, 1. ],\n",
      "       [0.1, 1. ],\n",
      "       [0.1, 1. ]], dtype=float32)>\n",
      "tf.Tensor(\n",
      "[[0.1 0.1]\n",
      " [6.  0.1]\n",
      " [0.1 1. ]\n",
      " [0.1 1. ]\n",
      " [0.1 1. ]], shape=(5, 2), dtype=float32)\n",
      "tf.Tensor([0.14142136 6.000833   1.0049876  1.0049876  1.0049876 ], shape=(5,), dtype=float32)\n",
      "tf.Tensor([0.14142136 6.000833   1.0049876  1.0049876  1.0049876 ], shape=(5,), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "alpha = tf.Variable([[0.1,0.1],[6,0.1],[0.1,1],[0.1,1],[0.1,1]])\n",
    "print(alpha)\n",
    "print(tf.squeeze(alpha))\n",
    "print(tf.norm(tf.squeeze(alpha),axis=1))\n",
    "print(tf.norm(alpha,axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tf.Variable 'Variable:0' shape=() dtype=float32, numpy=nan>\n",
      "tf.Tensor(\n",
      "[[False False]\n",
      " [False False]\n",
      " [False False]\n",
      " [False False]\n",
      " [False False]], shape=(5, 2), dtype=bool)\n",
      "tf.Tensor(False, shape=(), dtype=bool)\n"
     ]
    }
   ],
   "source": [
    "alpha = tf.Variable([[0.1,0.1],[6,0.1],[0.1,1],[0.1,1],[0.1,1]])\n",
    "x = tf.Variable( float(\"NaN\"))\n",
    "print(x)\n",
    "print(tf.math.is_nan(alpha))\n",
    "print(tf.reduce_any(tf.math.is_nan(alpha)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz to ./data\\FashionMNIST\\raw\\train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f0a1087f275491094d086f1bccd207a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/26421880 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data\\FashionMNIST\\raw\\train-images-idx3-ubyte.gz to ./data\\FashionMNIST\\raw\n",
      "\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz to ./data\\FashionMNIST\\raw\\train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c0fd25ed925c4f74bbd070a5e20693a1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/29515 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data\\FashionMNIST\\raw\\train-labels-idx1-ubyte.gz to ./data\\FashionMNIST\\raw\n",
      "\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz to ./data\\FashionMNIST\\raw\\t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b766f076f8c9471d95b4d39782b97966",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4422102 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data\\FashionMNIST\\raw\\t10k-images-idx3-ubyte.gz to ./data\\FashionMNIST\\raw\n",
      "\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz to ./data\\FashionMNIST\\raw\\t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ba079d55d57d45cbbcd533e5930059a9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5148 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data\\FashionMNIST\\raw\\t10k-labels-idx1-ubyte.gz to ./data\\FashionMNIST\\raw\n",
      "\n",
      "Training set has 60000 instances\n",
      "Validation set has 10000 instances\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "# PyTorch TensorBoard support\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))])\n",
    "\n",
    "# Create datasets for training & validation, download if necessary\n",
    "training_set = torchvision.datasets.FashionMNIST('./data', train=True, transform=transform, download=True)\n",
    "validation_set = torchvision.datasets.FashionMNIST('./data', train=False, transform=transform, download=True)\n",
    "\n",
    "# Create data loaders for our datasets; shuffle for training, not for validation\n",
    "training_loader = torch.utils.data.DataLoader(training_set, batch_size=4, shuffle=True, num_workers=2)\n",
    "validation_loader = torch.utils.data.DataLoader(validation_set, batch_size=4, shuffle=False, num_workers=2)\n",
    "\n",
    "# Class labels\n",
    "classes = ('T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat',\n",
    "        'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle Boot')\n",
    "\n",
    "# Report split sizes\n",
    "print('Training set has {} instances'.format(len(training_set)))\n",
    "print('Validation set has {} instances'.format(len(validation_set)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dir tf.Tensor(0.002861670235601034, shape=(), dtype=float64)\n",
      "tf tf.Tensor(0.2409108756761404, shape=(), dtype=float64)\n",
      "----\n",
      "dir tensor(0.0003, dtype=torch.float64)\n",
      "torch tensor(0.2383, dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.distributions import Dirichlet\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "sys.path.append(\"..\") # Adds higher directory to python modules path.\n",
    "import os\n",
    "import numpy as np\n",
    "import sys\n",
    "import os\n",
    "import pickle\n",
    "import argparse\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torchvision.transforms as trn\n",
    "import torchvision.datasets as dset\n",
    "import torch.nn.functional as F\n",
    "from skimage.filters import gaussian as gblur\n",
    "from PIL import Image as PILImage\n",
    "# from uncertainty_est.models.ebm.mcmc import MCMC\n",
    "# from uncertainty_est.models.priornet.dpn_losses import UnfixedDirichletKLLoss\n",
    "# from uncertainty_est.models.priornet.uncertainties import (\n",
    "#     dirichlet_prior_network_uncertainty,\n",
    "# )\n",
    "def classifier_loss(ld_logits, y_l, lg_logits = None):\n",
    "        alpha = torch.exp(ld_logits)  # / self.p_y.unsqueeze(0).to(self.device)\n",
    "        # evidence = exp_evidence(logits)\n",
    "        # print(alpha)\n",
    "        # if self.alpha_fix:\n",
    "        alpha = alpha + 1\n",
    "         # alpha = evidence + 1\n",
    "        # print(alpha)\n",
    "        # soft_output = F.one_hot(y_l, len(y_l))\n",
    "        soft_output = y_l\n",
    "        #get logits/Y\n",
    "        # print(soft_output)\n",
    "        alpha_0 = alpha.sum(1).unsqueeze(-1).repeat(1, len(y_l))\n",
    "        #S = tf.reduce_sum(alpha,axis=1,keepdims=True) \n",
    "        #expand sum of alpha to size of classes which is unnessecary\n",
    "        # print(alpha_0)\n",
    "        # print(torch.digamma(alpha_0))\n",
    "        # print(torch.digamma(alpha))\n",
    "        UCE_loss = torch.mean(\n",
    "            soft_output * (torch.digamma(alpha_0) - torch.digamma(alpha))\n",
    "        )\n",
    "        dirchlet_weight = 0.0001 * -Dirichlet(alpha).entropy().mean()\n",
    "        print(\"dir\",dirchlet_weight)\n",
    "        ### this is the \n",
    "        UCE_loss = (\n",
    "            UCE_loss + dirchlet_weight\n",
    "        )\n",
    "        # self.log(\"train/clf_loss\", UCE_loss)\n",
    "\n",
    "        return UCE_loss\n",
    "import tensorflow_probability as tfp\n",
    "def classifier_loss_tf(labels, logits):\n",
    "    # alpha = torch.exp(ld_logits)  # / self.p_y.unsqueeze(0).to(self.device)\n",
    "    \n",
    "    evidence = tf.exp(logits)\n",
    "    # print(evidence)\n",
    "    # evidence = exp_evidence(logits)\n",
    "\n",
    "    # if self.alpha_fix:\n",
    "        # alpha = alpha + 1\n",
    "    alpha = evidence + 1\n",
    "    # print(alpha)\n",
    "    # soft_output = F.one_hot(y_l, self.n_classes)\n",
    "    #get lables/Y\n",
    "    # print(labels)/\n",
    "    # alpha_0 = alpha.sum(1).unsqueeze(-1).repeat(1, len(labels))\n",
    "    S = tf.reduce_sum(alpha,keepdims=True)\n",
    "    #expand sum of alpha to size of classes which is unnessecary\n",
    "    # print(S)\n",
    "    # print(tf.compat.v1.digamma(S) )\n",
    "    # print(tf.compat.v1.digamma(alpha) )\n",
    "    UCE_loss = tf.reduce_mean(\n",
    "        labels * (tf.compat.v1.digamma(S) - tf.compat.v1.digamma(alpha))\n",
    "    )\n",
    "    ### this is the \n",
    "    _alpha = alpha.numpy()\n",
    "    _alpha = torch.tensor(_alpha)\n",
    "    # dirichlet_weight = 0.0001 * -Dirichlet(_alpha).entropy().mean()\n",
    "    dirichlet_weight = 0.001 * tf.reduce_mean(-tfp.distributions.Dirichlet(alpha).entropy())\n",
    "    print(\"dir\",dirichlet_weight)\n",
    "    UCE_loss = (\n",
    "        UCE_loss + dirichlet_weight\n",
    "    )\n",
    "    # self.log(\"train/clf_loss\", UCE_loss)\n",
    "\n",
    "    return UCE_loss\n",
    "\n",
    "logits = np.array([[0.5,0.5,1.5,0.5]])\n",
    "labels =  np.array([0,0,1,0])\n",
    "print(\"tf\",classifier_loss_tf(labels, logits))\n",
    "print(\"----\")\n",
    "print(\"torch\",classifier_loss(torch.tensor(logits), torch.tensor(labels)))\n",
    "\n",
    "# S_alpha = tf.reduce_sum(alpha,keepdims=True)\n",
    "#     S_beta = tf.reduce_sum(beta,keepdims=True)\n",
    "#     lnB = tf.compat.v1.lgamma(S_alpha) - tf.reduce_sum(tf.compat.v1.lgamma(alpha),keepdims=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'training_loader' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_26500/2759885763.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     13\u001b[0m         \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnpimg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m \u001b[0mdataiter\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0miter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtraining_loader\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     16\u001b[0m \u001b[0mimages\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdataiter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'training_loader' is not defined"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Helper function for inline image display\n",
    "def matplotlib_imshow(img, one_channel=False):\n",
    "    if one_channel:\n",
    "        img = img.mean(dim=0)\n",
    "    img = img / 2 + 0.5     # unnormalize\n",
    "    npimg = img.numpy()\n",
    "    if one_channel:\n",
    "        plt.imshow(npimg, cmap=\"Greys\")\n",
    "    else:\n",
    "        plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "\n",
    "dataiter = iter(training_loader)\n",
    "images, labels = dataiter.next()\n",
    "\n",
    "# Create a grid from the images and show them\n",
    "img_grid = torchvision.utils.make_grid(images)\n",
    "matplotlib_imshow(img_grid, one_channel=True)\n",
    "print('  '.join(classes[labels[j]] for j in range(4)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# PyTorch models inherit from torch.nn.Module\n",
    "class GarmentClassifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(GarmentClassifier, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 6, 5)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        self.fc1 = nn.Linear(16 * 4 * 4, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = x.view(-1, 16 * 4 * 4)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "model = GarmentClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.7407, 0.2628, 0.1607, 0.2345, 0.9636, 0.6613, 0.5190, 0.5094, 0.7109,\n",
      "         0.1418],\n",
      "        [0.7736, 0.7108, 0.8648, 0.1413, 0.3945, 0.9406, 0.6282, 0.6684, 0.0633,\n",
      "         0.1361],\n",
      "        [0.0420, 0.9455, 0.6659, 0.9571, 0.2122, 0.6753, 0.6661, 0.7386, 0.3048,\n",
      "         0.7909],\n",
      "        [0.7384, 0.4126, 0.5886, 0.7754, 0.2548, 0.9817, 0.2804, 0.9997, 0.7108,\n",
      "         0.1492]])\n",
      "tensor([1, 5, 3, 7])\n",
      "Total loss for this batch: 2.1059627532958984\n",
      "evidence tensor([[2.0974, 1.3006, 1.1743, 1.2643, 2.6211, 1.9373, 1.6803, 1.6643, 2.0358,\n",
      "         1.1523],\n",
      "        [2.1676, 2.0356, 2.3745, 1.1518, 1.4836, 2.5615, 1.8742, 1.9511, 1.0653,\n",
      "         1.1458],\n",
      "        [1.0429, 2.5741, 1.9462, 2.6041, 1.2364, 1.9646, 1.9466, 2.0930, 1.3564,\n",
      "         2.2054],\n",
      "        [2.0926, 1.5107, 1.8015, 2.1715, 1.2902, 2.6690, 1.3237, 2.7175, 2.0356,\n",
      "         1.1609]])\n",
      "alpha tensor([[3.0974, 2.3006, 2.1743, 2.2643, 3.6211, 2.9373, 2.6803, 2.6643, 3.0358,\n",
      "         2.1523],\n",
      "        [3.1676, 3.0356, 3.3745, 2.1518, 2.4836, 3.5615, 2.8742, 2.9511, 2.0653,\n",
      "         2.1458],\n",
      "        [2.0429, 3.5741, 2.9462, 3.6041, 2.2364, 2.9646, 2.9466, 3.0930, 2.3564,\n",
      "         3.2054],\n",
      "        [3.0926, 2.5107, 2.8015, 3.1715, 2.2902, 3.6690, 2.3237, 3.7175, 3.0356,\n",
      "         2.1609]])\n",
      "soft_output tensor([[0, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 1, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 1, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 1, 0, 0]])\n",
      "sum tensor([26.9278, 27.8111, 28.9698, 28.7731])\n",
      "alpha_0 tensor([[26.9278, 26.9278, 26.9278, 26.9278, 26.9278, 26.9278, 26.9278, 26.9278,\n",
      "         26.9278, 26.9278],\n",
      "        [27.8111, 27.8111, 27.8111, 27.8111, 27.8111, 27.8111, 27.8111, 27.8111,\n",
      "         27.8111, 27.8111],\n",
      "        [28.9698, 28.9698, 28.9698, 28.9698, 28.9698, 28.9698, 28.9698, 28.9698,\n",
      "         28.9698, 28.9698],\n",
      "        [28.7731, 28.7731, 28.7731, 28.7731, 28.7731, 28.7731, 28.7731, 28.7731,\n",
      "         28.7731, 28.7731]])\n",
      "UCE_loss_1 tensor(0.2310)\n",
      "tensor(0.0015)\n",
      "UCE_loss_3 tensor(0.2325)\n",
      "Total loss for this batch: 0.23248888552188873\n"
     ]
    }
   ],
   "source": [
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "dummy_outputs = torch.tensor([[0.7407, 0.2628, 0.1607, 0.2345, 0.9636, 0.6613, 0.5190, 0.5094, 0.7109,\n",
    "         0.1418],\n",
    "        [0.7736, 0.7108, 0.8648, 0.1413, 0.3945, 0.9406, 0.6282, 0.6684, 0.0633,\n",
    "         0.1361],\n",
    "        [0.0420, 0.9455, 0.6659, 0.9571, 0.2122, 0.6753, 0.6661, 0.7386, 0.3048,\n",
    "         0.7909],\n",
    "        [0.7384, 0.4126, 0.5886, 0.7754, 0.2548, 0.9817, 0.2804, 0.9997, 0.7108,\n",
    "         0.1492]])\n",
    "dummy_labels = torch.tensor([1, 5, 3, 7])\n",
    "# NB: Loss functions expect data in batches, so we're creating batches of 4\n",
    "# Represents the model's confidence in each of the 10 classes for a given input\n",
    "# dummy_outputs = torch.rand(4, 10)\n",
    "# Represents the correct class among the 10 being tested\n",
    "# dummy_labels = torch.tensor([1, 5, 3, 7])\n",
    "\n",
    "print(dummy_outputs)\n",
    "print(dummy_labels)\n",
    "\n",
    "loss = loss_fn(dummy_outputs, dummy_labels)\n",
    "print('Total loss for this batch: {}'.format(loss.item()))\n",
    "\n",
    "\n",
    "\n",
    "from torch.distributions import Dirichlet\n",
    "def classifier_loss(ld_logits, y_l):\n",
    "        n_classes=10\n",
    "        alpha = torch.exp(ld_logits)  # / self.p_y.unsqueeze(0).to(self.device)\n",
    "        # Multiply by class counts for Bayesian update\n",
    "        print(\"evidence\",alpha)\n",
    "        # if self.alpha_fix:\n",
    "        alpha = alpha + 1\n",
    "        print(\"alpha\",alpha)\n",
    "        soft_output = F.one_hot(y_l, n_classes)\n",
    "        print(\"soft_output\",soft_output)\n",
    "        print(\"sum\",alpha.sum(1))\n",
    "        alpha_0 = alpha.sum(1).unsqueeze(-1).repeat(1, n_classes)\n",
    "        print(\"alpha_0\",alpha_0)\n",
    "        UCE_loss = torch.mean(\n",
    "            soft_output * (torch.digamma(alpha_0) - torch.digamma(alpha))\n",
    "        )\n",
    "        print(\"UCE_loss_1\",UCE_loss)\n",
    "        dirichlet_weight = 0.0001 * -Dirichlet(alpha).entropy().mean()\n",
    "        print(dirichlet_weight)\n",
    "        \n",
    "        UCE_loss = (\n",
    "            UCE_loss + 0.0001 * -Dirichlet(alpha).entropy().mean()\n",
    "        )\n",
    "        print(\"UCE_loss_3\",UCE_loss)\n",
    "        # self.log(\"train/clf_loss\", UCE_loss)\n",
    "\n",
    "        return UCE_loss\n",
    "loss = classifier_loss(dummy_outputs, dummy_labels)\n",
    "print('Total loss for this batch: {}'.format(loss.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "dummy_outputs = tf.Variable([[0.7407, 0.2628, 0.1607, 0.2345, 0.9636, 0.6613, 0.5190, 0.5094, 0.7109,\n",
    "         0.1418],\n",
    "        [0.7736, 0.7108, 0.8648, 0.1413, 0.3945, 0.9406, 0.6282, 0.6684, 0.0633,\n",
    "         0.1361],\n",
    "        [0.0420, 0.9455, 0.6659, 0.9571, 0.2122, 0.6753, 0.6661, 0.7386, 0.3048,\n",
    "         0.7909],\n",
    "        [0.7384, 0.4126, 0.5886, 0.7754, 0.2548, 0.9817, 0.2804, 0.9997, 0.7108,\n",
    "         0.1492]],dtype=tf.float32)\n",
    "dummy_labels = tf.Variable([1, 5, 3, 7])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "soft_output tf.Tensor(\n",
      "[[0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]], shape=(4, 10), dtype=float32)\n",
      "UCE_loss_1 tf.Tensor(0.23098798, shape=(), dtype=float32)\n",
      "dr tf.Tensor(0.0015009035, shape=(), dtype=float32)\n",
      "Total loss for this batch: 0.23248888552188873\n"
     ]
    }
   ],
   "source": [
    "labels =tf.Variable([[0, 0, 0, 0, 0, 0, 0, 0, 0, 1]])\n",
    "outputs =tf.Variable([[70.5058365, 17.3287029, 6.09617901, 7.22885942, 26.5094051, 4.77077579, 41.5663223, 19.999897, 5.15687704, 21.2428246]],dtype=tf.float32)\n",
    "# evidence [[4.17153364e+30 2.98016243e-08 444.15744 1378.64917 3.0698109e-12 118.010757 1.12726083e+18 2.06136597e-09 173.621384 5.94782945e-10]]\n",
    "# S [[4.17153364e+30]]\n",
    "# UCE_loss_1 7.10830545\n",
    "# drweight -0\n",
    "\n",
    "\n",
    "import tensorflow_probability as tfp\n",
    "def classifier_loss_tf(labels, outputs):\n",
    "    n_classes=10\n",
    "    evidence = tf.exp(outputs)\n",
    "    # print(\"evidence\",evidence)\n",
    "    # evidence = exp_evidence(outputs)\n",
    "    alpha = evidence + 1\n",
    "    # print(alpha)\n",
    "    soft_output = tf.one_hot(labels, n_classes)\n",
    "    print(\"soft_output\",soft_output)\n",
    "    S = tf.reduce_sum(alpha,axis=1,keepdims=True)\n",
    "    # print(\"S\",S)\n",
    "    alpha_0 = S * tf.ones((outputs.shape[-1]))\n",
    "    # print(\"alpha_0\",alpha_0)\n",
    "    UCE_loss = tf.reduce_mean(\n",
    "        tf.cast(soft_output,dtype=tf.float32) * (tf.compat.v1.digamma(alpha_0) - tf.compat.v1.digamma(alpha))\n",
    "    )\n",
    "    print(\"UCE_loss_1\",UCE_loss)\n",
    "    dirichlet_weight = 0.0001 * tf.reduce_mean(-tfp.distributions.Dirichlet(alpha).entropy())\n",
    "    print(\"dr\",dirichlet_weight)\n",
    "    UCE_loss = (\n",
    "        UCE_loss + dirichlet_weight\n",
    "    )\n",
    "\n",
    "    return UCE_loss\n",
    "loss = classifier_loss_tf(dummy_labels,dummy_outputs)\n",
    "print('Total loss for this batch: {}'.format(loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimizers specified in the torch.optim package\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.001, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(epoch_index, tb_writer):\n",
    "    running_loss = 0.\n",
    "    last_loss = 0.\n",
    "\n",
    "    # Here, we use enumerate(training_loader) instead of\n",
    "    # iter(training_loader) so that we can track the batch\n",
    "    # index and do some intra-epoch reporting\n",
    "    for i, data in enumerate(training_loader):\n",
    "        # Every data instance is an input + label pair\n",
    "        inputs, labels = data\n",
    "\n",
    "        # Zero your gradients for every batch!\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Make predictions for this batch\n",
    "        outputs = model(inputs)\n",
    "\n",
    "        # Compute the loss and its gradients\n",
    "        loss = loss_fn(outputs, labels)\n",
    "        loss.backward()\n",
    "\n",
    "        # Adjust learning weights\n",
    "        optimizer.step()\n",
    "\n",
    "        # Gather data and report\n",
    "        running_loss += loss.item()\n",
    "        if i % 1000 == 999:\n",
    "            last_loss = running_loss / 1000 # loss per batch\n",
    "            print('  batch {} loss: {}'.format(i + 1, last_loss))\n",
    "            tb_x = epoch_index * len(training_loader) + i + 1\n",
    "            tb_writer.add_scalar('Loss/train', last_loss, tb_x)\n",
    "            running_loss = 0.\n",
    "\n",
    "    return last_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "    \n",
    "class TinyModel(torch.nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(TinyModel, self).__init__()\n",
    "\n",
    "        self.linear1 = torch.nn.Linear(100, 200)\n",
    "        self.activation = torch.nn.ReLU()\n",
    "        self.linear2 = torch.nn.Linear(200, 10)\n",
    "        self.softmax = torch.nn.Softmax()\n",
    "    \n",
    "    def classifier_loss(ld_logits, y_l):\n",
    "        n_classes=10\n",
    "        alpha = torch.exp(ld_logits)  # / self.p_y.unsqueeze(0).to(self.device)\n",
    "        # Multiply by class counts for Bayesian update\n",
    "\n",
    "        # if self.alpha_fix:\n",
    "        alpha = alpha + 1\n",
    "\n",
    "        soft_output = F.one_hot(y_l, n_classes)\n",
    "        alpha_0 = alpha.sum(1).unsqueeze(-1).repeat(1, self.n_classes)\n",
    "        UCE_loss = torch.mean(\n",
    "            soft_output * (torch.digamma(alpha_0) - torch.digamma(alpha))\n",
    "        )\n",
    "        UCE_loss = (\n",
    "            UCE_loss + 0.0001 * -Dirichlet(alpha).entropy().mean()\n",
    "        )\n",
    "        # self.log(\"train/clf_loss\", UCE_loss)\n",
    "\n",
    "        return UCE_loss\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.linear1(x)\n",
    "        x = self.activation(x)\n",
    "        x = self.linear2(x)\n",
    "        x = self.softmax(x)\n",
    "        return x\n",
    "\n",
    "tinymodel = TinyModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#inspired by https://github.com/selflein/MA-EBM/blob/1e90da47118127ab432d11b9e2b2aa57305386fc/uncertainty_est/models/ebm/mcmc_priornet.py\n",
    "def MAEBM():\n",
    "    def loss(labels, outputs):\n",
    "        # evidence = tf.exp(outputs) #activation function\n",
    "        evidence = exp_evidence(outputs)\n",
    "        alpha = evidence + 1\n",
    "        S = tf.reduce_sum(alpha)\n",
    "        # tf.print(S)\n",
    "        alpha_0 = tf.fill(alpha.shape,S)\n",
    "        # tf.print(alpha_0)\n",
    "        \n",
    "        # tf.print(alpha)\n",
    "        UCE_loss = tf.reduce_mean(labels * (tf.compat.v1.digamma(alpha_0) - tf.compat.v1.digamma(alpha)))\n",
    "        \n",
    "        UCE_loss  +=  0.0001  * (tf.reduce_sum(tf.compat.v1.lgamma(alpha)) - tf.compat.v1.lgamma(S) -\n",
    "                (len(alpha) - S) * tf.compat.v1.digamma(S) -\n",
    "                ((alpha - 1.0) * tf.reduce_sum(tf.compat.v1.digamma(alpha))))\n",
    "        # tf.print(UCE_loss)\n",
    "        return UCE_loss\n",
    "    return loss \n",
    "\n",
    "def classifier_loss(ld_logits, y_l, lg_logits):\n",
    "        alpha = torch.exp(ld_logits)  # / self.p_y.unsqueeze(0).to(self.device)\n",
    "        # Multiply by class counts for Bayesian update\n",
    "\n",
    "        # if self.alpha_fix:\n",
    "        alpha = alpha + 1\n",
    "        n_classes = 3\n",
    "        entropy_reg_weight = 0.0001\n",
    "        \n",
    "        soft_output = F.one_hot(y_l, n_classes)\n",
    "        print(\"labels\",soft_output)\n",
    "        alpha_0 = alpha.sum(1)\n",
    "        alpha_0= alpha_0.unsqueeze(-1).repeat(1, n_classes)\n",
    "        print(\"alpha_0\",alpha_0)\n",
    "        UCE_loss = torch.mean(\n",
    "            soft_output * (torch.digamma(alpha_0) - torch.digamma(alpha))\n",
    "        )\n",
    "        print(\"loss_1\",UCE_loss)\n",
    "        UCE_loss = (\n",
    "            UCE_loss + entropy_reg_weight * -Dirichlet(alpha).entropy().mean()\n",
    "        )\n",
    "        # self.log(\"train/clf_loss\", UCE_loss)\n",
    "\n",
    "        return UCE_loss\n",
    "\n",
    "def ebm():\n",
    "    def loss_fn(labels, outputs):\n",
    "        # evidence = tf.exp(outputs) #activation function\n",
    "        evidence = tf.nn.log_softmax(outputs)\n",
    "        loss  =tf.reduce_mean(tf.reduce_sum(-label * evidence)) \n",
    "        # loss += 0.0001 * \n",
    "\n",
    "        return loss\n",
    "        \n",
    "    return loss_fn \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "labels tensor([[0, 1, 0]])\n",
      "alpha_0 tensor([[6.3155, 6.3155, 6.3155]])\n",
      "loss_1 tensor(0.4244)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(0.4245)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alpha = torch.tensor([[.1,.1,.1]])\n",
    "classifier_loss(alpha,torch.tensor([1]),None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(13.0250)\n"
     ]
    }
   ],
   "source": [
    "alpha = np.array([.1,.1,.1])\n",
    "alpha = torch.tensor([.1,.1,.1])\n",
    "\n",
    "\n",
    "print(-Dirichlet(alpha).entropy().mean())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    net.train()  # enter train mode\n",
    "    loss_avg = 0.0\n",
    "\n",
    "    # start at a random point of the outlier dataset; this induces more randomness without obliterating locality\n",
    "    train_loader_out.dataset.offset = np.random.randint(len(train_loader_out.dataset))\n",
    "    for in_set, out_set in zip(train_loader_in, train_loader_out):\n",
    "        data = torch.cat((in_set[0], out_set[0]), 0) #dataset of the in distrib and out of distrib\n",
    "        target = in_set[1] # labels for in distrub\n",
    "\n",
    "        data, target = data.cuda(), target.cuda()\n",
    "\n",
    "        # forward\n",
    "        x = net(data) #predictions for all of data\n",
    "\n",
    "        # backward\n",
    "        scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        loss = F.cross_entropy(x[:len(in_set[0])], target) # loss of all in distribution inputs\n",
    "        # cross-entropy from softmax distribution to uniform distribution\n",
    "        if args.score == 'energy':\n",
    "            Ec_out = -torch.logsumexp(x[len(in_set[0]):], dim=1) # logsumexp of all of the OOD inputs\n",
    "            Ec_in = -torch.logsumexp(x[:len(in_set[0])], dim=1) # logsumexp of all of the IOD inputs\n",
    "            loss += 0.1*(torch.pow(F.relu(Ec_in-args.m_in), 2).mean() + torch.pow(F.relu(args.m_out-Ec_out), 2).mean()) #loss is equal to 0.1 times the pow2 of the relu of Energy of all IoD + the pow2 of the relu of Energy of all OOD\n",
    "                            '''\n",
    "                            so the IoD energy is reduced by 0.1\n",
    "                            each energy is run through a relu and is minus a threshold, the default is -25 and -7 for IoD and OOD.\n",
    "                            energy is measured as a negative number. \n",
    "                            \n",
    "                            \n",
    "                            '''\n",
    "        elif args.score == 'OE':\n",
    "            loss += 0.5 * -(x[len(in_set[0]):].mean(1) - torch.logsumexp(x[len(in_set[0]):], dim=1)).mean()\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # exponential moving average\n",
    "        loss_avg = loss_avg * 0.8 + float(loss) * 0.2\n",
    "    state['train_loss'] = loss_avg\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy\n",
    "from scipy.special import logsumexp\n",
    "import tensorflow as tf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-1.6546810021220524\n"
     ]
    }
   ],
   "source": [
    "x = np.array ([.54,.32,.76])\n",
    "Ec_in = -logsumexp(x)\n",
    "print(Ec_in)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "54.500391911268096\n"
     ]
    }
   ],
   "source": [
    "loss =0\n",
    "loss += 0.1*(pow(tf.nn.relu(Ec_in- (-25)), 2).numpy().mean()) #+ torch.pow(F.relu(args.m_out-Ec_out), 2).mean()) #loss is equal to 0.1 times the pow2 of the relu of Energy of all IoD + the pow2 of the relu of Energy of all OOD\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "from scipy.special import gammaln, digamma\n",
    "\n",
    "\n",
    "def dirichlet_prior_network_uncertainty(logits, epsilon=1e-10, alpha_correction=True):\n",
    "    \"\"\"\n",
    "    :param logits:\n",
    "    :param epsilon:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "\n",
    "    logits = np.asarray(logits, dtype=np.float64)\n",
    "    alphas = np.exp(logits)\n",
    "\n",
    "    alphas = np.clip(alphas, 0, np.finfo(np.dtype(\"float32\")).max)\n",
    "\n",
    "    if alpha_correction:\n",
    "        alphas = alphas + 1\n",
    "\n",
    "    alpha0 = np.sum(alphas, axis=1, keepdims=True)\n",
    "    probs = alphas / alpha0\n",
    "\n",
    "    conf = np.max(probs, axis=1)\n",
    "\n",
    "    entropy_of_exp = -np.sum(probs * np.log(probs + epsilon), axis=1)\n",
    "    expected_entropy = -np.sum(\n",
    "        (alphas / alpha0) * (digamma(alphas + 1) - digamma(alpha0 + 1.0)), axis=1\n",
    "    )\n",
    "\n",
    "    mutual_info = entropy_of_exp - expected_entropy\n",
    "\n",
    "    epkl = np.squeeze((alphas.shape[1] - 1.0) / alpha0)\n",
    "\n",
    "    dentropy = (\n",
    "        np.sum(\n",
    "            gammaln(alphas) - (alphas - 1.0) * (digamma(alphas) - digamma(alpha0)),\n",
    "            axis=1,\n",
    "            keepdims=True,\n",
    "        )\n",
    "        - gammaln(alpha0)\n",
    "    )\n",
    "\n",
    "    uncertainty = {\n",
    "        \"confidence_alea_uncert.\": conf,\n",
    "        \"entropy_of_expected\": -entropy_of_exp,\n",
    "        \"expected_entropy\": -expected_entropy,\n",
    "        \"mutual_information\": -mutual_info,\n",
    "        \"EPKL\": -epkl,\n",
    "        \"differential_entropy\": -np.squeeze(dentropy),\n",
    "    }\n",
    "    results = {}\n",
    "    for k,v in uncertainty.items():\n",
    "        if type(v) is type(np.array(0)):\n",
    "            results[k] = torch.from_numpy(v).float()\n",
    "        else: \n",
    "            results[k] = torch.from_numpy(np.array(v)).float()\n",
    "    # k: torch.from_numpy(v).float() for k, v in uncertainty.items()\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'confidence_alea_uncert.': tensor([0.1000]), 'entropy_of_expected': tensor([-2.3026]), 'expected_entropy': tensor([-2.1875]), 'mutual_information': tensor([-0.1151]), 'EPKL': tensor(-0.2420), 'differential_entropy': tensor(15.8416), 'p(x)': tensor([3.3026]), 'max p(y|x)': tensor([0.1000])}\n"
     ]
    }
   ],
   "source": [
    "dirichlet_prior_network_uncertainty([[1,1,1,1,1,1,1,1,1,1]])\n",
    "logits = torch.tensor([[1,1,1,1,1,1,1,1,1,1]])\n",
    "dir_uncert = dirichlet_prior_network_uncertainty(logits)\n",
    "dir_uncert[\"p(x)\"] = logits.logsumexp(1)\n",
    "dir_uncert[\"max p(y|x)\"] = logits.float().softmax(1).max(1).values\n",
    "print(dir_uncert)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#comparasion of tf and torch version of this function.\n",
    "x = {'confidence_alea_uncert.': tensor([0.1000]), \n",
    " 'entropy_of_expected': tensor([-2.3026]), \n",
    " 'expected_entropy': tensor([-2.1875]), \n",
    " 'mutual_information': tensor([-0.1151]), \n",
    " 'EPKL': tensor(-0.2420), \n",
    " 'differential_entropy': tensor(15.8416), \n",
    " 'p(x)': tensor([3.3026]), \n",
    " 'max p(y|x)': tensor([0.1000])}\n",
    "\n",
    "\n",
    "y = {'confidence_alea_uncert.': 0.09999999999999999, \n",
    " 'entropy_of_expected': -2.302585091994046, \n",
    " 'expected_entropy': -2.1874864443604523, \n",
    " 'mutual_information': -0.11509864763359356, \n",
    " 'EPKL': -0.2420472792329956, \n",
    " 'differential_entropy': 15.84162062462957, \n",
    " 'p(x)': <tf.Tensor: shape=(), dtype=float32, numpy=3.3025851>, \n",
    " 'max p(y|x)': 0.1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'logsumexp'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_2180/805850854.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     58\u001b[0m \u001b[0mlogits\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     59\u001b[0m \u001b[0mdir_uncert\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdirichlet_prior_network_uncertainty\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 60\u001b[1;33m \u001b[0mdir_uncert\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"p(x)\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlogits\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlogsumexp\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     61\u001b[0m \u001b[0mdir_uncert\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"max p(y|x)\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlogits\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdir_uncert\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'list' object has no attribute 'logsumexp'"
     ]
    }
   ],
   "source": [
    "\n",
    "import numpy as np\n",
    "from scipy.special import gammaln, digamma\n",
    "\n",
    "\n",
    "def dirichlet_prior_network_uncertainty(logits, epsilon=1e-10, alpha_correction=True):\n",
    "    \"\"\"\n",
    "    :param logits:\n",
    "    :param epsilon:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "\n",
    "    logits = np.asarray(logits, dtype=np.float64)\n",
    "    alphas = np.exp(logits)\n",
    "\n",
    "    alphas = np.clip(alphas, 0, np.finfo(np.dtype(\"float32\")).max)\n",
    "\n",
    "    if alpha_correction:\n",
    "        alphas = alphas + 1\n",
    "\n",
    "    alpha0 = np.sum(alphas, axis=1, keepdims=True)\n",
    "    probs = alphas / alpha0\n",
    "\n",
    "    conf = np.max(probs, axis=1)\n",
    "\n",
    "    entropy_of_exp = -np.sum(probs * np.log(probs + epsilon), axis=1)\n",
    "    expected_entropy = -np.sum(\n",
    "        (alphas / alpha0) * (digamma(alphas + 1) - digamma(alpha0 + 1.0)), axis=1\n",
    "    )\n",
    "\n",
    "    mutual_info = entropy_of_exp - expected_entropy\n",
    "\n",
    "    epkl = np.squeeze((alphas.shape[1] - 1.0) / alpha0)\n",
    "\n",
    "    dentropy = (\n",
    "        np.sum(\n",
    "            gammaln(alphas) - (alphas - 1.0) * (digamma(alphas) - digamma(alpha0)),\n",
    "            axis=1,\n",
    "            keepdims=True,\n",
    "        )\n",
    "        - gammaln(alpha0)\n",
    "    )\n",
    "\n",
    "    uncertainty = {\n",
    "        \"confidence_alea_uncert.\": conf,\n",
    "        \"entropy_of_expected\": -entropy_of_exp,\n",
    "        \"expected_entropy\": -expected_entropy,\n",
    "        \"mutual_information\": -mutual_info,\n",
    "        \"EPKL\": -epkl,\n",
    "        \"differential_entropy\": -np.squeeze(dentropy),\n",
    "    }\n",
    "    results = {}\n",
    "    \n",
    "    for k,v in uncertainty.items():\n",
    "            results[k] =v.mean()\n",
    "    # k: torch.from_numpy(v).float() for k, v in uncertainty.items()\n",
    "    return results\n",
    "# dirichlet_prior_network_uncertainty([[1,1,1,1,1,1,1,1,1,1]])\n",
    "logits = [[1,1,1,1,1,1,1,1,1,1]]\n",
    "dir_uncert = dirichlet_prior_network_uncertainty(logits)\n",
    "dir_uncert[\"p(x)\"] = logits.logsumexp(1)\n",
    "dir_uncert[\"max p(y|x)\"] = logits.softmax(1).max(1).values\n",
    "print(dir_uncert)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py36",
   "language": "python",
   "name": "py36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
