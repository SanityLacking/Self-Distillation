{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "# os.environ['CUDA_VISIBLE_DEVICES'] = '-1'\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, models\n",
    "os.environ['TF_DETERMINISTIC_OPS'] = '1'\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "import os\n",
    "sys.path.append(\"..\") # Adds higher directory to python modules path.\n",
    "# import branchingdnn as branching\n",
    "# dataset = branching.dataset.prepare.dataset(tf.keras.datasets.cifar10.load_data(),64,5000,22500,(227,227),include_targets=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 66\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "tf.random.set_seed(seed)\n",
    "\n",
    "# tf.debugging.experimental.enable_dump_debug_info(logdir, tensor_debug_mode=\"FULL_HEALTH\", circular_buffer_size=-1)\n",
    "(train_images, train_labels), (test_images, test_labels) = keras.datasets.cifar10.load_data()\n",
    "\n",
    "CLASS_NAMES= ['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n",
    "\n",
    "# import csv\n",
    "# with open('results/altTrain_labels.csv', newline='') as f:\n",
    "    # reader = csv.reader(f,quoting=csv.QUOTE_NONNUMERIC)\n",
    "    # alt_trainLabels = list(reader)\n",
    "# with open('results/altTest_labels.csv', newline='') as f:\n",
    "    # reader = csv.reader(f,quoting=csv.QUOTE_NONNUMERIC)\n",
    "    # alt_testLabels = list(reader)\n",
    "\n",
    "# altTraining = tf.data.Dataset.from_tensor_slices((train_images,alt_trainLabels))\n",
    "\n",
    "# validation_images, validation_labels = train_images[:5000], alt_trainLabels[:5000]\n",
    "# train_ds = tf.data.Dataset.from_tensor_slices((train_images, alt_trainLabels))\n",
    "# test_ds = tf.data.Dataset.from_tensor_slices((test_images, alt_testLabels))\n",
    "train_labels = tf.keras.utils.to_categorical(train_labels,10)\n",
    "test_labels = tf.keras.utils.to_categorical(test_labels,10)\n",
    "\n",
    "###normal method\n",
    "validation_images, validation_labels = train_images[:5000], train_labels[:5000] #get the first 5k training samples as validation set\n",
    "train_images, train_labels = train_images[5000:], train_labels[5000:] # now remove the validation set from the training set.\n",
    "train_ds = tf.data.Dataset.from_tensor_slices((train_images, train_labels))\n",
    "test_ds = tf.data.Dataset.from_tensor_slices((test_images, test_labels))\n",
    "validation_ds = tf.data.Dataset.from_tensor_slices((validation_images, validation_labels))\n",
    "\n",
    "def augment_images(image, label):\n",
    "    # Normalize images to have a mean of 0 and standard deviation of 1\n",
    "    # image = tf.image.per_image_standardization(image)\n",
    "    # Resize images from 32x32 to 277x277\n",
    "    image = tf.image.resize(image, (227,227))\n",
    "    return image, label\n",
    "\n",
    "train_ds_size = len(list(train_ds))\n",
    "test_ds_size = len(list(test_ds))\n",
    "validation_ds_size = len(list(validation_ds))\n",
    "\n",
    "train_ds = (train_ds\n",
    "                  .map(augment_images)\n",
    "                  .shuffle(buffer_size=train_ds_size,seed=42,reshuffle_each_iteration=False)\n",
    "                  .batch(batch_size=32, drop_remainder=True))\n",
    "\n",
    "test_ds = (test_ds\n",
    "                  .map(augment_images)\n",
    "                #   .shuffle(buffer_size=train_ds_size)\n",
    "                  .batch(batch_size=1, drop_remainder=True))\n",
    "\n",
    "validation_ds = (validation_ds\n",
    "                  .map(augment_images)\n",
    "                #   .shuffle(buffer_size=validation_ds_size)\n",
    "                  .batch(batch_size=32, drop_remainder=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# student_model = tf.keras.models.load_model(\"models/alexNetv6_logits_student.hdf5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"alexnet\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_4 (InputLayer)        [(None, 227, 227, 3)]     0         \n",
      "                                                                 \n",
      " conv2d_6 (Conv2D)           (None, 55, 55, 96)        34944     \n",
      "                                                                 \n",
      " batch_normalization_6 (Batc  (None, 55, 55, 96)       384       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " max_pooling2d_6 (MaxPooling  (None, 27, 27, 96)       0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_7 (Conv2D)           (None, 27, 27, 256)       614656    \n",
      "                                                                 \n",
      " batch_normalization_7 (Batc  (None, 27, 27, 256)      1024      \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " max_pooling2d_7 (MaxPooling  (None, 13, 13, 256)      0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " branch_flatten_3 (Flatten)  (None, 43264)             0         \n",
      "                                                                 \n",
      " branch124_3 (Dense)         (None, 124)               5364860   \n",
      "                                                                 \n",
      " branch64_3 (Dense)          (None, 64)                8000      \n",
      "                                                                 \n",
      " branch_output_3 (Dense)     (None, 10)                650       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 6,024,518\n",
      "Trainable params: 6,023,814\n",
      "Non-trainable params: 704\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "seed = 42\n",
    "# random.seed(seed)\n",
    "tf.random.set_seed(seed)\n",
    "np.random.seed(seed)\n",
    "inputs = keras.Input(shape=(227,227,3))\n",
    "# targets = keras.Input(shape=(10,))\n",
    "x = keras.layers.Conv2D(filters=96, kernel_size=(11,11), strides=(4,4), activation='relu', input_shape=(227,227,3))(inputs)\n",
    "x = keras.layers.BatchNormalization()(x)\n",
    "x = keras.layers.MaxPool2D(pool_size=(3,3), strides=(2,2))(x)\n",
    "x = keras.layers.Conv2D(filters=256, kernel_size=(5,5), strides=(1,1), activation='relu', padding=\"same\")(x)\n",
    "x = keras.layers.BatchNormalization()(x)\n",
    "x = keras.layers.MaxPool2D(pool_size=(3,3), strides=(2,2))(x)\n",
    "# x = keras.layers.Conv2D(filters=384, kernel_size=(3,3), strides=(1,1), activation='relu', padding=\"same\")(x)\n",
    "# x = keras.layers.BatchNormalization()(x)\n",
    "# x = keras.layers.Conv2D(filters=384, kernel_size=(1,1), strides=(1,1), activation='relu', padding=\"same\")(x)\n",
    "# x = keras.layers.BatchNormalization()(x)\n",
    "# x = keras.layers.Conv2D(filters=256, kernel_size=(1,1), strides=(1,1), activation='relu', padding=\"same\")(x)\n",
    "# x = keras.layers.BatchNormalization()(x)\n",
    "# x = keras.layers.MaxPool2D(pool_size=(3,3), strides=(2,2))(x)\n",
    "# x = keras.layers.Flatten()(x)\n",
    "# x = keras.layers.Dense(4096, activation='relu')(x)\n",
    "# x = keras.layers.Dropout(0.5)(x)\n",
    "\n",
    "# ### first branch\n",
    "branchLayer = keras.layers.Flatten(name=tf.compat.v1.get_default_graph().unique_name(\"branch_flatten\"))(x)\n",
    "branchLayer = keras.layers.Dense(124, activation=\"relu\",name=tf.compat.v1.get_default_graph().unique_name(\"branch124\"))(branchLayer)\n",
    "branchLayer = keras.layers.Dense(64, activation=\"relu\",name=tf.compat.v1.get_default_graph().unique_name(\"branch64\"))(branchLayer)\n",
    "x = keras.layers.Dense(10, activation=\"softmax\", name=tf.compat.v1.get_default_graph().unique_name(\"branch_output\"))(branchLayer)\n",
    "\n",
    "\n",
    "student_model = keras.Model(inputs=(inputs), outputs=[x], name=\"alexnet\")\n",
    "\n",
    "new_model = keras.Model(student_model.inputs, student_model.outputs, name=student_model.name)\n",
    "new_model.summary()\n",
    "\n",
    "# student_model.compile(loss='categorical_crossentropy', optimizer=tf.optimizers.SGD(lr=0.001,momentum=0.9), metrics=['accuracy'])\n",
    "# student_model.save(\"models/alexNetv6_second_Exit.hdf5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"alexnet\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 227, 227, 3)]     0         \n",
      "                                                                 \n",
      " conv2d (Conv2D)             (None, 55, 55, 96)        34944     \n",
      "                                                                 \n",
      " batch_normalization (BatchN  (None, 55, 55, 96)       384       \n",
      " ormalization)                                                   \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2D  (None, 27, 27, 96)       0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 27, 27, 256)       614656    \n",
      "                                                                 \n",
      " batch_normalization_1 (Batc  (None, 27, 27, 256)      1024      \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPooling  (None, 13, 13, 256)      0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " branch_flatten (Flatten)    (None, 43264)             0         \n",
      "                                                                 \n",
      " branch124 (Dense)           (None, 124)               5364860   \n",
      "                                                                 \n",
      " branch64 (Dense)            (None, 64)                8000      \n",
      "                                                                 \n",
      " branch_output (Dense)       (None, 10)                650       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 6,024,518\n",
      "Trainable params: 6,023,814\n",
      "Non-trainable params: 704\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "student_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import collections\n",
    "import copy\n",
    "import itertools\n",
    "import json\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import six\n",
    "from six.moves import zip  # pylint: disable=redefined-builtin\n",
    "\n",
    "# from tensorflow.python.distribute import distribute_coordinator as dc\n",
    "# from tensorflow.python.distribute import distribute_coordinator_context as dc_context\n",
    "# from tensorflow.python.distribute import distribution_strategy_context as ds_context\n",
    "# from tensorflow.python.distribute import parameter_server_strategy\n",
    "# from tensorflow.python.distribute import values as ds_values\n",
    "# from tensorflow.python.eager import backprop\n",
    "# from tensorflow.python.eager import context\n",
    "# from tensorflow.python.eager import def_function\n",
    "# from tensorflow.python.eager import monitoring\n",
    "# from tensorflow.python.framework import sparse_tensor\n",
    "# from tensorflow.python.keras import callbacks as callbacks_module\n",
    "# from tensorflow.python.keras import optimizers\n",
    "# from tensorflow.python.keras.distribute import distributed_training_utils as dist_utils\n",
    "# from tensorflow.python.keras.engine import compile_utils\n",
    "from tensorflow.python.keras.engine import data_adapter\n",
    "# from tensorflow.python.keras.engine import network\n",
    "# from tensorflow.python.keras.engine import training_utils\n",
    "# from tensorflow.python.keras.mixed_precision.experimental import loss_scale_optimizer as lso\n",
    "# from tensorflow.python.keras.saving.saved_model import model_serialization\n",
    "# from tensorflow.python.keras.utils import tf_utils\n",
    "# from tensorflow.python.keras.utils import version_utils\n",
    "# from tensorflow.python.keras.utils.mode_keys import ModeKeys\n",
    "# from tensorflow.python.ops import array_ops\n",
    "# from tensorflow.python.ops import sparse_ops\n",
    "# from tensorflow.python.ops.ragged import ragged_concat_ops\n",
    "# from tensorflow.python.ops.ragged import ragged_tensor\n",
    "# from tensorflow.python.profiler import traceme\n",
    "# from tensorflow.python.training.tracking import base as trackable\n",
    "# from tensorflow.python.util import deprecation\n",
    "# from tensorflow.python.util import nest\n",
    "# from tensorflow.python.util import tf_decorator\n",
    "# from tensorflow.python.util.tf_export import keras_export\n",
    "\n",
    "from tensorflow.python.eager import context\n",
    "from tensorflow.python.framework import composite_tensor\n",
    "from tensorflow.python.framework import constant_op\n",
    "from tensorflow.python.framework import errors\n",
    "from tensorflow.python.framework import errors_impl\n",
    "from tensorflow.python.framework import func_graph\n",
    "from tensorflow.python.framework import ops\n",
    "from tensorflow.python.framework import tensor_shape\n",
    "from tensorflow.python.keras import backend\n",
    "from tensorflow.python.keras.engine import base_layer\n",
    "from tensorflow.python.keras.engine import base_layer_utils\n",
    "from tensorflow.python.keras.engine import compile_utils\n",
    "from tensorflow.python.keras.engine import input_layer as input_layer_module\n",
    "from tensorflow.python.keras.engine import node as node_module\n",
    "from tensorflow.python.keras.engine import training_utils\n",
    "from tensorflow.python.keras.saving import hdf5_format\n",
    "from tensorflow.python.keras.saving import save\n",
    "from tensorflow.python.keras.saving.saved_model import network_serialization\n",
    "from tensorflow.python.keras.utils import generic_utils\n",
    "from tensorflow.python.keras.utils import layer_utils\n",
    "from tensorflow.python.keras.utils import tf_utils\n",
    "from tensorflow.python.keras.utils.io_utils import ask_to_proceed_with_overwrite\n",
    "from tensorflow.python.ops import array_ops\n",
    "from tensorflow.python.ops import math_ops\n",
    "from tensorflow.python.ops.ragged import ragged_tensor\n",
    "from tensorflow.python.platform import tf_logging as logging\n",
    "from tensorflow.python.training import checkpoint_management\n",
    "from tensorflow.python.training import py_checkpoint_reader\n",
    "from tensorflow.python.training.tracking import base as trackable\n",
    "from tensorflow.python.training.tracking import data_structures\n",
    "from tensorflow.python.training.tracking import layer_utils as trackable_layer_utils\n",
    "from tensorflow.python.training.tracking import tracking\n",
    "from tensorflow.python.training.tracking import util as trackable_utils\n",
    "from tensorflow.python.util import nest\n",
    "from tensorflow.python.util import serialization\n",
    "from tensorflow.python.util import tf_inspect\n",
    "\n",
    "\n",
    "# pylint: disable=g-import-not-at-top\n",
    "try:\n",
    "  import h5py\n",
    "except ImportError:\n",
    "  h5py = None\n",
    "\n",
    "try:\n",
    "  import yaml\n",
    "except ImportError:\n",
    "  yaml = None\n",
    "# pylint: enable=g-import-not-at-top"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'keras' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_5488/3476777937.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mclass\u001b[0m \u001b[0mbranched\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mModel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstudent\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mteacher\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m         \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbranched\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mteacher\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mteacher\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstudent\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclone_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstudent\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'keras' is not defined"
     ]
    }
   ],
   "source": [
    "class branched(keras.Model):\n",
    "    def __init__(self, student, teacher):\n",
    "        super(branched, self).__init__()\n",
    "        self.teacher = teacher\n",
    "        self.student = tf.keras.models.clone_model(student)\n",
    "    def compile(\n",
    "        self,\n",
    "        optimizer,\n",
    "        metrics,\n",
    "        student_loss_fn,\n",
    "        distillation_loss_fn,\n",
    "        alpha=0.1,\n",
    "        temperature=1,        \n",
    "    ):\n",
    "        \"\"\" Configure the distiller.\n",
    "        Args:\n",
    "            optimizer: Keras optimizer for the student weights\n",
    "            metrics: Keras metrics for evaluation\n",
    "            student_loss_fn: Loss function of difference between student\n",
    "                predictions and ground-truth\n",
    "            distillation_loss_fn: Loss function of difference between soft\n",
    "                student predictions and soft teacher predictions\n",
    "            alpha: weight to student_loss_fn and 1-alpha to distillation_loss_fn\n",
    "            temperature: Temperature for softening probability distributions.\n",
    "                Larger temperature gives softer distributions.\n",
    "        \"\"\"\n",
    "        super(branched, self).compile(optimizer=optimizer, metrics=metrics)\n",
    "        self.student_loss_fn = student_loss_fn\n",
    "        self.distillation_loss_fn = distillation_loss_fn\n",
    "        self.alpha = alpha\n",
    "        self.temperature = temperature\n",
    "\n",
    "    def train_step(self, data):\n",
    "        # Unpack data\n",
    "        x, y = data\n",
    "        # Forward pass of teacher\n",
    "        teacher_predictions = self.teacher(x, training=False)\n",
    "\n",
    "        with tf.GradientTape() as tape:\n",
    "            # Forward pass of student\n",
    "            student_predictions = self.student(x, training=True)\n",
    "\n",
    "            # Compute losses\n",
    "            student_loss = self.student_loss_fn(y, student_predictions)\n",
    "            distillation_loss = self.distillation_loss_fn(\n",
    "                (teacher_predictions / self.temperature),\n",
    "                (student_predictions / self.temperature),\n",
    "            )\n",
    "            student_loss = student_loss * self.alpha\n",
    "            distillation_loss = (distillation_loss) * (1 - self.alpha)\n",
    "#             loss = distillation_loss\n",
    "            loss=student_loss +distillation_loss\n",
    "        # Compute gradients\n",
    "        trainable_vars = self.student.trainable_variables\n",
    "        gradients = tape.gradient(loss, trainable_vars)\n",
    "\n",
    "        # Update weights\n",
    "        self.optimizer.apply_gradients(zip(gradients, trainable_vars))\n",
    "\n",
    "        # Update the metrics configured in `compile()`.\n",
    "        self.compiled_metrics.update_state(y, student_predictions)\n",
    "\n",
    "        # Return a dict of performance\n",
    "        results = {m.name: m.result() for m in self.metrics}\n",
    "        results.update(\n",
    "            {\"loss\":loss,\"student_loss\": student_loss, \"distillation_loss\": distillation_loss}\n",
    "        )\n",
    "        return results\n",
    "\n",
    "    def test_step(self, data):\n",
    "        \"\"\"The logic for one evaluation step.\n",
    "\n",
    "        This method can be overridden to support custom evaluation logic.\n",
    "        This method is called by `Model.make_test_function`.\n",
    "\n",
    "        This function should contain the mathemetical logic for one step of\n",
    "        evaluation.\n",
    "        This typically includes the forward pass, loss calculation, and metrics\n",
    "        updates.\n",
    "\n",
    "        Configuration details for *how* this logic is run (e.g. `tf.function` and\n",
    "        `tf.distribute.Strategy` settings), should be left to\n",
    "        `Model.make_test_function`, which can also be overridden.\n",
    "\n",
    "        Arguments:\n",
    "          data: A nested structure of `Tensor`s.\n",
    "\n",
    "        Returns:\n",
    "          A `dict` containing values that will be passed to\n",
    "          `tf.keras.callbacks.CallbackList.on_train_batch_end`. Typically, the\n",
    "          values of the `Model`'s metrics are returned.\n",
    "        \"\"\"\n",
    "        data = data_adapter.expand_1d(data)\n",
    "        x, y, sample_weight = data_adapter.unpack_x_y_sample_weight(data)\n",
    "\n",
    "        y_pred = self(x, training=False)\n",
    "        print(self)\n",
    "        # Updates stateful loss metrics.\n",
    "        self.compiled_loss(\n",
    "            y, y_pred, sample_weight, regularization_losses=self.losses)\n",
    "\n",
    "        self.compiled_metrics.update_state(y, y_pred, sample_weight)\n",
    "        return {m.name: m.result() for m in self.metrics}\n",
    "\n",
    "\n",
    "    def test_step(self, data):\n",
    "        # Unpack the data\n",
    "        x, y = data\n",
    "        # Compute predictions\n",
    "        y_prediction = self.student(x, training=False)\n",
    "\n",
    "        # Calculate the loss\n",
    "        student_loss = self.student_loss_fn(y, y_prediction)\n",
    "\n",
    "        # Update the metrics.\n",
    "        self.compiled_metrics.update_state(y, y_prediction)\n",
    "\n",
    "        # Return a dict of performance\n",
    "        results = {m.name: m.result() for m in self.metrics}\n",
    "        results.update({\"student_loss\": student_loss})\n",
    "        return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class thing():\n",
    "    def __call__(self, input):\n",
    "        return input *3\n",
    "    def call(self,input):\n",
    "        return input * 2\n",
    "t = thing()\n",
    "t(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n"
     ]
    }
   ],
   "source": [
    "x = {\"test\":10}\n",
    "\n",
    "y = x.get(\"test\",0)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import collections\n",
    "import copy\n",
    "import itertools\n",
    "import warnings\n",
    "\n",
    "from tensorflow.python.eager import context\n",
    "from tensorflow.python.framework import dtypes\n",
    "from tensorflow.python.framework import ops\n",
    "from tensorflow.python.keras import backend\n",
    "from tensorflow.python.keras.engine import base_layer\n",
    "from tensorflow.python.keras.engine import base_layer_utils\n",
    "from tensorflow.python.keras.engine import input_layer as input_layer_module\n",
    "from tensorflow.python.keras.engine import input_spec\n",
    "from tensorflow.python.keras.engine import node as node_module\n",
    "from tensorflow.python.keras.engine import training as training_lib\n",
    "from tensorflow.python.keras.engine import training_utils\n",
    "from tensorflow.python.keras.saving.saved_model import network_serialization\n",
    "from tensorflow.python.keras.utils import generic_utils\n",
    "from tensorflow.python.keras.utils import tf_inspect\n",
    "from tensorflow.python.keras.utils import tf_utils\n",
    "from tensorflow.python.ops import array_ops\n",
    "from tensorflow.python.ops import math_ops\n",
    "from tensorflow.python.platform import tf_logging as logging\n",
    "from tensorflow.python.training.tracking import base as trackable\n",
    "from tensorflow.python.util import nest\n",
    "from tensorflow.tools.docs import doc_controls\n",
    "\n",
    "\n",
    "from tensorflow.python.keras.engine import functional\n",
    "\n",
    "import types \n",
    "\n",
    "def _map_graph_network(inputs, outputs):\n",
    "        \"\"\"Validates a network's topology and gather its layers and nodes.\n",
    "\n",
    "        Args:\n",
    "        inputs: List of input tensors.\n",
    "        outputs: List of outputs tensors.\n",
    "\n",
    "        Returns:\n",
    "        A tuple `(nodes, nodes_by_depth, layers, layers_by_depth)`.\n",
    "        - nodes: list of Node instances.\n",
    "        - nodes_by_depth: dict mapping ints (depth) to lists of node instances.\n",
    "        - layers: list of Layer instances.\n",
    "        - layers_by_depth: dict mapping ints (depth) to lists of layer instances.\n",
    "\n",
    "        Raises:\n",
    "        ValueError: In case the network is not valid (e.g. disconnected graph).\n",
    "        \"\"\"\n",
    "\n",
    "        print(\"-----graph------\")\n",
    "        # \"depth\" is number of layers between output Node and the Node.\n",
    "        # Nodes are ordered from inputs -> outputs.\n",
    "        print(\"test\")\n",
    "        nodes_in_decreasing_depth, layer_indices = functional._build_map(outputs)\n",
    "        network_nodes = {\n",
    "          functional._make_node_key(node.layer.name, node.layer._inbound_nodes.index(node))\n",
    "          for node in nodes_in_decreasing_depth\n",
    "        }\n",
    "\n",
    "        nodes_depths = {}  # dict {node: depth value}\n",
    "        layers_depths = {}  # dict {layer: depth value}\n",
    "\n",
    "        for node in reversed(nodes_in_decreasing_depth):\n",
    "            print(node.layer.name)\n",
    "            # If the depth is not set, the node has no outbound nodes (depth 0).\n",
    "            depth = nodes_depths.setdefault(node, 0)\n",
    "\n",
    "            # Update the depth of the corresponding layer\n",
    "            previous_depth = layers_depths.get(node.layer, 0)\n",
    "            # If we've seen this layer before at a higher depth,\n",
    "            # we should use that depth instead of the node depth.\n",
    "            # This is necessary for shared layers that have inputs at different\n",
    "            # depth levels in the graph.\n",
    "            depth = max(depth, previous_depth)\n",
    "            layers_depths[node.layer] = depth\n",
    "            nodes_depths[node] = depth\n",
    "\n",
    "            # Update the depth of inbound nodes.\n",
    "            # The \"depth\" of a node is the max of the depths\n",
    "            # of all nodes it is connected to + 1.\n",
    "            for node_dep in node.parent_nodes:\n",
    "                previous_depth = nodes_depths.get(node_dep, 0)\n",
    "                nodes_depths[node_dep] = max(depth + 1, previous_depth)\n",
    "\n",
    "        # Handle inputs that are not connected to outputs.\n",
    "        # We do not error out here because the inputs may be used to compute losses\n",
    "        # and metrics.\n",
    "        for input_t in inputs:\n",
    "            input_layer = input_t._keras_history[0]\n",
    "            if input_layer not in layers_depths:\n",
    "                layers_depths[input_layer] = 0\n",
    "                layer_indices[input_layer] = -1\n",
    "                nodes_depths[input_layer._inbound_nodes[0]] = 0\n",
    "                network_nodes.add(_make_node_key(input_layer.name, 0))\n",
    "\n",
    "        # Build a dict {depth: list of nodes with this depth}\n",
    "        nodes_by_depth = collections.defaultdict(list)\n",
    "        for node, depth in nodes_depths.items():\n",
    "            nodes_by_depth[depth].append(node)\n",
    "\n",
    "        # Build a dict {depth: list of layers with this depth}\n",
    "        layers_by_depth = collections.defaultdict(list)\n",
    "        for layer, depth in layers_depths.items():\n",
    "            layers_by_depth[depth].append(layer)\n",
    "\n",
    "        # Get sorted list of layer depths.\n",
    "        depth_keys = list(layers_by_depth.keys())\n",
    "        depth_keys.sort(reverse=True)\n",
    "\n",
    "        # Set self.layers ordered by depth.\n",
    "        layers = []\n",
    "        for depth in depth_keys:\n",
    "            layers_for_depth = layers_by_depth[depth]\n",
    "            # Network.layers needs to have a deterministic order:\n",
    "            # here we order them by traversal order.\n",
    "            layers_for_depth.sort(key=lambda x: layer_indices[x])\n",
    "            layers.extend(layers_for_depth)\n",
    "\n",
    "        # Get sorted list of node depths.\n",
    "        depth_keys = list(nodes_by_depth.keys())\n",
    "        depth_keys.sort(reverse=True)\n",
    "\n",
    "        # Check that all tensors required are computable.\n",
    "        # computable_tensors: all tensors in the graph\n",
    "        # that can be computed from the inputs provided.\n",
    "        computable_tensors = set()\n",
    "        for x in inputs:\n",
    "            computable_tensors.add(id(x))\n",
    "\n",
    "        layers_with_complete_input = []  # To provide a better error msg.\n",
    "        for depth in depth_keys:\n",
    "            for node in nodes_by_depth[depth]:\n",
    "                layer = node.layer\n",
    "                if layer and not node.is_input:\n",
    "                    for x in nest.flatten(node.keras_inputs):\n",
    "                        if id(x) not in computable_tensors:\n",
    "                            raise ValueError('Graph disconnected: '\n",
    "                                             'cannot obtain value for tensor ' + str(x) +\n",
    "                                             ' at layer \"' + layer.name + '\". '\n",
    "                                             'The following previous layers '\n",
    "                                             'were accessed without issue: ' +\n",
    "                                             str(layers_with_complete_input))\n",
    "                    for x in nest.flatten(node.outputs):\n",
    "                        computable_tensors.add(id(x))\n",
    "                    layers_with_complete_input.append(layer.name)\n",
    "\n",
    "        # Ensure name unicity, which will be crucial for serialization\n",
    "        # (since serialized nodes refer to layers by their name).\n",
    "        all_names = [layer.name for layer in layers]\n",
    "        for name in all_names:\n",
    "            if all_names.count(name) != 1:\n",
    "                raise ValueError('The name \"' + name + '\" is used ' +\n",
    "                               str(all_names.count(name)) + ' times in the model. '\n",
    "                               'All layer names should be unique.')\n",
    "        print(\"end graph\")\n",
    "\n",
    "        return network_nodes, nodes_by_depth, layers, layers_by_depth\n",
    "    \n",
    "class BranchFinished():\n",
    "    def __init__(self, val):\n",
    "        self.val = val\n",
    "\n",
    "class MyModel(tf.keras.Model):\n",
    "    \n",
    "#     def __init__(self, *args, **kwargs):\n",
    "#         super(MyModel, self).__init__(*args, **kwargs)\n",
    "#         print(\"---init----\")\n",
    "#         print(self._nodes_by_depth)\n",
    "# #         self.self._nodes_by_depth = self.build_new_depth_map()\n",
    "# #         print(\"---new depth ----\")\n",
    "# #         print(self._nodes_by_depth)\n",
    "#         print(\"---end----\")\n",
    "#         funcType = types.MethodType\n",
    "#         self._map_graph_network = funcType(_map_graph_network_new,self)\n",
    "#         _init_graph_network_new(self, self.inputs,self.outputs)\n",
    "#         super(MyModel, self).__init__(*args, **kwargs)\n",
    "        \n",
    "#     def build_new_depth_map(self):\n",
    "#         results = _map_graph_network_new(self.inputs,self.outputs)\n",
    "#         print(\"results\", results)\n",
    "\n",
    "    \n",
    "\n",
    "    # @trackable.no_automatic_dependency_tracking\n",
    "    def _init_graph_network(self, inputs, outputs):\n",
    "        # This method is needed for Sequential to reinitialize graph network when\n",
    "        # layer is added or removed.\n",
    "        self._is_graph_network = True\n",
    "\n",
    "        # Normalize and set self.inputs, self.outputs.\n",
    "        if isinstance(inputs, list) and len(nest.flatten(inputs)) == 1:\n",
    "          inputs = inputs[0]\n",
    "        if isinstance(outputs, list) and len(nest.flatten(outputs)) == 1:\n",
    "          outputs = outputs[0]\n",
    "        self._nested_inputs = inputs\n",
    "        self._nested_outputs = outputs\n",
    "        self.inputs = nest.flatten(inputs)\n",
    "        self.outputs = nest.flatten(outputs)\n",
    "\n",
    "        # Models constructed with a single Tensor or list of Tensors can\n",
    "        # be called with a dict, where the keys of the dict are the names\n",
    "        # of the `Input` objects. Extra keys are ignored with warning.\n",
    "        if not nest.is_nested(self._nested_inputs):\n",
    "          self._enable_dict_to_input_mapping = True\n",
    "        elif (isinstance(self._nested_inputs, (list, tuple)) and\n",
    "              not any(nest.is_nested(t) for t in self._nested_inputs)):\n",
    "          self._enable_dict_to_input_mapping = True\n",
    "        elif (isinstance(self._nested_inputs, dict) and\n",
    "              not any(nest.is_nested(t) for t in self._nested_inputs.values())):\n",
    "          self._enable_dict_to_input_mapping = True\n",
    "        else:\n",
    "          self._enable_dict_to_input_mapping = False\n",
    "\n",
    "        if not ops.executing_eagerly_outside_functions():\n",
    "          if any(not hasattr(tensor, '_keras_history') for tensor in self.outputs):\n",
    "            base_layer_utils.create_keras_history(self._nested_outputs)\n",
    "\n",
    "        self._validate_graph_inputs_and_outputs()\n",
    "\n",
    "        # A Network does not create weights of its own, thus it is already\n",
    "        # built.\n",
    "        self.built = True\n",
    "        self._build_input_shape = nest.map_structure(lambda x: x.shape, inputs)\n",
    "        self._compute_output_and_mask_jointly = True\n",
    "        # `_expects_training_arg` is True since the `training` argument is always\n",
    "        # present in the signature of the `call` method of a graph network.\n",
    "        self._expects_training_arg = True\n",
    "        self._expects_mask_arg = True\n",
    "        # A graph network does not autocast inputs, as its layers will cast them\n",
    "        # instead.\n",
    "        self._autocast = False\n",
    "\n",
    "        self._input_layers = []\n",
    "        self._output_layers = []\n",
    "        self._input_coordinates = []\n",
    "        self._output_coordinates = []\n",
    "\n",
    "        # This is for performance optimization when calling the Network on new\n",
    "        # inputs. Every time the Network is called on a set on input tensors,\n",
    "        # we compute the output tensors, output masks and output shapes in one pass,\n",
    "        # then cache them here. When any of these outputs is queried later, we\n",
    "        # retrieve it from there instead of recomputing it.\n",
    "        self._output_mask_cache = {}\n",
    "        self._output_tensor_cache = {}\n",
    "        self._output_shape_cache = {}\n",
    "\n",
    "        # Build self._output_layers:\n",
    "        for x in self.outputs:\n",
    "          layer, node_index, tensor_index = x._keras_history  # pylint: disable=protected-access\n",
    "          self._output_layers.append(layer)\n",
    "          self._output_coordinates.append((layer, node_index, tensor_index))\n",
    "\n",
    "        # Build self._input_layers:\n",
    "        for x in self.inputs:\n",
    "          layer, node_index, tensor_index = x._keras_history  # pylint: disable=protected-access\n",
    "          # It's supposed to be an input layer, so only one node\n",
    "          # and one tensor output.\n",
    "          assert node_index == 0\n",
    "          assert tensor_index == 0\n",
    "          self._input_layers.append(layer)\n",
    "          self._input_coordinates.append((layer, node_index, tensor_index))\n",
    "\n",
    "        # Keep track of the network's nodes and layers.\n",
    "        nodes, nodes_by_depth, layers, _ = _map_graph_network(\n",
    "            self.inputs, self.outputs)\n",
    "        self._network_nodes = nodes\n",
    "        self._nodes_by_depth = nodes_by_depth\n",
    "        self._self_tracked_trackables = layers\n",
    "        self._layer_call_argspecs = {}\n",
    "        for layer in self._self_tracked_trackables:\n",
    "          self._layer_call_argspecs[layer] = tf_inspect.getfullargspec(layer.call)\n",
    "\n",
    "        # Build self.input_names and self.output_names.\n",
    "        self._set_output_names()\n",
    "        self.input_names = []\n",
    "        self._feed_input_names = []\n",
    "        self._feed_inputs = []\n",
    "        self._feed_input_shapes = []\n",
    "        for layer in self._input_layers:\n",
    "          self.input_names.append(layer.name)\n",
    "          if layer.is_placeholder:\n",
    "            self._feed_input_names.append(layer.name)\n",
    "            # Use batch_input_shape here because non-eager composite tensors may not\n",
    "            # have a shape attribute that's meaningful (sparse, for instance, has\n",
    "            # a tensor that's non-constant and needs to be fed). This means that\n",
    "            # input layers that create placeholders will need to have the\n",
    "            # batch_input_shape attr to allow for input shape validation.\n",
    "            self._feed_input_shapes.append(layer._batch_input_shape)\n",
    "            self._feed_inputs.append(layer.input)\n",
    "\n",
    "        self._compute_tensor_usage_count()\n",
    "        self._set_save_spec(self._nested_inputs)\n",
    "        tf_utils.assert_no_legacy_layers(self.layers)    \n",
    "        \n",
    "#     def call(self, inputs, training=None, mask=None):\n",
    "#         \"\"\"Calls the model on new inputs.\n",
    "\n",
    "#         In this case `call` just reapplies\n",
    "#         all ops in the graph to the new inputs\n",
    "#         (e.g. build a new computational graph from the provided inputs).\n",
    "\n",
    "#         Arguments:\n",
    "#             inputs: A tensor or list of tensors.\n",
    "#             training: Boolean or boolean scalar tensor, indicating whether to run\n",
    "#               the `Network` in training mode or inference mode.\n",
    "#             mask: A mask or list of masks. A mask can be\n",
    "#                 either a tensor or None (no mask).\n",
    "\n",
    "#         Returns:\n",
    "#             A tensor if there is a single output, or\n",
    "#             a list of tensors if there are more than one outputs.\n",
    "#         \"\"\"\n",
    "#         if not self._is_graph_network:\n",
    "#             raise NotImplementedError('When subclassing the `Model` class, you should'\n",
    "#                                     ' implement a `call` method.')\n",
    "\n",
    "#         return self._run_internal_graph(\n",
    "#             inputs, training=training, mask=mask,\n",
    "#             convert_kwargs_to_constants=base_layer_utils.call_context().saving)\n",
    "    \n",
    "    \n",
    "    ##### --------------------------------\n",
    "\n",
    "#     def test_step(self, data):\n",
    "#         \"\"\"The logic for one evaluation step.\n",
    "\n",
    "#         This method can be overridden to support custom evaluation logic.\n",
    "#         This method is called by `Model.make_test_function`.\n",
    "\n",
    "#         This function should contain the mathemetical logic for one step of\n",
    "#         evaluation.\n",
    "#         This typically includes the forward pass, loss calculation, and metrics\n",
    "#         updates.\n",
    "\n",
    "#         Configuration details for *how* this logic is run (e.g. `tf.function` and\n",
    "#         `tf.distribute.Strategy` settings), should be left to\n",
    "#         `Model.make_test_function`, which can also be overridden.\n",
    "\n",
    "#         Arguments:\n",
    "#           data: A nested structure of `Tensor`s.\n",
    "\n",
    "#         Returns:\n",
    "#           A `dict` containing values that will be passed to\n",
    "#           `tf.keras.callbacks.CallbackList.on_train_batch_end`. Typically, the\n",
    "#           values of the `Model`'s metrics are returned.\n",
    "#         \"\"\"\n",
    "#         print(data)\n",
    "#         data = data_adapter.expand_1d(data)\n",
    "#         x, y, sample_weight = data_adapter.unpack_x_y_sample_weight(data)\n",
    "\n",
    "#         y_pred = self(x, training=False)\n",
    "#         print(self.layers)\n",
    "#         # Updates stateful loss metrics.\n",
    "#         self.compiled_loss(\n",
    "#             y, y_pred, sample_weight, regularization_losses=self.losses)\n",
    "\n",
    "#         self.compiled_metrics.update_state(y, y_pred, sample_weight)\n",
    "#         return {m.name: m.result() for m in self.metrics}\n",
    "\n",
    "    \n",
    "#     def predict_step(self, data):\n",
    "#         \"\"\"The logic for one inference step.\n",
    "\n",
    "#         This method can be overridden to support custom inference logic.\n",
    "#         This method is called by `Model.make_predict_function`.\n",
    "\n",
    "#         This method should contain the mathemetical logic for one step of inference.\n",
    "#         This typically includes the forward pass.\n",
    "\n",
    "#         Configuration details for *how* this logic is run (e.g. `tf.function` and\n",
    "#         `tf.distribute.Strategy` settings), should be left to\n",
    "#         `Model.make_predict_function`, which can also be overridden.\n",
    "\n",
    "#         Arguments:\n",
    "#           data: A nested structure of `Tensor`s.\n",
    "\n",
    "#         Returns:\n",
    "#           The result of one inference step, typically the output of calling the\n",
    "#           `Model` on data.\n",
    "#         \"\"\"\n",
    "#         data = data_adapter.expand_1d(data)\n",
    "#         x, _, _ = data_adapter.unpack_x_y_sample_weight(data)\n",
    "#         print(\"predict\",self.layers)\n",
    "#         try:\n",
    "#             while True:\n",
    "#                 result = self(x, training=False)\n",
    "#         except BranchFinished as Branch_result:\n",
    "#             print(\"branchPoint Hit\")\n",
    "#             print(Branch_result.val)\n",
    "#             pass\n",
    "#     # or print \"Goodbye.\" if you want \n",
    "        \n",
    "#         return result\n",
    "\n",
    "    def turnOnThresholds(self):\n",
    "        for layers in self.layers:\n",
    "            if hasattr(layers,'threshold'):\n",
    "                layers.threshold = True\n",
    "    \n",
    "class MyDenseLayer(tf.keras.layers.Layer):\n",
    "    \n",
    "    def __init__(self, nodes, name=None):\n",
    "        super(MyDenseLayer, self).__init__()\n",
    "        self.num_outputs = nodes\n",
    "        self.threshold = False\n",
    "        \n",
    "        \n",
    "    def get_config(self):\n",
    "            config = super().get_config().copy()\n",
    "            config.update({\n",
    "                'num_outputs': self.num_outputs,\n",
    "                'name': self.name\n",
    "            })\n",
    "            return config\n",
    "    \n",
    "    def build(self, input_shape):\n",
    "        print(\"=\"*20)\n",
    "        print('we are here in build')\n",
    "        self.kernel = self.add_weight(\"kernel\",shape=      [int(input_shape[-1]),self.num_outputs],dtype=tf.float32)        \n",
    "    \n",
    "    def call(self, input):\n",
    "#         print('='*20)\n",
    "#         print('we are here in call')\n",
    "#         print(input)\n",
    "#         if self.threshold:\n",
    "#             print('thresholds')\n",
    "#             raise BranchFinished(tf.matmul(input, self.kernel))\n",
    "            \n",
    "        return tf.matmul(input, self.kernel)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'MyDenseLayer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_35264/4163369442.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[0mbranchLayer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mFlatten\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mv1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_default_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munique_name\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"branch_flatten\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[0mdense1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDense\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m124\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"relu\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mv1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_default_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munique_name\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"before\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbranchLayer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 23\u001b[1;33m \u001b[0mexit1\u001b[0m \u001b[1;33m=\u001b[0m\u001b[0mMyDenseLayer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mv1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_default_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munique_name\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"exit\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdense1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     24\u001b[0m \u001b[0mbranchLayer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDense\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m124\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"relu\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mv1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_default_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munique_name\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"alternate1_\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbranchLayer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'MyDenseLayer' is not defined"
     ]
    }
   ],
   "source": [
    "inputs = keras.Input(shape=(227,227,3))\n",
    "# targets = keras.Input(shape=(10,))\n",
    "x = keras.layers.Conv2D(filters=96, kernel_size=(11,11), strides=(4,4), activation='relu', input_shape=(227,227,3))(inputs)\n",
    "x = keras.layers.BatchNormalization()(x)\n",
    "x = keras.layers.MaxPool2D(pool_size=(3,3), strides=(2,2))(x)\n",
    "x = keras.layers.Conv2D(filters=256, kernel_size=(5,5), strides=(1,1), activation='relu', padding=\"same\")(x)\n",
    "x = keras.layers.BatchNormalization()(x)\n",
    "x = keras.layers.MaxPool2D(pool_size=(3,3), strides=(2,2))(x)\n",
    "# x = keras.layers.Conv2D(filters=384, kernel_size=(3,3), strides=(1,1), activation='relu', padding=\"same\")(x)\n",
    "# x = keras.layers.BatchNormalization()(x)\n",
    "# x = keras.layers.Conv2D(filters=384, kernel_size=(1,1), strides=(1,1), activation='relu', padding=\"same\")(x)\n",
    "# x = keras.layers.BatchNormalization()(x)\n",
    "# x = keras.layers.Conv2D(filters=256, kernel_size=(1,1), strides=(1,1), activation='relu', padding=\"same\")(x)\n",
    "# x = keras.layers.BatchNormalization()(x)\n",
    "# x = keras.layers.MaxPool2D(pool_size=(3,3), strides=(2,2))(x)\n",
    "# x = keras.layers.Flatten()(x)\n",
    "# x = keras.layers.Dense(4096, activation='relu')(x)\n",
    "# x = keras.layers.Dropout(0.5)(x)\n",
    "\n",
    "# ### first branch\n",
    "branchLayer = keras.layers.Flatten(name=tf.compat.v1.get_default_graph().unique_name(\"branch_flatten\"))(x)\n",
    "dense1 = keras.layers.Dense(124, activation=\"relu\",name=tf.compat.v1.get_default_graph().unique_name(\"before\"))(branchLayer)\n",
    "exit1 =MyDenseLayer(10,name=tf.compat.v1.get_default_graph().unique_name(\"exit\"))(dense1)\n",
    "branchLayer = keras.layers.Dense(124, activation=\"relu\",name=tf.compat.v1.get_default_graph().unique_name(\"alternate1_\"))(branchLayer)\n",
    "\n",
    "branchLayer = keras.layers.Dense(64, activation=\"relu\",name=tf.compat.v1.get_default_graph().unique_name(\"alternate2_\"))(branchLayer)\n",
    "x = keras.layers.Dense(10, activation=\"softmax\", name=tf.compat.v1.get_default_graph().unique_name(\"alternate_exit\"))(branchLayer)\n",
    "\n",
    "\n",
    "# model = keras.Model(inputs=(inputs), outputs=[x,exit1], name=\"alexnet\")\n",
    "model = MyModel(inputs=(inputs), outputs=[x,exit1], name=\"alexnet\")\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer=tf.optimizers.SGD(lr=0.001,momentum=0.9), metrics=['accuracy'])\n",
    "# student_model.save(\"models/alexNetv6_second_Exit.hdf5\")\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.turnOnThresholds()\n",
    "result = model.predict(test_ds.take(1))\n",
    "print(result)\n",
    "predictions = []\n",
    "for i in result:\n",
    "    predictions.append(np.argmax(i,axis=1))\n",
    "print(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NEXT THING: Figure out why the depth is messed up, and think of a way of reordering the graph so that the branches are evaluated before the main path continues. \n",
    "\n",
    "# also the whole thing doesn't work when the exits are re-ordered. and its all a bit of a hack really. I need a more elegant solution, otherwise I'll have to reupdate this every time this function changes... which might have to be the case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"alexnet\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_30 (InputLayer)          [(None, 227, 227, 3  0           []                               \n",
      "                                )]                                                                \n",
      "                                                                                                  \n",
      " conv2d_130 (Conv2D)            (None, 55, 55, 96)   34944       ['input_30[0][0]']               \n",
      "                                                                                                  \n",
      " batch_normalization_130 (Batch  (None, 55, 55, 96)  384         ['conv2d_130[0][0]']             \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " max_pooling2d_82 (MaxPooling2D  (None, 27, 27, 96)  0           ['batch_normalization_130[0][0]']\n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " conv2d_131 (Conv2D)            (None, 27, 27, 256)  614656      ['max_pooling2d_82[0][0]']       \n",
      "                                                                                                  \n",
      " batch_normalization_131 (Batch  (None, 27, 27, 256)  1024       ['conv2d_131[0][0]']             \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " max_pooling2d_83 (MaxPooling2D  (None, 13, 13, 256)  0          ['batch_normalization_131[0][0]']\n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " conv2d_132 (Conv2D)            (None, 13, 13, 384)  885120      ['max_pooling2d_83[0][0]']       \n",
      "                                                                                                  \n",
      " batch_normalization_132 (Batch  (None, 13, 13, 384)  1536       ['conv2d_132[0][0]']             \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " conv2d_133 (Conv2D)            (None, 13, 13, 384)  147840      ['batch_normalization_132[0][0]']\n",
      "                                                                                                  \n",
      " batch_normalization_133 (Batch  (None, 13, 13, 384)  1536       ['conv2d_133[0][0]']             \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " conv2d_134 (Conv2D)            (None, 13, 13, 256)  98560       ['batch_normalization_133[0][0]']\n",
      "                                                                                                  \n",
      " batch_normalization_134 (Batch  (None, 13, 13, 256)  1024       ['conv2d_134[0][0]']             \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " max_pooling2d_84 (MaxPooling2D  (None, 6, 6, 256)   0           ['batch_normalization_134[0][0]']\n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " flatten_24 (Flatten)           (None, 9216)         0           ['max_pooling2d_84[0][0]']       \n",
      "                                                                                                  \n",
      " dense_24 (Dense)               (None, 4096)         37752832    ['flatten_24[0][0]']             \n",
      "                                                                                                  \n",
      " dropout_24 (Dropout)           (None, 4096)         0           ['dense_24[0][0]']               \n",
      "                                                                                                  \n",
      " main_path124_24 (Dense)        (None, 124)          508028      ['dropout_24[0][0]']             \n",
      "                                                                                                  \n",
      " main_path64_24 (Dense)         (None, 64)           8000        ['main_path124_24[0][0]']        \n",
      "                                                                                                  \n",
      " exit_24 (Dense)                (None, 10)           650         ['main_path64_24[0][0]']         \n",
      "                                                                                                  \n",
      " branch_4 (Branch)              (None, 10)           5373510     ['max_pooling2d_83[0][0]']       \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 45,429,644\n",
      "Trainable params: 45,426,892\n",
      "Non-trainable params: 2,752\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "import collections\n",
    "import copy\n",
    "import itertools\n",
    "import warnings\n",
    "\n",
    "from tensorflow.python.eager import context\n",
    "from tensorflow.python.framework import dtypes\n",
    "from tensorflow.python.framework import ops\n",
    "from tensorflow.python.keras import backend\n",
    "from tensorflow.python.keras.engine import base_layer\n",
    "from tensorflow.python.keras.engine import base_layer_utils\n",
    "from tensorflow.python.keras.engine import input_layer as input_layer_module\n",
    "from tensorflow.python.keras.engine import input_spec\n",
    "from tensorflow.python.keras.engine import node as node_module\n",
    "from tensorflow.python.keras.engine import training as training_lib\n",
    "from tensorflow.python.keras.engine import training_utils\n",
    "from tensorflow.python.keras.saving.saved_model import network_serialization\n",
    "from tensorflow.python.keras.utils import generic_utils\n",
    "from tensorflow.python.keras.utils import tf_inspect\n",
    "from tensorflow.python.keras.utils import tf_utils\n",
    "from tensorflow.python.ops import array_ops\n",
    "from tensorflow.python.ops import math_ops\n",
    "from tensorflow.python.platform import tf_logging as logging\n",
    "from tensorflow.python.training.tracking import base as trackable\n",
    "from tensorflow.python.util import nest\n",
    "from tensorflow.tools.docs import doc_controls\n",
    "from tensorflow.python.keras.engine import functional\n",
    "\n",
    "class simpleModel(tf.keras.Model):      \n",
    "    \n",
    "\n",
    "    def _map_graph_network(self, inputs, outputs,reverse =True):\n",
    "      \"\"\"Validates a network's topology and gather its layers and nodes.\n",
    "    \n",
    "      Args:\n",
    "        inputs: List of input tensors.\n",
    "        outputs: List of outputs tensors.\n",
    "\n",
    "      Returns:\n",
    "        A tuple `(nodes, nodes_by_depth, layers, layers_by_depth)`.\n",
    "        - nodes: list of Node instances.\n",
    "        - nodes_by_depth: dict mapping ints (depth) to lists of node instances.\n",
    "        - layers: list of Layer instances.\n",
    "        - layers_by_depth: dict mapping ints (depth) to lists of layer instances.\n",
    "\n",
    "      Raises:\n",
    "        ValueError: In case the network is not valid (e.g. disconnected graph).\n",
    "      \"\"\"\n",
    "      print(\"_map_graph_network\")\n",
    "      # \"depth\" is number of layers between output Node and the Node.\n",
    "      # Nodes are ordered from inputs -> outputs.\n",
    "      nodes_in_decreasing_depth, layer_indices = functional._build_map(outputs)\n",
    "      network_nodes = {\n",
    "          functional._make_node_key(node.layer.name, node.layer._inbound_nodes.index(node))\n",
    "          for node in nodes_in_decreasing_depth\n",
    "      }\n",
    "\n",
    "      nodes_depths = {}  # dict {node: depth value}\n",
    "      layers_depths = {}  # dict {layer: depth value}\n",
    "      if reverse:\n",
    "          for node in reversed(nodes_in_decreasing_depth):\n",
    "            # print(node.layer.name)\n",
    "            # If the depth is not set, the node has no outbound nodes (depth 0).\n",
    "\n",
    "            depth = nodes_depths.setdefault(node, 0)\n",
    "            \n",
    "            # Update the depth of the corresponding layer\n",
    "            previous_depth = layers_depths.get(node.layer, 0)\n",
    "            # If we've seen this layer before at a higher depth,\n",
    "            # we should use that depth instead of the node depth.\n",
    "            # This is necessary for shared layers that have inputs at different\n",
    "            # depth levels in the graph.\n",
    "            depth = max(depth, previous_depth)\n",
    "            layers_depths[node.layer] = depth\n",
    "            nodes_depths[node] = depth\n",
    "\n",
    "            # Update the depth of inbound nodes.\n",
    "            # The \"depth\" of a node is the max of the depths\n",
    "            # of all nodes it is connected to + 1.\n",
    "            for node_dep in node.parent_nodes:\n",
    "              previous_depth = nodes_depths.get(node_dep, 0)\n",
    "              nodes_depths[node_dep] = max(depth + 1, previous_depth)\n",
    "            print(node.layer.name, \" depth: \", depth )\n",
    "          print(\"\")\n",
    "          for node in (nodes_in_decreasing_depth):\n",
    "            # print(node.layer.name)\n",
    "            # If the depth is not set, the node has no outbound nodes (depth 0).\n",
    "\n",
    "            depth = nodes_depths.setdefault(node, 0)\n",
    "\n",
    "            # Update the depth of the corresponding layer\n",
    "            previous_depth = layers_depths.get(node.layer, 0)\n",
    "            # print(\"prev depth\", previous_depth)\n",
    "            # If we've seen this layer before at a higher depth,\n",
    "            # we should use that depth instead of the node depth.\n",
    "            # This is necessary for shared layers that have inputs at different\n",
    "            # depth levels in the graph.\n",
    "            # if depth is not 0:\n",
    "                # print(depth)\n",
    "            depth = max(depth, previous_depth)\n",
    "            layers_depths[node.layer] = depth\n",
    "            nodes_depths[node] = depth\n",
    "\n",
    "            # Update the depth of inbound nodes.\n",
    "            # The \"depth\" of a node is the max of the depths\n",
    "            # of all nodes it is connected to + 1.\n",
    "            for node_dep in node.parent_nodes:\n",
    "              previous_depth = nodes_depths.get(node_dep, 0)\n",
    "              depth  = max(previous_depth - 1, 0)\n",
    "              print(node.layer.name, \": \", depth, \" \", node_dep.layer.name, \": \", previous_depth)\n",
    "              \n",
    "              # depth = previous_depth + 1\n",
    "              nodes_depths[node] = depth\n",
    "      # Handle inputs that are not connected to outputs.\n",
    "      # We do not error out here because the inputs may be used to compute losses\n",
    "      # and metrics.\n",
    "      for input_t in inputs:\n",
    "        input_layer = input_t._keras_history[0]\n",
    "        if input_layer not in layers_depths:\n",
    "          layers_depths[input_layer] = 0\n",
    "          layer_indices[input_layer] = -1\n",
    "          nodes_depths[input_layer._inbound_nodes[0]] = 0\n",
    "          network_nodes.add(_make_node_key(input_layer.name, 0))\n",
    "\n",
    "      # Build a dict {depth: list of nodes with this depth}\n",
    "      nodes_by_depth = collections.defaultdict(list)\n",
    "      for node, depth in nodes_depths.items():\n",
    "        nodes_by_depth[depth].append(node)\n",
    "\n",
    "      # Build a dict {depth: list of layers with this depth}\n",
    "      layers_by_depth = collections.defaultdict(list)\n",
    "      for layer, depth in layers_depths.items():\n",
    "        layers_by_depth[depth].append(layer)\n",
    "\n",
    "      # Get sorted list of layer depths.\n",
    "      depth_keys = list(layers_by_depth.keys())\n",
    "      depth_keys.sort(reverse=True)\n",
    "      print(depth_keys)\n",
    "      # Set self.layers ordered by depth.\n",
    "      layers = []\n",
    "      for depth in depth_keys:\n",
    "        layers_for_depth = layers_by_depth[depth]\n",
    "        # Network.layers needs to have a deterministic order:\n",
    "        # here we order them by traversal order.\n",
    "        layers_for_depth.sort(key=lambda x: layer_indices[x])\n",
    "        layers.extend(layers_for_depth)\n",
    "\n",
    "      # Get sorted list of node depths.\n",
    "      depth_keys = list(nodes_by_depth.keys())\n",
    "      depth_keys.sort(reverse=True)\n",
    "\n",
    "      # Check that all tensors required are computable.\n",
    "      # computable_tensors: all tensors in the graph\n",
    "      # that can be computed from the inputs provided.\n",
    "      computable_tensors = set()\n",
    "      for x in inputs:\n",
    "        computable_tensors.add(id(x))\n",
    "\n",
    "      layers_with_complete_input = []  # To provide a better error msg.\n",
    "      for depth in depth_keys:\n",
    "        for node in nodes_by_depth[depth]:\n",
    "          layer = node.layer\n",
    "          if layer and not node.is_input:\n",
    "            for x in nest.flatten(node.keras_inputs):\n",
    "              if id(x) not in computable_tensors:\n",
    "                raise ValueError('Graph disconnected: '\n",
    "                                 'cannot obtain value for tensor ' + str(x) +\n",
    "                                 ' at layer \"' + layer.name + '\". '\n",
    "                                 'The following previous layers '\n",
    "                                 'were accessed without issue: ' +\n",
    "                                 str(layers_with_complete_input))\n",
    "            for x in nest.flatten(node.outputs):\n",
    "              computable_tensors.add(id(x))\n",
    "            layers_with_complete_input.append(layer.name)\n",
    "\n",
    "      # Ensure name unicity, which will be crucial for serialization\n",
    "      # (since serialized nodes refer to layers by their name).\n",
    "      all_names = [layer.name for layer in layers]\n",
    "      for name in all_names:\n",
    "        if all_names.count(name) != 1:\n",
    "          raise ValueError('The name \"' + name + '\" is used ' +\n",
    "                           str(all_names.count(name)) + ' times in the model. '\n",
    "                           'All layer names should be unique.')\n",
    "        \n",
    "      \n",
    "      \n",
    "      self._nodes_by_depth = nodes_by_depth\n",
    "      \n",
    "      \n",
    "      # return network_nodes, nodes_by_depth, layers, layers_by_depth\n",
    "\n",
    "\n",
    "    def _run_internal_graph(self, inputs, training=None, mask=None):\n",
    "        \"\"\"Computes output tensors for new inputs.\n",
    "\n",
    "        # Note:\n",
    "            - Can be run on non-Keras tensors.\n",
    "\n",
    "        Args:\n",
    "            inputs: Tensor or nested structure of Tensors.\n",
    "            training: Boolean learning phase.\n",
    "            mask: (Optional) Tensor or nested structure of Tensors.\n",
    "\n",
    "        Returns:\n",
    "            output_tensors\n",
    "        \"\"\"\n",
    "        print(\"_run_internal_graph --custom\")\n",
    "        inputs = self._flatten_to_reference_inputs(inputs)\n",
    "        if mask is None:\n",
    "          masks = [None] * len(inputs)\n",
    "        else:\n",
    "          masks = self._flatten_to_reference_inputs(mask)\n",
    "        for input_t, mask in zip(inputs, masks):\n",
    "          input_t._keras_mask = mask\n",
    "\n",
    "        # Dictionary mapping reference tensors to computed tensors.\n",
    "        tensor_dict = {}\n",
    "        tensor_usage_count = self._tensor_usage_count\n",
    "        for x, y in zip(self.inputs, inputs):\n",
    "          y = self._conform_to_reference_input(y, ref_input=x)\n",
    "          x_id = str(id(x))\n",
    "          tensor_dict[x_id] = [y] * tensor_usage_count[x_id]\n",
    "\n",
    "        nodes_by_depth = self._nodes_by_depth\n",
    "        depth_keys = list(nodes_by_depth.keys())\n",
    "        depth_keys.sort(reverse=True)\n",
    "    \n",
    "    \n",
    "        for depth in depth_keys:\n",
    "          nodes = nodes_by_depth[depth]\n",
    "          for node in nodes:\n",
    "            print(node.layer.name)\n",
    "            if node.is_input:\n",
    "              continue  # Input tensors already exist.\n",
    "\n",
    "            if any(t_id not in tensor_dict for t_id in node.flat_input_ids):\n",
    "              continue  # Node is not computable, try skipping.\n",
    "\n",
    "            args, kwargs = node.map_arguments(tensor_dict)\n",
    "            outputs = node.layer(*args, **kwargs)\n",
    "            \n",
    "                \n",
    "            # Update tensor_dict.\n",
    "            for x_id, y in zip(node.flat_output_ids, nest.flatten(outputs)):\n",
    "              tensor_dict[x_id] = [y] * tensor_usage_count[x_id]\n",
    "            \n",
    "            \n",
    "            if \"branch_exit\" in node.layer.name:\n",
    "                print(\"branch exit activated\")\n",
    "                output_tensors = []\n",
    "                for x_id, y in zip(node.flat_output_ids, nest.flatten(outputs)):\n",
    "                    for x in self.outputs:\n",
    "                      output_id = str(id(x))  \n",
    "                      if output_id == x_id:\n",
    "                        output_tensors.append(tensor_dict[x_id])\n",
    "                      else:\n",
    "                        # print(tensor_dict[x_id][0].shape)\n",
    "                        output_tensors.append(tf.zeros(tensor_dict[x_id][0].shape))\n",
    "                        \n",
    "                      # x_id_output = str(id(x))\n",
    "                      # assert x_id in tensor_dict, 'Could not compute output ' + str(x)\n",
    "                      # output_tensors.append(tensor_dict[x_id])\n",
    "\n",
    "                return nest.pack_sequence_as(self._nested_outputs, output_tensors)\n",
    "            \n",
    "            \n",
    "        output_tensors = []\n",
    "        for x in self.outputs:\n",
    "          x_id = str(id(x))\n",
    "          assert x_id in tensor_dict, 'Could not compute output ' + str(x)\n",
    "          output_tensors.append(tensor_dict[x_id].pop())\n",
    "\n",
    "        return nest.pack_sequence_as(self._nested_outputs, output_tensors)\n",
    "#     def call(self, inputs, training=None, mask=None):\n",
    "#         x = inputs\n",
    "        \n",
    "#         for layer in self.layers:\n",
    "#             # print(layer)\n",
    "#             x = layer(x)\n",
    "            \n",
    "#         return x\n",
    "\n",
    "    def turnOnThresholds(self):\n",
    "        for layers in self.layers:\n",
    "            if hasattr(layers,'threshold'):\n",
    "                layers.threshold = True\n",
    "\n",
    "class Branch(tf.keras.Model):\n",
    "\n",
    "  def __init__(self,name=''):\n",
    "    super().__init__(name=name)\n",
    "    self.layer = keras.layers.Layer(name=name)\n",
    "    # self.layer.name = name\n",
    "    # self.name=tf.compat.v1.get_default_graph().unique_name(\"branch\")\n",
    "    self.branchLayer1 = keras.layers.Flatten(name=tf.compat.v1.get_default_graph().unique_name(\"branch_flatten\"))\n",
    "    self.branchLayer2 = keras.layers.Dense(124, activation=\"relu\",name=tf.compat.v1.get_default_graph().unique_name(\"branch124\"))\n",
    "    self.branchLayer3 = keras.layers.Dense(64, activation=\"relu\",name=tf.compat.v1.get_default_graph().unique_name(\"branch64\"))\n",
    "    self.exit =keras.layers.Dense(10,name=tf.compat.v1.get_default_graph().unique_name(\"branch_exit\"))\n",
    "\n",
    "  def call(self, input_tensor, training=False):\n",
    "    x = self.branchLayer1(input_tensor)\n",
    "    x = tf.nn.relu(x)\n",
    "    x = self.branchLayer2(x)\n",
    "    x = tf.nn.relu(x)\n",
    "    x = self.branchLayer3(x)\n",
    "    x = self.exit(x)\n",
    "    return x\n",
    "                \n",
    "\n",
    "inputs = keras.Input(shape=(227,227,3))\n",
    "# targets = keras.Input(shape=(10,))\n",
    "x = keras.layers.Conv2D(filters=96, kernel_size=(11,11), strides=(4,4), activation='relu', input_shape=(227,227,3))(inputs)\n",
    "x = keras.layers.BatchNormalization()(x)\n",
    "x = keras.layers.MaxPool2D(pool_size=(3,3), strides=(2,2))(x)\n",
    "x = keras.layers.Conv2D(filters=256, kernel_size=(5,5), strides=(1,1), activation='relu', padding=\"same\")(x)\n",
    "x = keras.layers.BatchNormalization()(x)\n",
    "x = keras.layers.MaxPool2D(pool_size=(3,3), strides=(2,2))(x)\n",
    "# ### first branch\n",
    "\n",
    "\n",
    "branch_exit = Branch(tf.compat.v1.get_default_graph().unique_name(\"branch\"))(x)\n",
    "x = keras.layers.Conv2D(filters=384, kernel_size=(3,3), strides=(1,1), activation='relu', padding=\"same\")(x)\n",
    "x = keras.layers.BatchNormalization()(x)\n",
    "x = keras.layers.Conv2D(filters=384, kernel_size=(1,1), strides=(1,1), activation='relu', padding=\"same\")(x)\n",
    "x = keras.layers.BatchNormalization()(x)\n",
    "x = keras.layers.Conv2D(filters=256, kernel_size=(1,1), strides=(1,1), activation='relu', padding=\"same\")(x)\n",
    "x = keras.layers.BatchNormalization()(x)\n",
    "x = keras.layers.MaxPool2D(pool_size=(3,3), strides=(2,2))(x)\n",
    "x = keras.layers.Flatten()(x)\n",
    "x = keras.layers.Dense(4096, activation='relu')(x)\n",
    "x = keras.layers.Dropout(0.5)(x)\n",
    "x = keras.layers.Dense(124, activation=\"relu\",name=tf.compat.v1.get_default_graph().unique_name(\"main_path124\"))(x)\n",
    "x = keras.layers.Dense(64, activation=\"relu\",name=tf.compat.v1.get_default_graph().unique_name(\"main_path64\"))(x)\n",
    "x = keras.layers.Dense(10, activation=\"softmax\", name=tf.compat.v1.get_default_graph().unique_name(\"exit\"))(x)\n",
    "\n",
    "\n",
    "# model = keras.Model(inputs=(inputs), outputs=[x,branch_exit], name=\"alexnet\")\n",
    "model = simpleModel(inputs=(inputs), outputs=[x,branch_exit], name=\"alexnet\")\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer=tf.optimizers.SGD(lr=0.001,momentum=0.9), metrics=['accuracy'])\n",
    "# student_model.save(\"models/alexNetv6_second_Exit.hdf5\")\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_map_graph_network\n",
      "branch_4  depth:  0\n",
      "exit_24  depth:  0\n",
      "main_path64_24  depth:  1\n",
      "main_path124_24  depth:  2\n",
      "dropout_24  depth:  3\n",
      "dense_24  depth:  4\n",
      "flatten_24  depth:  5\n",
      "max_pooling2d_84  depth:  6\n",
      "batch_normalization_134  depth:  7\n",
      "conv2d_134  depth:  8\n",
      "batch_normalization_133  depth:  9\n",
      "conv2d_133  depth:  10\n",
      "batch_normalization_132  depth:  11\n",
      "conv2d_132  depth:  12\n",
      "max_pooling2d_83  depth:  13\n",
      "batch_normalization_131  depth:  14\n",
      "conv2d_131  depth:  15\n",
      "max_pooling2d_82  depth:  16\n",
      "batch_normalization_130  depth:  17\n",
      "conv2d_130  depth:  18\n",
      "input_30  depth:  19\n",
      "\n",
      "conv2d_130 :  18   input_30 :  19\n",
      "batch_normalization_130 :  17   conv2d_130 :  18\n",
      "max_pooling2d_82 :  16   batch_normalization_130 :  17\n",
      "conv2d_131 :  15   max_pooling2d_82 :  16\n",
      "batch_normalization_131 :  14   conv2d_131 :  15\n",
      "max_pooling2d_83 :  13   batch_normalization_131 :  14\n",
      "conv2d_132 :  12   max_pooling2d_83 :  13\n",
      "batch_normalization_132 :  11   conv2d_132 :  12\n",
      "conv2d_133 :  10   batch_normalization_132 :  11\n",
      "batch_normalization_133 :  9   conv2d_133 :  10\n",
      "conv2d_134 :  8   batch_normalization_133 :  9\n",
      "batch_normalization_134 :  7   conv2d_134 :  8\n",
      "max_pooling2d_84 :  6   batch_normalization_134 :  7\n",
      "flatten_24 :  5   max_pooling2d_84 :  6\n",
      "dense_24 :  4   flatten_24 :  5\n",
      "dropout_24 :  3   dense_24 :  4\n",
      "main_path124_24 :  2   dropout_24 :  3\n",
      "main_path64_24 :  1   main_path124_24 :  2\n",
      "exit_24 :  0   main_path64_24 :  1\n",
      "branch_4 :  12   max_pooling2d_83 :  13\n",
      "[19, 18, 17, 16, 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0]\n"
     ]
    }
   ],
   "source": [
    "# model.call??\n",
    "# model._run_internal_graph??\n",
    "# model._map_graph_network(model.inputs,model.outputs, False)\n",
    "# print(\"\")\n",
    "# print(\"reverse\")\n",
    "# print(\"\")\n",
    "model._map_graph_network(model.inputs,model.outputs, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_run_internal_graph --custom\n",
      "input_26\n",
      "conv2d_110\n",
      "batch_normalization_110\n",
      "max_pooling2d_70\n",
      "conv2d_111\n",
      "batch_normalization_111\n",
      "max_pooling2d_71\n",
      "branch_exit_22\n",
      "branch exit activated\n",
      "[<tf.Tensor: shape=(1, 10), dtype=float32, numpy=array([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]], dtype=float32)>, [<tf.Tensor: shape=(1, 10), dtype=float32, numpy=\n",
      "array([[  6.8509197,   3.2038624,   9.884068 ,  -2.0487866,  22.573503 ,\n",
      "        -16.507559 , -23.67594  ,   3.1319938, -24.057451 ,  17.183311 ]],\n",
      "      dtype=float32)>]]\n"
     ]
    }
   ],
   "source": [
    "# %%timeit\n",
    "# print(test_ds.take(1).get_single_element())\n",
    "x  = test_ds.take(1).get_single_element()\n",
    "# for i in range(32):\n",
    "result = model(x[0])\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['__abs__', '__add__', '__and__', '__bool__', '__ceil__', '__class__', '__delattr__', '__dir__', '__divmod__', '__doc__', '__eq__', '__float__', '__floor__', '__floordiv__', '__format__', '__ge__', '__getattribute__', '__getnewargs__', '__gt__', '__hash__', '__index__', '__init__', '__init_subclass__', '__int__', '__invert__', '__le__', '__lshift__', '__lt__', '__mod__', '__mul__', '__ne__', '__neg__', '__new__', '__or__', '__pos__', '__pow__', '__radd__', '__rand__', '__rdivmod__', '__reduce__', '__reduce_ex__', '__repr__', '__rfloordiv__', '__rlshift__', '__rmod__', '__rmul__', '__ror__', '__round__', '__rpow__', '__rrshift__', '__rshift__', '__rsub__', '__rtruediv__', '__rxor__', '__setattr__', '__sizeof__', '__str__', '__sub__', '__subclasshook__', '__truediv__', '__trunc__', '__xor__', 'bit_length', 'conjugate', 'denominator', 'from_bytes', 'imag', 'numerator', 'real', 'to_bytes']\n",
      "['__abs__', '__add__', '__and__', '__bool__', '__ceil__', '__class__', '__delattr__', '__dir__', '__divmod__', '__doc__', '__eq__', '__float__', '__floor__', '__floordiv__', '__format__', '__ge__', '__getattribute__', '__getnewargs__', '__gt__', '__hash__', '__index__', '__init__', '__init_subclass__', '__int__', '__invert__', '__le__', '__lshift__', '__lt__', '__mod__', '__mul__', '__ne__', '__neg__', '__new__', '__or__', '__pos__', '__pow__', '__radd__', '__rand__', '__rdivmod__', '__reduce__', '__reduce_ex__', '__repr__', '__rfloordiv__', '__rlshift__', '__rmod__', '__rmul__', '__ror__', '__round__', '__rpow__', '__rrshift__', '__rshift__', '__rsub__', '__rtruediv__', '__rxor__', '__setattr__', '__sizeof__', '__str__', '__sub__', '__subclasshook__', '__truediv__', '__trunc__', '__xor__', 'bit_length', 'conjugate', 'denominator', 'from_bytes', 'imag', 'numerator', 'real', 'to_bytes']\n",
      "['__abs__', '__add__', '__and__', '__bool__', '__ceil__', '__class__', '__delattr__', '__dir__', '__divmod__', '__doc__', '__eq__', '__float__', '__floor__', '__floordiv__', '__format__', '__ge__', '__getattribute__', '__getnewargs__', '__gt__', '__hash__', '__index__', '__init__', '__init_subclass__', '__int__', '__invert__', '__le__', '__lshift__', '__lt__', '__mod__', '__mul__', '__ne__', '__neg__', '__new__', '__or__', '__pos__', '__pow__', '__radd__', '__rand__', '__rdivmod__', '__reduce__', '__reduce_ex__', '__repr__', '__rfloordiv__', '__rlshift__', '__rmod__', '__rmul__', '__ror__', '__round__', '__rpow__', '__rrshift__', '__rshift__', '__rsub__', '__rtruediv__', '__rxor__', '__setattr__', '__sizeof__', '__str__', '__sub__', '__subclasshook__', '__truediv__', '__trunc__', '__xor__', 'bit_length', 'conjugate', 'denominator', 'from_bytes', 'imag', 'numerator', 'real', 'to_bytes']\n",
      "['__abs__', '__add__', '__and__', '__bool__', '__ceil__', '__class__', '__delattr__', '__dir__', '__divmod__', '__doc__', '__eq__', '__float__', '__floor__', '__floordiv__', '__format__', '__ge__', '__getattribute__', '__getnewargs__', '__gt__', '__hash__', '__index__', '__init__', '__init_subclass__', '__int__', '__invert__', '__le__', '__lshift__', '__lt__', '__mod__', '__mul__', '__ne__', '__neg__', '__new__', '__or__', '__pos__', '__pow__', '__radd__', '__rand__', '__rdivmod__', '__reduce__', '__reduce_ex__', '__repr__', '__rfloordiv__', '__rlshift__', '__rmod__', '__rmul__', '__ror__', '__round__', '__rpow__', '__rrshift__', '__rshift__', '__rsub__', '__rtruediv__', '__rxor__', '__setattr__', '__sizeof__', '__str__', '__sub__', '__subclasshook__', '__truediv__', '__trunc__', '__xor__', 'bit_length', 'conjugate', 'denominator', 'from_bytes', 'imag', 'numerator', 'real', 'to_bytes']\n",
      "['__abs__', '__add__', '__and__', '__bool__', '__ceil__', '__class__', '__delattr__', '__dir__', '__divmod__', '__doc__', '__eq__', '__float__', '__floor__', '__floordiv__', '__format__', '__ge__', '__getattribute__', '__getnewargs__', '__gt__', '__hash__', '__index__', '__init__', '__init_subclass__', '__int__', '__invert__', '__le__', '__lshift__', '__lt__', '__mod__', '__mul__', '__ne__', '__neg__', '__new__', '__or__', '__pos__', '__pow__', '__radd__', '__rand__', '__rdivmod__', '__reduce__', '__reduce_ex__', '__repr__', '__rfloordiv__', '__rlshift__', '__rmod__', '__rmul__', '__ror__', '__round__', '__rpow__', '__rrshift__', '__rshift__', '__rsub__', '__rtruediv__', '__rxor__', '__setattr__', '__sizeof__', '__str__', '__sub__', '__subclasshook__', '__truediv__', '__trunc__', '__xor__', 'bit_length', 'conjugate', 'denominator', 'from_bytes', 'imag', 'numerator', 'real', 'to_bytes']\n",
      "['__abs__', '__add__', '__and__', '__bool__', '__ceil__', '__class__', '__delattr__', '__dir__', '__divmod__', '__doc__', '__eq__', '__float__', '__floor__', '__floordiv__', '__format__', '__ge__', '__getattribute__', '__getnewargs__', '__gt__', '__hash__', '__index__', '__init__', '__init_subclass__', '__int__', '__invert__', '__le__', '__lshift__', '__lt__', '__mod__', '__mul__', '__ne__', '__neg__', '__new__', '__or__', '__pos__', '__pow__', '__radd__', '__rand__', '__rdivmod__', '__reduce__', '__reduce_ex__', '__repr__', '__rfloordiv__', '__rlshift__', '__rmod__', '__rmul__', '__ror__', '__round__', '__rpow__', '__rrshift__', '__rshift__', '__rsub__', '__rtruediv__', '__rxor__', '__setattr__', '__sizeof__', '__str__', '__sub__', '__subclasshook__', '__truediv__', '__trunc__', '__xor__', 'bit_length', 'conjugate', 'denominator', 'from_bytes', 'imag', 'numerator', 'real', 'to_bytes']\n",
      "['__abs__', '__add__', '__and__', '__bool__', '__ceil__', '__class__', '__delattr__', '__dir__', '__divmod__', '__doc__', '__eq__', '__float__', '__floor__', '__floordiv__', '__format__', '__ge__', '__getattribute__', '__getnewargs__', '__gt__', '__hash__', '__index__', '__init__', '__init_subclass__', '__int__', '__invert__', '__le__', '__lshift__', '__lt__', '__mod__', '__mul__', '__ne__', '__neg__', '__new__', '__or__', '__pos__', '__pow__', '__radd__', '__rand__', '__rdivmod__', '__reduce__', '__reduce_ex__', '__repr__', '__rfloordiv__', '__rlshift__', '__rmod__', '__rmul__', '__ror__', '__round__', '__rpow__', '__rrshift__', '__rshift__', '__rsub__', '__rtruediv__', '__rxor__', '__setattr__', '__sizeof__', '__str__', '__sub__', '__subclasshook__', '__truediv__', '__trunc__', '__xor__', 'bit_length', 'conjugate', 'denominator', 'from_bytes', 'imag', 'numerator', 'real', 'to_bytes']\n",
      "['__abs__', '__add__', '__and__', '__bool__', '__ceil__', '__class__', '__delattr__', '__dir__', '__divmod__', '__doc__', '__eq__', '__float__', '__floor__', '__floordiv__', '__format__', '__ge__', '__getattribute__', '__getnewargs__', '__gt__', '__hash__', '__index__', '__init__', '__init_subclass__', '__int__', '__invert__', '__le__', '__lshift__', '__lt__', '__mod__', '__mul__', '__ne__', '__neg__', '__new__', '__or__', '__pos__', '__pow__', '__radd__', '__rand__', '__rdivmod__', '__reduce__', '__reduce_ex__', '__repr__', '__rfloordiv__', '__rlshift__', '__rmod__', '__rmul__', '__ror__', '__round__', '__rpow__', '__rrshift__', '__rshift__', '__rsub__', '__rtruediv__', '__rxor__', '__setattr__', '__sizeof__', '__str__', '__sub__', '__subclasshook__', '__truediv__', '__trunc__', '__xor__', 'bit_length', 'conjugate', 'denominator', 'from_bytes', 'imag', 'numerator', 'real', 'to_bytes']\n",
      "['__abs__', '__add__', '__and__', '__bool__', '__ceil__', '__class__', '__delattr__', '__dir__', '__divmod__', '__doc__', '__eq__', '__float__', '__floor__', '__floordiv__', '__format__', '__ge__', '__getattribute__', '__getnewargs__', '__gt__', '__hash__', '__index__', '__init__', '__init_subclass__', '__int__', '__invert__', '__le__', '__lshift__', '__lt__', '__mod__', '__mul__', '__ne__', '__neg__', '__new__', '__or__', '__pos__', '__pow__', '__radd__', '__rand__', '__rdivmod__', '__reduce__', '__reduce_ex__', '__repr__', '__rfloordiv__', '__rlshift__', '__rmod__', '__rmul__', '__ror__', '__round__', '__rpow__', '__rrshift__', '__rshift__', '__rsub__', '__rtruediv__', '__rxor__', '__setattr__', '__sizeof__', '__str__', '__sub__', '__subclasshook__', '__truediv__', '__trunc__', '__xor__', 'bit_length', 'conjugate', 'denominator', 'from_bytes', 'imag', 'numerator', 'real', 'to_bytes']\n",
      "['__abs__', '__add__', '__and__', '__bool__', '__ceil__', '__class__', '__delattr__', '__dir__', '__divmod__', '__doc__', '__eq__', '__float__', '__floor__', '__floordiv__', '__format__', '__ge__', '__getattribute__', '__getnewargs__', '__gt__', '__hash__', '__index__', '__init__', '__init_subclass__', '__int__', '__invert__', '__le__', '__lshift__', '__lt__', '__mod__', '__mul__', '__ne__', '__neg__', '__new__', '__or__', '__pos__', '__pow__', '__radd__', '__rand__', '__rdivmod__', '__reduce__', '__reduce_ex__', '__repr__', '__rfloordiv__', '__rlshift__', '__rmod__', '__rmul__', '__ror__', '__round__', '__rpow__', '__rrshift__', '__rshift__', '__rsub__', '__rtruediv__', '__rxor__', '__setattr__', '__sizeof__', '__str__', '__sub__', '__subclasshook__', '__truediv__', '__trunc__', '__xor__', 'bit_length', 'conjugate', 'denominator', 'from_bytes', 'imag', 'numerator', 'real', 'to_bytes']\n",
      "['__abs__', '__add__', '__and__', '__bool__', '__ceil__', '__class__', '__delattr__', '__dir__', '__divmod__', '__doc__', '__eq__', '__float__', '__floor__', '__floordiv__', '__format__', '__ge__', '__getattribute__', '__getnewargs__', '__gt__', '__hash__', '__index__', '__init__', '__init_subclass__', '__int__', '__invert__', '__le__', '__lshift__', '__lt__', '__mod__', '__mul__', '__ne__', '__neg__', '__new__', '__or__', '__pos__', '__pow__', '__radd__', '__rand__', '__rdivmod__', '__reduce__', '__reduce_ex__', '__repr__', '__rfloordiv__', '__rlshift__', '__rmod__', '__rmul__', '__ror__', '__round__', '__rpow__', '__rrshift__', '__rshift__', '__rsub__', '__rtruediv__', '__rxor__', '__setattr__', '__sizeof__', '__str__', '__sub__', '__subclasshook__', '__truediv__', '__trunc__', '__xor__', 'bit_length', 'conjugate', 'denominator', 'from_bytes', 'imag', 'numerator', 'real', 'to_bytes']\n"
     ]
    }
   ],
   "source": [
    "for nodes in model._nodes_by_depth:\n",
    "    print(dir(nodes))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[5.0970970e-20 2.4690530e-09 1.2282757e-02 2.9390292e-15 3.4506284e-05\n",
      "  2.4547730e-13 3.0208079e-11 9.8768264e-01 8.1208469e-12 1.5698492e-07]]\n"
     ]
    }
   ],
   "source": [
    "# %%timeit\n",
    "#using basic model with custom call function\n",
    "# model.turnOnThresholds()\n",
    "result = model.predict(test_ds.take(1))\n",
    "print(result)\n",
    "# predictions = []\n",
    "# for i in result:\n",
    "    # predictions.append(np.argmax(i,axis=1))\n",
    "# print(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "380 ms ± 1.48 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "## using no custom call functionality\n",
    "\n",
    "# model.turnOnThresholds()\n",
    "result = model.predict(test_ds.take(126))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_run_internal_graph --custom\n",
      "input_30\n",
      "conv2d_130\n",
      "batch_normalization_130\n",
      "max_pooling2d_82\n",
      "conv2d_131\n",
      "batch_normalization_131\n",
      "max_pooling2d_83\n",
      "branch_4\n",
      "conv2d_132\n",
      "batch_normalization_132\n",
      "conv2d_133\n",
      "batch_normalization_133\n",
      "conv2d_134\n",
      "batch_normalization_134\n",
      "max_pooling2d_84\n",
      "flatten_24\n",
      "dense_24\n",
      "dropout_24\n",
      "main_path124_24\n",
      "main_path64_24\n",
      "exit_24\n",
      "385 ms ± 6.29 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "## using custom call functionality\n",
    "\n",
    "# model.turnOnThresholds()\n",
    "result = model.predict(test_ds.take(126))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# %%timeit\n",
    "# print(test_ds.take(1).get_single_element())\n",
    "x  = test_ds.take(1).get_single_element()\n",
    "# for i in range(32):\n",
    "result = model(x[0])\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "394 ms ± 4.94 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "# print(test_ds.take(1).get_single_element())\n",
    "x  = test_ds.take(1).get_single_element()\n",
    "for i in range(126):\n",
    "    result = model(x[0])\n",
    "# print(result)\n",
    "\n",
    "#### simpleModel with custom call function... surprisingly very very similar to the normal method..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "431 ms ± 13.2 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "# print(test_ds.take(1).get_single_element())\n",
    "x  = test_ds.take(1).get_single_element()\n",
    "for i in range(126):\n",
    "    result = model(x[0])\n",
    "# print(result)\n",
    "\n",
    "#### simpleModel with custom call function... surprisingly very very similar to the normal method..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 376ms/step - loss: 2.3951 - accuracy: 0.0000e+00\n"
     ]
    }
   ],
   "source": [
    "result = model.evaluate(test_ds.take(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 276ms/step - loss: 0.7777 - accuracy: 0.0000e+00\n"
     ]
    }
   ],
   "source": [
    "result = model.evaluate(test_ds.take(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
