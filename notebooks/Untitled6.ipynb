{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c691df8e-e6e7-45d4-bf9b-da66d14ad438",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c60e3f2-6d9b-4996-a8f1-bbb6e5f68eac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8d29f43-14e5-496a-82b7-5611035f85e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def dirichlet_prior_network_uncertainty(logits, epsilon=1e-10, alpha_correction=True):\n",
    "    \"\"\"\n",
    "    :param logits:\n",
    "    :param epsilon:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "\n",
    "    logits = np.asarray(logits, dtype=np.float64)\n",
    "    alphas = np.exp(logits)\n",
    "\n",
    "    alphas = np.clip(alphas, 0, np.finfo(np.dtype(\"float32\")).max)\n",
    "\n",
    "    if alpha_correction:\n",
    "        alphas = alphas + 1\n",
    "\n",
    "    alpha0 = np.sum(alphas, axis=1, keepdims=True)\n",
    "    probs = alphas / alpha0\n",
    "\n",
    "    conf = np.max(probs, axis=1)\n",
    "\n",
    "    entropy_of_exp = -np.sum(probs * np.log(probs + epsilon), axis=1)\n",
    "    expected_entropy = -np.sum(\n",
    "        (alphas / alpha0) * (digamma(alphas + 1) - digamma(alpha0 + 1.0)), axis=1\n",
    "    )\n",
    "\n",
    "    mutual_info = entropy_of_exp - expected_entropy\n",
    "\n",
    "    epkl = np.squeeze((alphas.shape[1] - 1.0) / alpha0)\n",
    "\n",
    "    dentropy = (\n",
    "        np.sum(\n",
    "            gammaln(alphas) - (alphas - 1.0) * (digamma(alphas) - digamma(alpha0)),\n",
    "            axis=1,\n",
    "            keepdims=True,\n",
    "        )\n",
    "        - gammaln(alpha0)\n",
    "    )\n",
    "\n",
    "    uncertainty = {\n",
    "        \"confidence_alea_uncert.\": conf,\n",
    "        \"entropy_of_expected\": -entropy_of_exp,\n",
    "        \"expected_entropy\": -expected_entropy,\n",
    "        \"mutual_information\": -mutual_info,\n",
    "        \"EPKL\": -epkl,\n",
    "        \"differential_entropy\": -np.squeeze(dentropy),\n",
    "    }\n",
    "\n",
    "    return {k: torch.from_numpy(v).float() for k, v in uncertainty.items()}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py36",
   "language": "python",
   "name": "py36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
