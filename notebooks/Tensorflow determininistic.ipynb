{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deterministic Tensorflow \n",
    "\n",
    "## Getting repeatable results with keras models.\n",
    "\n",
    "---\n",
    "If your like me then during your Machine learning work you've struggled with getting repeatable results out of your deep learning models. Often when building a network you don't have too much leeway to repeat and retest your model and training cycle to confirm your results and instead you have to take what you have gotten first time around and as long as its passed your validation and testing requirements, run with that to show off your work. However if you have the luxury (or requirement)  of retraining and repeating your work you will most likely find that given the same training settings, hyperparameters and training time, the final results of your hard work will be different. If you are lucky only slightly different and a positive change, but if your deadline is close, more likely it will be moderately worse and throw your whole hyposthesis and last couple of weeks work into doubt. This isn't an unexpected outcome and is part and parcel of using machine learning approaches that are non-deterministic and based on multiple random variables to kickstart change into the model. But still it can be a real blow to the confidence of your results especially when you are trying out something new, hoping to get a small improvement that is smaller then the possible variance of the original model's performance. <br>\n",
    "Now the standard approach to deal with these varying results is to just simply repeat the process mutliple times and then average the result and publish that result with the footnote that this value was a collected average. When you don't see this it can put into doubt the exact accuracy of a paper's proclaimed results, was this result representative of the average expected performance? or instead a lucky random seed that performed extra well? \n",
    "<br>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The core parts of this tutorial are recongition of the various sources of randomness that are present in tensorflow and machine learning models.\n",
    "<li>\n",
    "    first the seed randomness influencing weight creation and updates </li>\n",
    "    <li>next the gpu randomness sources </li>\n",
    "   <li> then the randomness of shuffling the dataset </li>\n",
    "   <li> and finally the effect on randomness of the order of execution (particularlly in notebook scenarios) </li>\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "import os\n",
    "sys.path.append(\"..\") # Adds higher directory to python modules path.\n",
    "import branchingdnn as branching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.3.1\n",
      "2.2.0\n",
      "[[11.435026]]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import keras\n",
    "print(keras.__version__)\n",
    "print (tf.__version__)\n",
    "from numpy import array\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.preprocessing.sequence import TimeseriesGenerator\n",
    "# define dataset\n",
    "series = array([1, 2, 3, 4, 5, 6, 7, 8, 9, 10])\n",
    "# define generator\n",
    "n_input = 2\n",
    "generator = TimeseriesGenerator(series, series, length=n_input, batch_size=8)\n",
    "# define model\n",
    "model = Sequential()\n",
    "model.add(Dense(100, activation='relu', input_dim=n_input))\n",
    "model.add(Dense(1))\n",
    "model.compile(optimizer='adam', loss='mse')\n",
    "# fit model\n",
    "model.fit_generator(generator, steps_per_epoch=1, epochs=200, verbose=0)\n",
    "# make a one step prediction out of sample\n",
    "x_input = array([9, 10]).reshape((1, n_input))\n",
    "yhat = model.predict(x_input, verbose=0)\n",
    "print(yhat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.6.0\n",
      "2.6.2\n",
      "1.19.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Sanity\\AppData\\Local\\conda\\conda\\envs\\py36\\lib\\site-packages\\keras\\engine\\training.py:1972: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
      "  warnings.warn('`Model.fit_generator` is deprecated and '\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import keras\n",
    "import numpy as np\n",
    "\n",
    "print(keras.__version__)\n",
    "print (tf.__version__)\n",
    "print (np.__version__)\n",
    "\n",
    "from numpy import array\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.preprocessing.sequence import TimeseriesGenerator\n",
    "# define dataset\n",
    "series = array([1, 2, 3, 4, 5, 6, 7, 8, 9, 10])\n",
    "# define generator\n",
    "n_input = 2\n",
    "generator = TimeseriesGenerator(series, series, length=n_input, batch_size=8)\n",
    "# define model\n",
    "model = Sequential()\n",
    "model.add(Dense(100, activation='relu', input_dim=n_input))\n",
    "model.add(Dense(1))\n",
    "model.compile(optimizer='adam', loss='mse')\n",
    "# fit model\n",
    "model.fit_generator(generator, steps_per_epoch=1, epochs=200, verbose=0)\n",
    "# make a one step prediction out of sample\n",
    "x_input = array([9, 10]).reshape((1, n_input))\n",
    "yhat = model.predict(x_input, verbose=0)\n",
    "print(yhat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "shuffle() missing 1 required positional argument: 'buffer_size'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-ffb4d171052d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[0mtest_images\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_labels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtest_images\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_labels\u001b[0m \u001b[1;31m# now remove the validation set from the training set.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m \u001b[0mtrain_ds\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrom_tensor_slices\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_images\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_labels\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     14\u001b[0m \u001b[0mtest_ds\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrom_tensor_slices\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_images\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_labels\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[0mvalidation_ds\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrom_tensor_slices\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalidation_images\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation_labels\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: shuffle() missing 1 required positional argument: 'buffer_size'"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import time\n",
    "(train_images, train_labels), (test_images, test_labels) = keras.datasets.cifar10.load_data()\n",
    "CLASS_NAMES= ['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n",
    "###normal method\n",
    "validation_images, validation_labels = train_images[:5000], train_labels[:5000] #get the first 5k training samples as validation set\n",
    "train_images, train_labels = train_images[5000:], train_labels[5000:] # now remove the validation set from the training set.\n",
    "test_images, test_labels = test_images, test_labels # now remove the validation set from the training set.\n",
    "\n",
    "train_ds = tf.data.Dataset.from_tensor_slices((train_images, train_labels)).shuffle()\n",
    "test_ds = tf.data.Dataset.from_tensor_slices((test_images, test_labels))\n",
    "validation_ds = tf.data.Dataset.from_tensor_slices((validation_images, validation_labels))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "root_logdir = os.path.join(os.curdir, \"logs\\\\fit\\\\\")\n",
    "\n",
    "def get_run_logdir():\n",
    "    run_id = time.strftime(\"run_%Y_%m_%d-%H_%M_%S\")\n",
    "    return os.path.join(root_logdir, run_id)\n",
    "\n",
    "run_logdir = get_run_logdir()\n",
    "tensorboard_cb = keras.callbacks.TensorBoard(run_logdir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = keras.Input(shape=(227,227,3))\n",
    "x = keras.layers.Conv2D(filters=96, kernel_size=(11,11), strides=(4,4), activation='relu', input_shape=(227,227,3))(inputs)\n",
    "x = keras.layers.BatchNormalization()(x)\n",
    "x = keras.layers.MaxPool2D(pool_size=(3,3), strides=(2,2))(x)\n",
    "x = keras.layers.Conv2D(filters=256, kernel_size=(5,5), strides=(1,1), activation='relu', padding=\"same\")(x)\n",
    "x = keras.layers.BatchNormalization()(x)\n",
    "x = keras.layers.MaxPool2D(pool_size=(3,3), strides=(2,2))(x)\n",
    "x = keras.layers.Conv2D(filters=384, kernel_size=(3,3), strides=(1,1), activation='relu', padding=\"same\")(x)\n",
    "x = keras.layers.BatchNormalization()(x)\n",
    "x = keras.layers.Conv2D(filters=384, kernel_size=(1,1), strides=(1,1), activation='relu', padding=\"same\")(x)\n",
    "x = keras.layers.BatchNormalization()(x)\n",
    "x = keras.layers.Conv2D(filters=256, kernel_size=(1,1), strides=(1,1), activation='relu', padding=\"same\")(x)\n",
    "x = keras.layers.BatchNormalization()(x)\n",
    "x = keras.layers.MaxPool2D(pool_size=(3,3), strides=(2,2))(x)\n",
    "x = keras.layers.Flatten()(x)\n",
    "x = keras.layers.Dense(4096, activation='relu')(x)\n",
    "x = keras.layers.Dropout(0.5)(x)\n",
    "\n",
    "# ### first branch\n",
    "# branchLayer = keras.layers.Flatten(name=tf.compat.v1.get_default_graph().unique_name(\"branch_flatten\"))(x)\n",
    "# branchLayer = keras.layers.Dense(124, activation=\"relu\",name=tf.compat.v1.get_default_graph().unique_name(\"branch124\"))(branchLayer)\n",
    "# branchLayer = keras.layers.Dense(64, activation=\"relu\",name=tf.compat.v1.get_default_graph().unique_name(\"branch64\"))(branchLayer)\n",
    "# branchLayer = keras.layers.Dense(10, name=tf.compat.v1.get_default_graph().unique_name(\"branch_output\"))(branchLayer)\n",
    "\n",
    "x = keras.layers.Dense(4096, activation='relu')(x)\n",
    "x = keras.layers.Dropout(0.5)(x)\n",
    "\n",
    "# ### second Branch\n",
    "# branchLayer2 = keras.layers.Flatten(name=tf.compat.v1.get_default_graph().unique_name(\"branch_flatten\"))(x)\n",
    "# branchLayer2 = keras.layers.Dense(10, name=tf.compat.v1.get_default_graph().unique_name(\"branch_output\"))(branchLayer2)\n",
    "\n",
    "x = keras.layers.Dense(10, activation='softmax')(x)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.Model(inputs=inputs, outputs=[x], name=\"alexnet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='sparse_categorical_crossentropy', optimizer=tf.optimizers.SGD(lr=0.001,momentum=0.9), metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# os.environ['CUDA_VISIBLE_DEVICES'] = '-1'\n",
    "import tensorflow as tf\n",
    "os.environ['TF_DETERMINISTIC_OPS'] = '1'\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "import os\n",
    "import keras\n",
    "sys.path.append(\"..\") # Adds higher directory to python modules path.\n",
    "import branchingdnn as branching\n",
    "# dataset = branching.dataset.prepare.dataset(tf.keras.datasets.cifar10.load_data(),64,5000,22500,(227,227),include_targets=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#minst\n",
    "from tensorflow.keras import layers, models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape: (60000, 28, 28, 1)\n",
      "60000 train samples\n",
      "10000 test samples\n"
     ]
    }
   ],
   "source": [
    "num_classes = 10\n",
    "input_shape = (28, 28, 1)\n",
    "\n",
    "# the data, split between train and test sets\n",
    "(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()\n",
    "\n",
    "# Scale images to the [0, 1] range\n",
    "x_train = x_train.astype(\"float32\") / 255\n",
    "x_test = x_test.astype(\"float32\") / 255\n",
    "# Make sure images have shape (28, 28, 1)\n",
    "x_train = np.expand_dims(x_train, -1)\n",
    "x_test = np.expand_dims(x_test, -1)\n",
    "print(\"x_train shape:\", x_train.shape)\n",
    "print(x_train.shape[0], \"train samples\")\n",
    "print(x_test.shape[0], \"test samples\")\n",
    "\n",
    "\n",
    "# convert class vectors to binary class matrices\n",
    "y_train = tf.keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = tf.keras.utils.to_categorical(y_test, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "summary of trainable variables before training: 19.9035952091217\n",
      "422/422 [==============================] - 3s 4ms/step - loss: 0.3412 - accuracy: 0.8958 - val_loss: 0.1167 - val_accuracy: 0.9653\n",
      "summary of trainable variables after training: -3558.5371806323528\n",
      "summary of trainable variables before training: 19.9035952091217\n",
      "422/422 [==============================] - 2s 4ms/step - loss: 0.3412 - accuracy: 0.8958 - val_loss: 0.1167 - val_accuracy: 0.9653\n",
      "summary of trainable variables after training: -3558.5371806323528\n",
      "summary of trainable variables before training: 19.9035952091217\n",
      "422/422 [==============================] - 2s 4ms/step - loss: 0.3412 - accuracy: 0.8958 - val_loss: 0.1167 - val_accuracy: 0.9653\n",
      "summary of trainable variables after training: -3558.5371806323528\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "def summarize_keras_trainable_variables(model, message):\n",
    "    s = sum(map(lambda x: x.sum(), model.get_weights()))\n",
    "    print(\"summary of trainable variables %s: %.13f\" % (message, s))\n",
    "    return s\n",
    "# train_ds, validation_ds, test_ds = dataset\n",
    "\n",
    "for i in range(3):\n",
    "    seed = 42\n",
    "    # random.seed(seed)\n",
    "    tf.random.set_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    outputs =[]\n",
    "    inputs = keras.Input(shape=(input_shape))\n",
    "    x = layers.Flatten(input_shape=(28,28))(inputs)\n",
    "    x = layers.Dense(512, activation=\"relu\")(x)\n",
    "    x= layers.Dropout(0.2)(x)\n",
    "    #exit 2\n",
    "    x = layers.Dense(512, activation=\"relu\")(x)\n",
    "    x= layers.Dropout(0.2)(x)\n",
    "    #exit 3\n",
    "    x = layers.Dense(512, activation=\"relu\")(x)\n",
    "    x= layers.Dropout(0.2)(x)\n",
    "    #exit 4\n",
    "    x = layers.Dense(512, activation=\"relu\")(x)\n",
    "    x= layers.Dropout(0.2)(x)\n",
    "    #exit 5\n",
    "    x = layers.Dense(512, activation=\"relu\")(x)\n",
    "    x= layers.Dropout(0.2)(x)\n",
    "    #exit 1 The main branch exit is refered to as \"exit 1\" or \"main exit\" to avoid confusion when adding addtional exits\n",
    "    output1 = layers.Dense(10, name=\"output1\")(x)\n",
    "    softmax = layers.Softmax()(output1)\n",
    "\n",
    "    outputs.append(softmax)\n",
    "    model = keras.Model(inputs=inputs, outputs=outputs, name=\"mnist_model_normal\")\n",
    "    batch_size = 128\n",
    "    epochs = 1\n",
    "    model.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "    summarize_keras_trainable_variables(model,\"before training\")\n",
    "    model.fit(x_train, y_train, shuffle=False,batch_size=batch_size, epochs=epochs, validation_split=0.1)\n",
    "    summarize_keras_trainable_variables(model,\"after training\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "422/422 [==============================] - 6s 14ms/step - loss: 0.3156 - accuracy: 0.9018 - val_loss: 0.1138 - val_accuracy: 0.9645\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1f0e27b8c50>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 128\n",
    "epochs = 1\n",
    "model.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "\n",
    "model.fit(x_train, y_train, batch_size=batch_size, epochs=epochs, validation_split=0.1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# tf.debugging.experimental.enable_dump_debug_info(logdir, tensor_debug_mode=\"FULL_HEALTH\", circular_buffer_size=-1)\n",
    "(train_images, train_labels), (test_images, test_labels) = keras.datasets.cifar10.load_data()\n",
    "\n",
    "CLASS_NAMES= ['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n",
    "\n",
    "# import csv\n",
    "# with open('results/altTrain_labels.csv', newline='') as f:\n",
    "    # reader = csv.reader(f,quoting=csv.QUOTE_NONNUMERIC)\n",
    "    # alt_trainLabels = list(reader)\n",
    "# with open('results/altTest_labels.csv', newline='') as f:\n",
    "    # reader = csv.reader(f,quoting=csv.QUOTE_NONNUMERIC)\n",
    "    # alt_testLabels = list(reader)\n",
    "\n",
    "# altTraining = tf.data.Dataset.from_tensor_slices((train_images,alt_trainLabels))\n",
    "\n",
    "# validation_images, validation_labels = train_images[:5000], alt_trainLabels[:5000]\n",
    "# train_ds = tf.data.Dataset.from_tensor_slices((train_images, alt_trainLabels))\n",
    "# test_ds = tf.data.Dataset.from_tensor_slices((test_images, alt_testLabels))\n",
    "train_labels = tf.keras.utils.to_categorical(train_labels,10)\n",
    "test_labels = tf.keras.utils.to_categorical(test_labels,10)\n",
    "\n",
    "###normal method\n",
    "validation_images, validation_labels = train_images[:5000], train_labels[:5000] #get the first 5k training samples as validation set\n",
    "train_images, train_labels = train_images[5000:], train_labels[5000:] # now remove the validation set from the training set.\n",
    "train_ds = tf.data.Dataset.from_tensor_slices((train_images, train_labels))\n",
    "test_ds = tf.data.Dataset.from_tensor_slices((test_images, test_labels))\n",
    "validation_ds = tf.data.Dataset.from_tensor_slices((validation_images, validation_labels))\n",
    "\n",
    "def augment_images(image, label):\n",
    "    # Normalize images to have a mean of 0 and standard deviation of 1\n",
    "    # image = tf.image.per_image_standardization(image)\n",
    "    # Resize images from 32x32 to 277x277\n",
    "    image = tf.image.resize(image, (227,227))\n",
    "    return image, label\n",
    "\n",
    "train_ds_size = len(list(train_ds))\n",
    "test_ds_size = len(list(test_ds))\n",
    "validation_ds_size = len(list(validation_ds))\n",
    "\n",
    "train_ds = (train_ds\n",
    "                  .map(augment_images)\n",
    "                  .shuffle(buffer_size=train_ds_size,seed=42,reshuffle_each_iteration=False)\n",
    "                  .batch(batch_size=128, drop_remainder=True))\n",
    "\n",
    "test_ds = (test_ds\n",
    "                  .map(augment_images)\n",
    "                #   .shuffle(buffer_size=train_ds_size)\n",
    "                  .batch(batch_size=128, drop_remainder=True))\n",
    "\n",
    "validation_ds = (validation_ds\n",
    "                  .map(augment_images)\n",
    "                #   .shuffle(buffer_size=validation_ds_size)\n",
    "                  .batch(batch_size=128, drop_remainder=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = keras.Input(shape=(227,227,3))\n",
    "x = keras.layers.Conv2D(filters=96, kernel_size=(11,11), strides=(4,4), activation='relu', input_shape=(227,227,3))(inputs)\n",
    "x = keras.layers.BatchNormalization()(x)\n",
    "x = keras.layers.MaxPool2D(pool_size=(3,3), strides=(2,2))(x)\n",
    "x = keras.layers.Conv2D(filters=256, kernel_size=(5,5), strides=(1,1), activation='relu', padding=\"same\")(x)\n",
    "x = keras.layers.BatchNormalization()(x)\n",
    "x = keras.layers.MaxPool2D(pool_size=(3,3), strides=(2,2))(x)\n",
    "x = keras.layers.Conv2D(filters=384, kernel_size=(3,3), strides=(1,1), activation='relu', padding=\"same\")(x)\n",
    "x = keras.layers.BatchNormalization()(x)\n",
    "x = keras.layers.Conv2D(filters=384, kernel_size=(1,1), strides=(1,1), activation='relu', padding=\"same\")(x)\n",
    "x = keras.layers.BatchNormalization()(x)\n",
    "x = keras.layers.Conv2D(filters=256, kernel_size=(1,1), strides=(1,1), activation='relu', padding=\"same\")(x)\n",
    "x = keras.layers.BatchNormalization()(x)\n",
    "x = keras.layers.MaxPool2D(pool_size=(3,3), strides=(2,2))(x)\n",
    "x = keras.layers.Flatten()(x)\n",
    "x = keras.layers.Dense(4096, activation='relu')(x)\n",
    "x = keras.layers.Dropout(0.5)(x)\n",
    "x = keras.layers.Dense(4096, activation='relu')(x)\n",
    "x = keras.layers.Dropout(0.5)(x)\n",
    "x = keras.layers.Dense(10, activation='softmax')(x)\n",
    "model = keras.Model(inputs=inputs, outputs=[x], name=\"alexnet\")    \n",
    "model.compile(loss='categorical_crossentropy', optimizer=tf.optimizers.SGD(lr=0.001,momentum=0.9), metrics=['accuracy'])\n",
    "\n",
    "model.save(\"distil_test.hdf5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "summary of trainable variables before training: 2931.7305606603622\n",
      "351/351 [==============================] - 185s 346ms/step - loss: 1.9391 - accuracy: 0.3624 - val_loss: 1.4100 - val_accuracy: 0.4982\n",
      "summary of trainable variables after training: 2656064.6382689797319\n",
      "78/78 [==============================] - 3s 41ms/step - loss: 1.4122 - accuracy: 0.4883\n",
      "[1.4122194051742554, 0.48828125]\n",
      "summary of trainable variables before training: 2931.7305606603622\n",
      "351/351 [==============================] - 136s 231ms/step - loss: 1.9391 - accuracy: 0.3624 - val_loss: 1.4100 - val_accuracy: 0.4982\n",
      "summary of trainable variables after training: 2656064.6382689797319\n",
      "78/78 [==============================] - 3s 40ms/step - loss: 1.4122 - accuracy: 0.4883\n",
      "[1.4122194051742554, 0.48828125]\n",
      "summary of trainable variables before training: 2931.7305606603622\n",
      "351/351 [==============================] - 78s 183ms/step - loss: 1.9391 - accuracy: 0.3624 - val_loss: 1.4100 - val_accuracy: 0.4982\n",
      "summary of trainable variables after training: 2656064.6382689797319\n",
      "78/78 [==============================] - 3s 38ms/step - loss: 1.4122 - accuracy: 0.4883\n",
      "[1.4122194051742554, 0.48828125]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "seed = 42\n",
    "# random.seed(seed)\n",
    "tf.random.set_seed(seed)\n",
    "np.random.seed(seed)\n",
    "\n",
    "def summarize_keras_trainable_variables(model, message):\n",
    "    s = sum(map(lambda x: x.sum(), model.get_weights()))\n",
    "    print(\"summary of trainable variables %s: %.13f\" % (message, s))\n",
    "    return s\n",
    "# train_ds, validation_ds, test_ds = dataset\n",
    "\n",
    "\n",
    "for i in range(3):\n",
    "    tf.random.set_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    model = tf.keras.models.load_model(\"distil_test.hdf5\")\n",
    "    summarize_keras_trainable_variables(model,\"before training\")\n",
    "    \n",
    "    model.fit(train_ds, shuffle=False,validation_data = validation_ds,epochs=1)\n",
    "    summarize_keras_trainable_variables(model,\"after training\")\n",
    "    results = model.evaluate(test_ds)\n",
    "    print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py36",
   "language": "python",
   "name": "py36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
