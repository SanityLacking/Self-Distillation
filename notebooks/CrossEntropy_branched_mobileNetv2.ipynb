{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "import os\n",
    "sys.path.append(\"..\") # Adds higher directory to python modules path.\n",
    "import branchingdnn as branching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (train_ds, test_ds, validation_ds) = branching.dataset.prepare.dataset(tf.keras.datasets.cifar10.load_data(),32,5000,22500,(227,227),include_targets=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calcEntropy(y_hat):\n",
    "        #entropy is the sum of y * log(y) for all possible labels.\n",
    "        if isinstance(y_hat, list):\n",
    "            y_hat = np.array(y_hat)\n",
    "        sum_entropy = 0\n",
    "        if y_hat.ndim >1:\n",
    "            return list(map(calcEntropy,y_hat))\n",
    "        for i in range(len(y_hat)):\n",
    "            if y_hat[i] != 0: # log of zero is undefined, see MacKay's book \"Information Theory, Inference, and Learning Algorithms\"  for more info on this workaround reasoning.\n",
    "                entropy =y_hat[i] * math.log(y_hat[i],2)\n",
    "                sum_entropy +=  entropy\n",
    "\n",
    "        return -sum_entropy\n",
    "    \n",
    "def calcEntropy_Tensors(y_hat):\n",
    "        #entropy is the sum of y * log(y) for all possible labels.\n",
    "        #doesn't deal with cases of log(0)\n",
    "        rank = tf.rank(y_hat)\n",
    "        def calc_E(y_hat):\n",
    "            results = tf.clip_by_value((tf.math.log(y_hat)/tf.math.log(tf.constant(2, dtype=y_hat.dtype))), -1e12, 1e12)\n",
    "#             results = tf.clip_by_value(results, -1e12, 1e12)\n",
    "#             print(\"res \", results)\n",
    "            return tf.reduce_sum(y_hat * results)\n",
    "\n",
    "        sumEntropies = (tf.map_fn(calc_E,tf.cast(y_hat,'float')))\n",
    "        \n",
    "        if rank == 1:\n",
    "            sumEntropies = tf.reduce_sum(sumEntropies)\n",
    "        return -sumEntropies\n",
    "    \n",
    "def calcEntropy_Tensors2(y_hat):\n",
    "    #entropy is the sum of y * log(y) for all possible labels.\n",
    "    #doesn't deal with cases of log(0)\n",
    "#     num = tf.math.log(y_hat)\n",
    "# #     print(\"num\",num)\n",
    "#     dem = tf.math.log(tf.constant(2, dtype=y_hat.dtype))\n",
    "# #     print(\"dem\",dem)\n",
    "#     E = num / dem\n",
    "# #     print(\"E\",E)\n",
    "#     P = y_hat * E\n",
    "# #     print(\"p\",P)\n",
    "#     mean = tf.reduce_mean(tf.boolean_mask(P, tf.math.is_finite(P)))\n",
    "#     print(\"mean\",mean)\n",
    "#     sumEntropies = mean\n",
    "    val = y_hat * tf.math.log(y_hat)/tf.math.log(tf.constant(2, dtype=y_hat.dtype))\n",
    "    sumEntropies =  tf.reduce_mean(tf.boolean_mask(val,tf.math.is_finite(val)))\n",
    "    return -sumEntropies\n",
    "    \n",
    "# This function to generate evidence is used for the first example\n",
    "def relu_evidence(logits):\n",
    "    return tf.nn.relu(logits)\n",
    "\n",
    "# This one usually works better and used for the second and third examples\n",
    "# For general settings and different datasets, you may try this one first\n",
    "def exp_evidence(logits): \n",
    "    return tf.exp(tf.clip_by_value(logits,-10,10))\n",
    "\n",
    "# This one is another alternative and \n",
    "# usually behaves better than the relu_evidence \n",
    "def softplus_evidence(logits):\n",
    "    return tf.nn.softplus(logits)\n",
    "\n",
    "def KL(alpha):\n",
    "    # print(\"K:\",K)\n",
    "    beta=tf.constant(np.ones((1,K)),dtype=tf.float32)\n",
    "    S_alpha = tf.reduce_sum(alpha,axis=1,keepdims=True)\n",
    "    S_beta = tf.reduce_sum(beta,axis=1,keepdims=True)\n",
    "    lnB = tf.compat.v1.lgamma(S_alpha) - tf.reduce_sum(tf.compat.v1.lgamma(alpha),axis=1,keepdims=True)\n",
    "    lnB_uni = tf.reduce_sum(tf.compat.v1.lgamma(beta),axis=1,keepdims=True) - tf.compat.v1.lgamma(S_beta)\n",
    "\n",
    "    dg0 = tf.compat.v1.digamma(S_alpha)\n",
    "    dg1 = tf.compat.v1.digamma(alpha)\n",
    "\n",
    "    kl = tf.reduce_sum((alpha - beta)*(dg1-dg0),axis=1,keepdims=True) + lnB + lnB_uni\n",
    "    # print(\"kl\", kl)\n",
    "    return kl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CrossEntropyEndpoint(tf.keras.layers.Layer):\n",
    "        def __init__(self, num_outputs, name=None, **kwargs):\n",
    "            super(CrossEntropyEndpoint, self).__init__(name=name)\n",
    "            self.num_outputs = num_outputs\n",
    "#             self.kl = tf.keras.losses.KLDivergence()\n",
    "            self.loss_fn = tf.keras.losses.CategoricalCrossentropy()\n",
    "#             self.loss_fn = tf.keras.losses.categorical_crossentropy\n",
    "            self.evidence = softplus_evidence\n",
    "#             self.evidence = tf.compat.v1.distributions.Dirichlet\n",
    "            self.temperature = 10\n",
    "            self.lmb = 0.005\n",
    "        def build(self, input_shape):\n",
    "            self.kernel = self.add_weight(\"kernel\", shape=[int(input_shape[-1]), self.num_outputs])\n",
    "        \n",
    "        def get_config(self):\n",
    "            config = super().get_config().copy()\n",
    "            config.update({\n",
    "                'num_outputs': self.num_outputs,\n",
    "                'name': self.name\n",
    "            })\n",
    "            return config\n",
    "\n",
    "        def call(self, inputs, labels,learning_rate=1):\n",
    "            outputs = tf.matmul(inputs,self.kernel)\n",
    "            softmax = tf.nn.softmax(outputs)\n",
    "            evidence = self.evidence (outputs)\n",
    "            alpha = evidence + 1\n",
    "            u = self.num_outputs / tf.reduce_sum(alpha, axis=1, keepdims=True) #uncertainty\n",
    "          \n",
    "            # prob = alpha/tf.reduce_sum(alpha, 1, keepdims=True) \n",
    "            pred = tf.argmax(outputs,1)\n",
    "            truth = tf.argmax(labels,1)\n",
    "            match = tf.reshape(tf.cast(tf.equal(pred, truth), tf.float32),(-1,1))\n",
    "            # total_evidence = tf.reduce_sum(evidence,1, keepdims=True)\n",
    "            mean_succ = tf.reduce_sum(tf.reduce_sum(evidence,1, keepdims=True)*match) / tf.reduce_sum(match+1e-20)\n",
    "            mean_fail = tf.reduce_sum(tf.reduce_sum(tf.reduce_sum(evidence,1, keepdims=True)*(1-match)) / (tf.reduce_sum(tf.abs(1-match))+1e-20) )\n",
    "            \n",
    "            self.add_metric(evidence, name=self.name+\"_evidence\",aggregation='mean')\n",
    "            self.add_metric(mean_succ, name=self.name+\"_mean_ev_succ\",aggregation='mean')\n",
    "            self.add_metric(mean_fail, name=self.name+\"_mean_ev_fail\",aggregation='mean')\n",
    "            \n",
    "            return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_function(annealing_rate=1, momentum=1, decay=1, global_loss=False):\n",
    "    #create a wrapper function that returns a function\n",
    "    temperature = 1\n",
    "    Classes = 10\n",
    "    keras_kl = tf.keras.losses.KLDivergence()\n",
    "    annealing_rate = annealing_rate\n",
    "    momentum_rate = momentum\n",
    "    decay_rate = decay\n",
    "\n",
    "    def cross_entropy_evidence(labels, outputs): \n",
    "        softmax = tf.nn.softmax(outputs)\n",
    "        # activated_outputs =tf.keras.activations.sigmoid(softmax)\n",
    "        evidence = softplus_evidence(outputs)\n",
    "        alpha = evidence + 1\n",
    "        S = tf.reduce_sum(alpha, axis=1, keepdims=True) \n",
    "        E = alpha - 1\n",
    "        m = alpha / S\n",
    "        A = tf.reduce_sum((labels-m)**2, axis=1, keepdims=True) \n",
    "        B = tf.reduce_sum(alpha*(S-alpha)/(S*S*(S+1)), axis=1, keepdims=True) \n",
    "\n",
    "        annealing_coef = tf.minimum(1.0,tf.cast(annealing_rate,tf.float32))\n",
    "#         annealing_coef = 1\n",
    "        alp = E*(1-labels) + 1 \n",
    "        C =  annealing_coef * KL(alp)\n",
    "        # C = keras_kl(labels,evidence)\n",
    "        loss = tf.keras.losses.categorical_crossentropy(labels, softmax)\n",
    "        pred = tf.argmax(outputs,1)\n",
    "        truth = tf.argmax(labels,1)\n",
    "        match = tf.reshape(tf.cast(tf.equal(pred, truth), tf.float32),(-1,1))\n",
    "        return loss + C\n",
    "        # return (A + B) + C\n",
    "\n",
    "    return  cross_entropy_evidence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "K = 10\n",
    "def loss_function(annealing_rate=1, momentum=1, decay=1, global_loss=False):\n",
    "    #create a wrapper function that returns a function\n",
    "    temperature = 1\n",
    "    Classes = 10\n",
    "    keras_kl = tf.keras.losses.KLDivergence()\n",
    "    annealing_rate = annealing_rate\n",
    "    momentum_rate = momentum\n",
    "    decay_rate = decay\n",
    "    def cross_entropy_evidence(labels, outputs): \n",
    "        softmax = tf.nn.softmax(outputs)\n",
    "        # activated_outputs =tf.keras.activations.sigmoid(softmax)\n",
    "        evidence = softplus_evidence(outputs)\n",
    "        alpha = evidence + 1\n",
    "        S = tf.reduce_sum(alpha, axis=1, keepdims=True) \n",
    "        E = alpha - 1\n",
    "        m = alpha / S\n",
    "        A = tf.reduce_sum((labels-m)**2, axis=1, keepdims=True) \n",
    "        B = tf.reduce_sum(alpha*(S-alpha)/(S*S*(S+1)), axis=1, keepdims=True) \n",
    "\n",
    "        annealing_coef = tf.minimum(1.0,tf.cast(annealing_rate,tf.float32))\n",
    "#         annealing_coef = 1\n",
    "        alp = E*(1-labels) + 1 \n",
    "        # print(\"alp\", alp)\n",
    "        C =  annealing_coef * KL(alp)\n",
    "        # C = keras_kl(labels,evidence)\n",
    "        loss = tf.keras.losses.categorical_crossentropy(labels, softmax)\n",
    "        pred = tf.argmax(outputs,1)\n",
    "        truth = tf.argmax(labels,1)\n",
    "        match = tf.reshape(tf.cast(tf.equal(pred, truth), tf.float32),(-1,1))\n",
    "        return loss + C\n",
    "        # return (A + B) + C\n",
    "    return  cross_entropy_evidence\n",
    "\n",
    "# outputs =[]\n",
    "# targets = tf.keras.Input(shape=(10,),name='targets')\n",
    "# inputs = tf.keras.Input(shape=(227,227,3))\n",
    "# x = tf.keras.layers.Conv2D(filters=96, kernel_size=(11,11), strides=(4,4), activation='relu', input_shape=(227,227,3))(inputs)\n",
    "# x = tf.keras.layers.BatchNormalization()(x)\n",
    "# x = tf.keras.layers.MaxPool2D(pool_size=(3,3), strides=(2,2),padding=\"same\")(x)\n",
    "\n",
    "\n",
    "# branchLayer = tf.keras.layers.Flatten(name=tf.compat.v1.get_default_graph().unique_name(\"branch_flatten\"))(x)\n",
    "# branchLayer = tf.keras.layers.Dense(124, activation=\"relu\",name=tf.compat.v1.get_default_graph().unique_name(\"branch124\"))(branchLayer)\n",
    "# branchLayer = tf.keras.layers.Dense(64, activation=\"relu\",name=tf.compat.v1.get_default_graph().unique_name(\"branch64\"))(branchLayer)\n",
    "# # branchLayer = tf.keras.layers.Dense(10, name=tf.compat.v1.get_default_graph().unique_name(\"branch_output\"))(branchLayer)\n",
    "# # branch_output_1 = (CrossEntropyEndpoint(10, name=tf.compat.v1.get_default_graph().unique_name(\"branch_endpoint\"))(branchLayer,targets))\n",
    "# branch_output_1 = tf.keras.layers.Dense(10, activation=\"softmax\", name=tf.compat.v1.get_default_graph().unique_name(\"branch_endpoint\"))(branchLayer)\n",
    "\n",
    "# x = tf.keras.layers.Conv2D(filters=256, kernel_size=(5,5), strides=(1,1), activation='relu', padding=\"same\")(x)\n",
    "# x = tf.keras.layers.BatchNormalization()(x)\n",
    "# x = tf.keras.layers.MaxPool2D(pool_size=(3,3), strides=(2,2),padding=\"same\")(x)\n",
    "\n",
    "\n",
    "# branchLayer = tf.keras.layers.Flatten(name=tf.compat.v1.get_default_graph().unique_name(\"branch_flatten\"))(x)\n",
    "# branchLayer = tf.keras.layers.Dense(124, activation=\"relu\",name=tf.compat.v1.get_default_graph().unique_name(\"branch124\"))(branchLayer)\n",
    "# branchLayer = tf.keras.layers.Dense(64, activation=\"relu\",name=tf.compat.v1.get_default_graph().unique_name(\"branch64\"))(branchLayer)\n",
    "# # branchLayer = tf.keras.layers.Dense(10, name=tf.compat.v1.get_default_graph().unique_name(\"branch_output\"))(branchLayer)\n",
    "# # branch_output_2 = (CrossEntropyEndpoint(10, name=tf.compat.v1.get_default_graph().unique_name(\"branch_endpoint\"))(branchLayer,targets))\n",
    "# branch_output_2 = tf.keras.layers.Dense(10, activation=\"softmax\", name=tf.compat.v1.get_default_graph().unique_name(\"branch_endpoint\"))(branchLayer)\n",
    "\n",
    "# x = tf.keras.layers.Conv2D(filters=384, kernel_size=(3,3), strides=(1,1), activation='relu', padding=\"same\")(x)\n",
    "# x = tf.keras.layers.BatchNormalization()(x)\n",
    "# x = tf.keras.layers.Conv2D(filters=384, kernel_size=(1,1), strides=(1,1), activation='relu', padding=\"same\")(x)\n",
    "# x = tf.keras.layers.BatchNormalization()(x)\n",
    "# x = tf.keras.layers.Conv2D(filters=256, kernel_size=(1,1), strides=(1,1), activation='relu', padding=\"same\")(x)\n",
    "# x = tf.keras.layers.BatchNormalization()(x)\n",
    "# x = tf.keras.layers.MaxPool2D(pool_size=(3,3), strides=(2,2),padding=\"same\")(x)\n",
    "# x = tf.keras.layers.Flatten()(x)\n",
    "# x = tf.keras.layers.Dense(4096, activation='relu')(x)\n",
    "\n",
    "\n",
    "\n",
    "# branchLayer = tf.keras.layers.Flatten(name=tf.compat.v1.get_default_graph().unique_name(\"branch_flatten\"))(x)\n",
    "# branchLayer = tf.keras.layers.Dense(124, activation=\"relu\",name=tf.compat.v1.get_default_graph().unique_name(\"branch124\"))(branchLayer)\n",
    "# branchLayer = tf.keras.layers.Dense(64, activation=\"relu\",name=tf.compat.v1.get_default_graph().unique_name(\"branch64\"))(branchLayer)\n",
    "# # branchLayer = tf.keras.layers.Dense(10, name=tf.compat.v1.get_default_graph().unique_name(\"branch_endpoint\"))(branchLayer)\n",
    "# branch_output_3 = tf.keras.layers.Dense(10, activation=\"softmax\", name=tf.compat.v1.get_default_graph().unique_name(\"branch_endpoint\"))(branchLayer)\n",
    "# # branch_output_3 = (CrossEntropyEndpoint(10, name=tf.compat.v1.get_default_graph().unique_name(\"branch_endpoint\"))(branchLayer,targets))\n",
    "\n",
    "# x = tf.keras.layers.Dropout(0.5)(x)\n",
    "# x = tf.keras.layers.Dense(4096, activation='relu')(x)\n",
    "# x = tf.keras.layers.Dropout(0.5)(x)\n",
    "# output = tf.keras.layers.Dense(10, activation=\"softmax\", name=tf.compat.v1.get_default_graph().unique_name(\"endpoint\"))(x)\n",
    "# # output = CrossEntropyEndpoint(10, name=tf.compat.v1.get_default_graph().unique_name(\"endpoint\"))(x, targets)\n",
    "\n",
    "# model = tf.keras.Model(inputs=[inputs,targets], outputs=[output,branch_output_1,branch_output_2,branch_output_3], name=\"alexnet_branched_entropy\")\n",
    "# loss_fn = loss_function()\n",
    "# model.compile( loss=\"categorical_crossentropy\", optimizer=tf.optimizers.SGD(lr=0.001,momentum=0.9), metrics=['accuracy'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import time\n",
    "# root_logdir = os.path.join(os.curdir, \"logs\\\\fit\\\\\")\n",
    "# checkpoint = tf.keras.callbacks.ModelCheckpoint(\"alexNetv6_entropy_branched_scratch_40.hdf5\", monitor='val_loss',verbose=1,save_best_only=True, mode='auto')\n",
    "# def get_run_logdir():\n",
    "#     run_id = time.strftime(\"run_%Y_%m_%d-%H_%M_%S\")\n",
    "#     return os.path.join(root_logdir, run_id)\n",
    "\n",
    "# run_logdir = get_run_logdir()\n",
    "# tensorboard_cb = tf.keras.callbacks.TensorBoard(run_logdir)\n",
    "# model.fit(train_ds,\n",
    "#         epochs=40,\n",
    "#         validation_data=validation_ds,\n",
    "#         validation_freq=1,\n",
    "#         # batch_size=1,\n",
    "#         verbose=1,\n",
    "#         callbacks=[tensorboard_cb,checkpoint])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "augment Dataset\n",
      "targetsis : True\n",
      "adding targets to inputs\n",
      "trainSize 45000\n",
      "testSize 10000\n"
     ]
    }
   ],
   "source": [
    "dataset = branching.dataset.prepare.dataset(tf.keras.datasets.cifar10.load_data(),32,5000,22500,(224,224), include_targets=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# base_model = tf.keras.applications.MobileNetV2(\n",
    "#     input_shape=(224,224,3),\n",
    "#     alpha=1.0,\n",
    "#     include_top=False,\n",
    "#     weights=\"imagenet\",\n",
    "#     input_tensor=None,\n",
    "#     pooling=\"avg\",\n",
    "# )\n",
    "\n",
    "\n",
    "# x = base_model.output\n",
    "# # x = tf.keras.layers.Dense(1024, activation=\"relu\")(x)\n",
    "# # x = tf.keras.layers.Dense(512, activation=\"relu\")(x)\n",
    "# x = tf.keras.layers.Dense(10, activation=\"softmax\", name=\"classification\")(x)\n",
    "\n",
    "# model = tf.keras.models.Model(inputs=base_model.input, outputs=x)\n",
    "\n",
    "\n",
    "# model.compile(optimizer='SGD', \n",
    "#                 loss='categorical_crossentropy',\n",
    "#                 metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "augment Dataset\n",
      "targetsis : False\n",
      "trainSize 45000\n",
      "testSize 10000\n"
     ]
    }
   ],
   "source": [
    "train_ds, test_ds, validation_ds = branching.dataset.prepare.dataset(tf.keras.datasets.cifar10.load_data(),32,5000,22500,(224,224), include_targets=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EPOCHS = 3\n",
    "# model.fit(train_ds, epochs=EPOCHS, validation_data = validation_ds, batch_size=32)\n",
    "# loss, accuracy = model.evaluate(test_ds, batch_size=32)\n",
    "# # model.save(\"mobileNetv2_finetuned.hdf5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "312/312 [==============================] - 14s 33ms/step - loss: 0.2226 - accuracy: 0.9270\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.2225652039051056, 0.9269831776618958]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model =  tf.keras.models.load_model(\"mobileNetv2_finetuned.hdf5\")\n",
    "model.evaluate(test_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.save(\"models/mobileNetv2_finetuned.hdf5\")\n",
    "# branchPoints = [\"block_2_add\",\"block_4_add\",\"block_8_add\"]\n",
    "branchPoints = [\"block_2_add\"]\n",
    "loss_fn = loss_function()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "targets already present?  False\n",
      "added targets\n",
      "Matching Branchpoint by id number\n",
      "Matching Branchpoint by name\n",
      "add Branch to branch point  block_2_add\n",
      "TensorShape([None, 56, 56, 24])\n",
      "inputShape TensorShape([None, 24])\n",
      "<branchingdnn.core.BranchingDnn.branched_model object at 0x000002001419DCC0>\n"
     ]
    }
   ],
   "source": [
    "# model  = tf.keras.models.load_model('alexNetv6_evidence_branched_contin_30.hdf5', custom_objects={\"CrossEntropyEndpoint\":CrossEntropyEndpoint,\"cross_entropy_evidence\":loss_function()})\n",
    "tf.compat.v1.reset_default_graph() #reset the graph to reset the naming scheme for the new branched model\n",
    "\n",
    "loss_fn = loss_function()\n",
    "brevis = (branching.core.branched_model(modelName=\"models/mobileNetv2_finetuned.hdf5\",saveName=\"mobileNetv2_evidence_conv2d_2\",transfer=True,custom_objects=\"\")\n",
    "            .add_branches(branching.branches.branch.branch_conv2d_mobilenet_evidence,branchPoints, target_input=True)\n",
    "            .set_dataset(dataset)\n",
    "            \n",
    "            )\n",
    "# brevis.evaluate(test_ds)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_branched\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 224, 224, 3) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "Conv1 (Conv2D)                  (None, 112, 112, 32) 864         input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "bn_Conv1 (BatchNormalization)   (None, 112, 112, 32) 128         Conv1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "Conv1_relu (ReLU)               (None, 112, 112, 32) 0           bn_Conv1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "expanded_conv_depthwise (Depthw (None, 112, 112, 32) 288         Conv1_relu[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "expanded_conv_depthwise_BN (Bat (None, 112, 112, 32) 128         expanded_conv_depthwise[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "expanded_conv_depthwise_relu (R (None, 112, 112, 32) 0           expanded_conv_depthwise_BN[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "expanded_conv_project (Conv2D)  (None, 112, 112, 16) 512         expanded_conv_depthwise_relu[0][0\n",
      "__________________________________________________________________________________________________\n",
      "expanded_conv_project_BN (Batch (None, 112, 112, 16) 64          expanded_conv_project[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "block_1_expand (Conv2D)         (None, 112, 112, 96) 1536        expanded_conv_project_BN[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "block_1_expand_BN (BatchNormali (None, 112, 112, 96) 384         block_1_expand[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "block_1_expand_relu (ReLU)      (None, 112, 112, 96) 0           block_1_expand_BN[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "block_1_pad (ZeroPadding2D)     (None, 113, 113, 96) 0           block_1_expand_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block_1_depthwise (DepthwiseCon (None, 56, 56, 96)   864         block_1_pad[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "block_1_depthwise_BN (BatchNorm (None, 56, 56, 96)   384         block_1_depthwise[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "block_1_depthwise_relu (ReLU)   (None, 56, 56, 96)   0           block_1_depthwise_BN[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "block_1_project (Conv2D)        (None, 56, 56, 24)   2304        block_1_depthwise_relu[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "block_1_project_BN (BatchNormal (None, 56, 56, 24)   96          block_1_project[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block_2_expand (Conv2D)         (None, 56, 56, 144)  3456        block_1_project_BN[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block_2_expand_BN (BatchNormali (None, 56, 56, 144)  576         block_2_expand[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "block_2_expand_relu (ReLU)      (None, 56, 56, 144)  0           block_2_expand_BN[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "block_2_depthwise (DepthwiseCon (None, 56, 56, 144)  1296        block_2_expand_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block_2_depthwise_BN (BatchNorm (None, 56, 56, 144)  576         block_2_depthwise[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "block_2_depthwise_relu (ReLU)   (None, 56, 56, 144)  0           block_2_depthwise_BN[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "block_2_project (Conv2D)        (None, 56, 56, 24)   3456        block_2_depthwise_relu[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "block_2_project_BN (BatchNormal (None, 56, 56, 24)   96          block_2_project[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block_2_add (Add)               (None, 56, 56, 24)   0           block_1_project_BN[0][0]         \n",
      "                                                                 block_2_project_BN[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block_3_expand (Conv2D)         (None, 56, 56, 144)  3456        block_2_add[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "block_3_expand_BN (BatchNormali (None, 56, 56, 144)  576         block_3_expand[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "block_3_expand_relu (ReLU)      (None, 56, 56, 144)  0           block_3_expand_BN[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "block_3_pad (ZeroPadding2D)     (None, 57, 57, 144)  0           block_3_expand_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block_3_depthwise (DepthwiseCon (None, 28, 28, 144)  1296        block_3_pad[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "block_3_depthwise_BN (BatchNorm (None, 28, 28, 144)  576         block_3_depthwise[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "block_3_depthwise_relu (ReLU)   (None, 28, 28, 144)  0           block_3_depthwise_BN[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "block_3_project (Conv2D)        (None, 28, 28, 32)   4608        block_3_depthwise_relu[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "block_3_project_BN (BatchNormal (None, 28, 28, 32)   128         block_3_project[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block_4_expand (Conv2D)         (None, 28, 28, 192)  6144        block_3_project_BN[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block_4_expand_BN (BatchNormali (None, 28, 28, 192)  768         block_4_expand[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "block_4_expand_relu (ReLU)      (None, 28, 28, 192)  0           block_4_expand_BN[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "block_4_depthwise (DepthwiseCon (None, 28, 28, 192)  1728        block_4_expand_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block_4_depthwise_BN (BatchNorm (None, 28, 28, 192)  768         block_4_depthwise[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "block_4_depthwise_relu (ReLU)   (None, 28, 28, 192)  0           block_4_depthwise_BN[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "block_4_project (Conv2D)        (None, 28, 28, 32)   6144        block_4_depthwise_relu[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "block_4_project_BN (BatchNormal (None, 28, 28, 32)   128         block_4_project[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block_4_add (Add)               (None, 28, 28, 32)   0           block_3_project_BN[0][0]         \n",
      "                                                                 block_4_project_BN[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block_5_expand (Conv2D)         (None, 28, 28, 192)  6144        block_4_add[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "block_5_expand_BN (BatchNormali (None, 28, 28, 192)  768         block_5_expand[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "block_5_expand_relu (ReLU)      (None, 28, 28, 192)  0           block_5_expand_BN[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "block_5_depthwise (DepthwiseCon (None, 28, 28, 192)  1728        block_5_expand_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block_5_depthwise_BN (BatchNorm (None, 28, 28, 192)  768         block_5_depthwise[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "block_5_depthwise_relu (ReLU)   (None, 28, 28, 192)  0           block_5_depthwise_BN[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "block_5_project (Conv2D)        (None, 28, 28, 32)   6144        block_5_depthwise_relu[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "block_5_project_BN (BatchNormal (None, 28, 28, 32)   128         block_5_project[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block_5_add (Add)               (None, 28, 28, 32)   0           block_4_add[0][0]                \n",
      "                                                                 block_5_project_BN[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block_6_expand (Conv2D)         (None, 28, 28, 192)  6144        block_5_add[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "block_6_expand_BN (BatchNormali (None, 28, 28, 192)  768         block_6_expand[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "block_6_expand_relu (ReLU)      (None, 28, 28, 192)  0           block_6_expand_BN[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "block_6_pad (ZeroPadding2D)     (None, 29, 29, 192)  0           block_6_expand_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block_6_depthwise (DepthwiseCon (None, 14, 14, 192)  1728        block_6_pad[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "block_6_depthwise_BN (BatchNorm (None, 14, 14, 192)  768         block_6_depthwise[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "block_6_depthwise_relu (ReLU)   (None, 14, 14, 192)  0           block_6_depthwise_BN[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "block_6_project (Conv2D)        (None, 14, 14, 64)   12288       block_6_depthwise_relu[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "block_6_project_BN (BatchNormal (None, 14, 14, 64)   256         block_6_project[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block_7_expand (Conv2D)         (None, 14, 14, 384)  24576       block_6_project_BN[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block_7_expand_BN (BatchNormali (None, 14, 14, 384)  1536        block_7_expand[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "block_7_expand_relu (ReLU)      (None, 14, 14, 384)  0           block_7_expand_BN[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "block_7_depthwise (DepthwiseCon (None, 14, 14, 384)  3456        block_7_expand_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block_7_depthwise_BN (BatchNorm (None, 14, 14, 384)  1536        block_7_depthwise[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "block_7_depthwise_relu (ReLU)   (None, 14, 14, 384)  0           block_7_depthwise_BN[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "block_7_project (Conv2D)        (None, 14, 14, 64)   24576       block_7_depthwise_relu[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "block_7_project_BN (BatchNormal (None, 14, 14, 64)   256         block_7_project[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block_7_add (Add)               (None, 14, 14, 64)   0           block_6_project_BN[0][0]         \n",
      "                                                                 block_7_project_BN[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block_8_expand (Conv2D)         (None, 14, 14, 384)  24576       block_7_add[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "block_8_expand_BN (BatchNormali (None, 14, 14, 384)  1536        block_8_expand[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "block_8_expand_relu (ReLU)      (None, 14, 14, 384)  0           block_8_expand_BN[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "block_8_depthwise (DepthwiseCon (None, 14, 14, 384)  3456        block_8_expand_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block_8_depthwise_BN (BatchNorm (None, 14, 14, 384)  1536        block_8_depthwise[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "block_8_depthwise_relu (ReLU)   (None, 14, 14, 384)  0           block_8_depthwise_BN[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "block_8_project (Conv2D)        (None, 14, 14, 64)   24576       block_8_depthwise_relu[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "block_8_project_BN (BatchNormal (None, 14, 14, 64)   256         block_8_project[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block_8_add (Add)               (None, 14, 14, 64)   0           block_7_add[0][0]                \n",
      "                                                                 block_8_project_BN[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block_9_expand (Conv2D)         (None, 14, 14, 384)  24576       block_8_add[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "block_9_expand_BN (BatchNormali (None, 14, 14, 384)  1536        block_9_expand[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "block_9_expand_relu (ReLU)      (None, 14, 14, 384)  0           block_9_expand_BN[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "block_9_depthwise (DepthwiseCon (None, 14, 14, 384)  3456        block_9_expand_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block_9_depthwise_BN (BatchNorm (None, 14, 14, 384)  1536        block_9_depthwise[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "block_9_depthwise_relu (ReLU)   (None, 14, 14, 384)  0           block_9_depthwise_BN[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "block_9_project (Conv2D)        (None, 14, 14, 64)   24576       block_9_depthwise_relu[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "block_9_project_BN (BatchNormal (None, 14, 14, 64)   256         block_9_project[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block_9_add (Add)               (None, 14, 14, 64)   0           block_8_add[0][0]                \n",
      "                                                                 block_9_project_BN[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block_10_expand (Conv2D)        (None, 14, 14, 384)  24576       block_9_add[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "block_10_expand_BN (BatchNormal (None, 14, 14, 384)  1536        block_10_expand[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block_10_expand_relu (ReLU)     (None, 14, 14, 384)  0           block_10_expand_BN[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block_10_depthwise (DepthwiseCo (None, 14, 14, 384)  3456        block_10_expand_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "block_10_depthwise_BN (BatchNor (None, 14, 14, 384)  1536        block_10_depthwise[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block_10_depthwise_relu (ReLU)  (None, 14, 14, 384)  0           block_10_depthwise_BN[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "block_10_project (Conv2D)       (None, 14, 14, 96)   36864       block_10_depthwise_relu[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "block_10_project_BN (BatchNorma (None, 14, 14, 96)   384         block_10_project[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "block_11_expand (Conv2D)        (None, 14, 14, 576)  55296       block_10_project_BN[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block_11_expand_BN (BatchNormal (None, 14, 14, 576)  2304        block_11_expand[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block_11_expand_relu (ReLU)     (None, 14, 14, 576)  0           block_11_expand_BN[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block_11_depthwise (DepthwiseCo (None, 14, 14, 576)  5184        block_11_expand_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "block_11_depthwise_BN (BatchNor (None, 14, 14, 576)  2304        block_11_depthwise[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block_11_depthwise_relu (ReLU)  (None, 14, 14, 576)  0           block_11_depthwise_BN[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "block_11_project (Conv2D)       (None, 14, 14, 96)   55296       block_11_depthwise_relu[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "block_11_project_BN (BatchNorma (None, 14, 14, 96)   384         block_11_project[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "block_11_add (Add)              (None, 14, 14, 96)   0           block_10_project_BN[0][0]        \n",
      "                                                                 block_11_project_BN[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block_12_expand (Conv2D)        (None, 14, 14, 576)  55296       block_11_add[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "block_12_expand_BN (BatchNormal (None, 14, 14, 576)  2304        block_12_expand[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block_12_expand_relu (ReLU)     (None, 14, 14, 576)  0           block_12_expand_BN[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block_12_depthwise (DepthwiseCo (None, 14, 14, 576)  5184        block_12_expand_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "block_12_depthwise_BN (BatchNor (None, 14, 14, 576)  2304        block_12_depthwise[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block_12_depthwise_relu (ReLU)  (None, 14, 14, 576)  0           block_12_depthwise_BN[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "block_12_project (Conv2D)       (None, 14, 14, 96)   55296       block_12_depthwise_relu[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "block_12_project_BN (BatchNorma (None, 14, 14, 96)   384         block_12_project[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "block_12_add (Add)              (None, 14, 14, 96)   0           block_11_add[0][0]               \n",
      "                                                                 block_12_project_BN[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block_13_expand (Conv2D)        (None, 14, 14, 576)  55296       block_12_add[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "block_13_expand_BN (BatchNormal (None, 14, 14, 576)  2304        block_13_expand[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block_13_expand_relu (ReLU)     (None, 14, 14, 576)  0           block_13_expand_BN[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block_13_pad (ZeroPadding2D)    (None, 15, 15, 576)  0           block_13_expand_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "block_13_depthwise (DepthwiseCo (None, 7, 7, 576)    5184        block_13_pad[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "block_13_depthwise_BN (BatchNor (None, 7, 7, 576)    2304        block_13_depthwise[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block_13_depthwise_relu (ReLU)  (None, 7, 7, 576)    0           block_13_depthwise_BN[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "block_13_project (Conv2D)       (None, 7, 7, 160)    92160       block_13_depthwise_relu[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "block_13_project_BN (BatchNorma (None, 7, 7, 160)    640         block_13_project[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "block_14_expand (Conv2D)        (None, 7, 7, 960)    153600      block_13_project_BN[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block_14_expand_BN (BatchNormal (None, 7, 7, 960)    3840        block_14_expand[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block_14_expand_relu (ReLU)     (None, 7, 7, 960)    0           block_14_expand_BN[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block_14_depthwise (DepthwiseCo (None, 7, 7, 960)    8640        block_14_expand_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "block_14_depthwise_BN (BatchNor (None, 7, 7, 960)    3840        block_14_depthwise[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block_14_depthwise_relu (ReLU)  (None, 7, 7, 960)    0           block_14_depthwise_BN[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "block_14_project (Conv2D)       (None, 7, 7, 160)    153600      block_14_depthwise_relu[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "block_14_project_BN (BatchNorma (None, 7, 7, 160)    640         block_14_project[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "block_14_add (Add)              (None, 7, 7, 160)    0           block_13_project_BN[0][0]        \n",
      "                                                                 block_14_project_BN[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block_15_expand (Conv2D)        (None, 7, 7, 960)    153600      block_14_add[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "block_15_expand_BN (BatchNormal (None, 7, 7, 960)    3840        block_15_expand[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block_15_expand_relu (ReLU)     (None, 7, 7, 960)    0           block_15_expand_BN[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block_15_depthwise (DepthwiseCo (None, 7, 7, 960)    8640        block_15_expand_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "block_15_depthwise_BN (BatchNor (None, 7, 7, 960)    3840        block_15_depthwise[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block_15_depthwise_relu (ReLU)  (None, 7, 7, 960)    0           block_15_depthwise_BN[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "block_15_project (Conv2D)       (None, 7, 7, 160)    153600      block_15_depthwise_relu[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "block_15_project_BN (BatchNorma (None, 7, 7, 160)    640         block_15_project[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "block_15_add (Add)              (None, 7, 7, 160)    0           block_14_add[0][0]               \n",
      "                                                                 block_15_project_BN[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block_16_expand (Conv2D)        (None, 7, 7, 960)    153600      block_15_add[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "block_16_expand_BN (BatchNormal (None, 7, 7, 960)    3840        block_16_expand[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block_16_expand_relu (ReLU)     (None, 7, 7, 960)    0           block_16_expand_BN[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block_16_depthwise (DepthwiseCo (None, 7, 7, 960)    8640        block_16_expand_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "block_16_depthwise_BN (BatchNor (None, 7, 7, 960)    3840        block_16_depthwise[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block_16_depthwise_relu (ReLU)  (None, 7, 7, 960)    0           block_16_depthwise_BN[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "block_16_project (Conv2D)       (None, 7, 7, 320)    307200      block_16_depthwise_relu[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "block_16_project_BN (BatchNorma (None, 7, 7, 320)    1280        block_16_project[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "Conv_1 (Conv2D)                 (None, 7, 7, 1280)   409600      block_16_project_BN[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "Conv_1_bn (BatchNormalization)  (None, 7, 7, 1280)   5120        Conv_1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "out_relu (ReLU)                 (None, 7, 7, 1280)   0           Conv_1_bn[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling2d (Globa (None, 1280)         0           out_relu[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "branch_avgPool (GlobalAveragePo (None, 24)           0           block_2_add[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "targets (InputLayer)            [(None, 10)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "classification (Dense)          (None, 10)           12810       global_average_pooling2d[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "branch_softmax (CrossEntropyEnd (None, 10)           240         branch_avgPool[0][0]             \n",
      "                                                                 targets[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 2,271,034\n",
      "Trainable params: 2,236,922\n",
      "Non-trainable params: 34,112\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# brevis.save(\"mobileNetv2_evidence_conv2d_2.hdf5\")\n",
    "brevis.model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = brevis.trainTransfer(12, loss=loss_fn, optimizer=tf.optimizers.SGD(), transfer=False, customOptions=\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "brevis.saveName=(\"mobileNetv2_entropy_flat_30\")\n",
    "model = brevis.trainTransfer(18, loss=\"categorical_crossentropy\", optimizer=tf.optimizers.SGD(), transfer=False, customOptions=\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "targets already present?  False\n",
      "added targets\n",
      "Matching Branchpoint by id number\n",
      "Matching Branchpoint by name\n",
      "add Branch to branch point  block_2_add\n",
      "inputShape TensorShape([None, 64])\n",
      "add Branch to branch point  block_4_add\n",
      "inputShape TensorShape([None, 64])\n",
      "add Branch to branch point  block_8_add\n",
      "inputShape TensorShape([None, 64])\n",
      "<branchingdnn.core.BranchingDnn.branched_model object at 0x00000213298085C0>\n"
     ]
    }
   ],
   "source": [
    "tf.compat.v1.reset_default_graph() #reset the graph to reset the naming scheme for the new branched model\n",
    "brevis = (branching.core.branched_model(modelName=\"models/mobileNetv2_finetuned.hdf5\",saveName=\"mobileNetv2_evidence_flat\",transfer=True,custom_objects=\"\")\n",
    "            .add_branches(branching.branches.branch.newBranch_flatten_incept_evid,branchPoints, target_input=True)\n",
    "            .set_dataset(dataset)\n",
    "            \n",
    "            )\n",
    "# brevis.evaluate(test_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting Main Layers  and branch layers training to true\n",
      "\n",
      "customOption: Other\n",
      "https://app.neptune.ai/cailen01/branchingDNN/e/BRAN-407\n",
      "Remember to stop your run once youve finished logging your metadata (https://docs.neptune.ai/api-reference/run#stop). It will be stopped automatically only when the notebook kernel/interactive console is terminated.\n",
      "Epoch 1/12\n",
      "1406/1406 [==============================] - ETA: 0s - loss: 8.0359 - classification_loss: 2.2157 - branch_softmax_loss: 2.1841 - branch_softmax_1_loss: 1.9603 - branch_softmax_2_loss: 1.6758 - classification_accuracy: 0.9654 - branch_softmax_accuracy: 0.3132 - branch_softmax_1_accuracy: 0.3544 - branch_softmax_2_accuracy: 0.4570 - branch_softmax_evidence: 0.1327 - branch_softmax_mean_ev_succ: 1.2293 - branch_softmax_mean_ev_fail: 1.3758 - branch_softmax_1_evidence: 0.0788 - branch_softmax_1_mean_ev_succ: 0.7584 - branch_softmax_1_mean_ev_fail: 0.8077 - branch_softmax_2_evidence: 0.0656 - branch_softmax_2_mean_ev_succ: 0.6626 - branch_softmax_2_mean_ev_fail: 0.6366"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: string series 'monitoring/stdout' value was longer than 1000 characters and was truncated. This warning is printed only once per series.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1406/1406 [==============================] - 433s 295ms/step - loss: 8.0351 - classification_loss: 2.2157 - branch_softmax_loss: 2.1838 - branch_softmax_1_loss: 1.9600 - branch_softmax_2_loss: 1.6756 - classification_accuracy: 0.9654 - branch_softmax_accuracy: 0.3133 - branch_softmax_1_accuracy: 0.3545 - branch_softmax_2_accuracy: 0.4571 - branch_softmax_evidence: 0.1327 - branch_softmax_mean_ev_succ: 1.2293 - branch_softmax_mean_ev_fail: 1.3758 - branch_softmax_1_evidence: 0.0788 - branch_softmax_1_mean_ev_succ: 0.7584 - branch_softmax_1_mean_ev_fail: 0.8077 - branch_softmax_2_evidence: 0.0656 - branch_softmax_2_mean_ev_succ: 0.6626 - branch_softmax_2_mean_ev_fail: 0.6366 - val_loss: 7.8700 - val_classification_loss: 2.2611 - val_branch_softmax_loss: 2.0598 - val_branch_softmax_1_loss: 1.8299 - val_branch_softmax_2_loss: 1.7191 - val_classification_accuracy: 0.9209 - val_branch_softmax_accuracy: 0.2995 - val_branch_softmax_1_accuracy: 0.3744 - val_branch_softmax_2_accuracy: 0.4547 - val_branch_softmax_evidence: 0.0230 - val_branch_softmax_mean_ev_succ: 0.2098 - val_branch_softmax_mean_ev_fail: 0.2385 - val_branch_softmax_1_evidence: 0.0132 - val_branch_softmax_1_mean_ev_succ: 0.1225 - val_branch_softmax_1_mean_ev_fail: 0.1387 - val_branch_softmax_2_evidence: 0.0078 - val_branch_softmax_2_mean_ev_succ: 0.0866 - val_branch_softmax_2_mean_ev_fail: 0.0716\n",
      "\n",
      "Epoch 00001: saving model to models\\mobileNetv2_evidence_flat.hdf5\n",
      "Epoch 2/12\n",
      "1406/1406 [==============================] - 439s 302ms/step - loss: 5.7672 - classification_loss: 2.2102 - branch_softmax_loss: 1.4194 - branch_softmax_1_loss: 1.2731 - branch_softmax_2_loss: 0.8645 - classification_accuracy: 0.9647 - branch_softmax_accuracy: 0.4941 - branch_softmax_1_accuracy: 0.5491 - branch_softmax_2_accuracy: 0.6939 - branch_softmax_evidence: 0.0176 - branch_softmax_mean_ev_succ: 0.1708 - branch_softmax_mean_ev_fail: 0.1817 - branch_softmax_1_evidence: 0.0160 - branch_softmax_1_mean_ev_succ: 0.1640 - branch_softmax_1_mean_ev_fail: 0.1557 - branch_softmax_2_evidence: 0.0210 - branch_softmax_2_mean_ev_succ: 0.2481 - branch_softmax_2_mean_ev_fail: 0.1166 - val_loss: 6.5356 - val_classification_loss: 2.2649 - val_branch_softmax_loss: 1.4625 - val_branch_softmax_1_loss: 1.5898 - val_branch_softmax_2_loss: 1.2184 - val_classification_accuracy: 0.9149 - val_branch_softmax_accuracy: 0.4700 - val_branch_softmax_1_accuracy: 0.4335 - val_branch_softmax_2_accuracy: 0.5909 - val_branch_softmax_evidence: 0.0171 - val_branch_softmax_mean_ev_succ: 0.1579 - val_branch_softmax_mean_ev_fail: 0.1827 - val_branch_softmax_1_evidence: 0.0091 - val_branch_softmax_1_mean_ev_succ: 0.1032 - val_branch_softmax_1_mean_ev_fail: 0.0830 - val_branch_softmax_2_evidence: 0.0086 - val_branch_softmax_2_mean_ev_succ: 0.1218 - val_branch_softmax_2_mean_ev_fail: 0.0365\n",
      "\n",
      "Epoch 00002: saving model to models\\mobileNetv2_evidence_flat.hdf5\n",
      "Epoch 3/12\n",
      "1406/1406 [==============================] - 445s 307ms/step - loss: 5.3001 - classification_loss: 2.2025 - branch_softmax_loss: 1.2868 - branch_softmax_1_loss: 1.1118 - branch_softmax_2_loss: 0.6990 - classification_accuracy: 0.9713 - branch_softmax_accuracy: 0.5417 - branch_softmax_1_accuracy: 0.6046 - branch_softmax_2_accuracy: 0.7540 - branch_softmax_evidence: 0.0125 - branch_softmax_mean_ev_succ: 0.1300 - branch_softmax_mean_ev_fail: 0.1196 - branch_softmax_1_evidence: 0.0135 - branch_softmax_1_mean_ev_succ: 0.1497 - branch_softmax_1_mean_ev_fail: 0.1122 - branch_softmax_2_evidence: 0.0221 - branch_softmax_2_mean_ev_succ: 0.2636 - branch_softmax_2_mean_ev_fail: 0.0850 - val_loss: 7.1907 - val_classification_loss: 2.2528 - val_branch_softmax_loss: 1.9185 - val_branch_softmax_1_loss: 2.0590 - val_branch_softmax_2_loss: 0.9604 - val_classification_accuracy: 0.9247 - val_branch_softmax_accuracy: 0.3716 - val_branch_softmax_1_accuracy: 0.3291 - val_branch_softmax_2_accuracy: 0.6488 - val_branch_softmax_evidence: 0.0051 - val_branch_softmax_mean_ev_succ: 0.0585 - val_branch_softmax_mean_ev_fail: 0.0471 - val_branch_softmax_1_evidence: 0.0082 - val_branch_softmax_1_mean_ev_succ: 0.0871 - val_branch_softmax_1_mean_ev_fail: 0.0798 - val_branch_softmax_2_evidence: 0.0149 - val_branch_softmax_2_mean_ev_succ: 0.1811 - val_branch_softmax_2_mean_ev_fail: 0.0891\n",
      "\n",
      "Epoch 00003: saving model to models\\mobileNetv2_evidence_flat.hdf5\n",
      "Epoch 4/12\n",
      "1406/1406 [==============================] - 443s 305ms/step - loss: 4.9453 - classification_loss: 2.1942 - branch_softmax_loss: 1.1788 - branch_softmax_1_loss: 0.9939 - branch_softmax_2_loss: 0.5784 - classification_accuracy: 0.9768 - branch_softmax_accuracy: 0.5822 - branch_softmax_1_accuracy: 0.6458 - branch_softmax_2_accuracy: 0.7987 - branch_softmax_evidence: 0.0103 - branch_softmax_mean_ev_succ: 0.1131 - branch_softmax_mean_ev_fail: 0.0902 - branch_softmax_1_evidence: 0.0124 - branch_softmax_1_mean_ev_succ: 0.1454 - branch_softmax_1_mean_ev_fail: 0.0859 - branch_softmax_2_evidence: 0.0252 - branch_softmax_2_mean_ev_succ: 0.2984 - branch_softmax_2_mean_ev_fail: 0.0727 - val_loss: 6.5384 - val_classification_loss: 2.2369 - val_branch_softmax_loss: 1.9098 - val_branch_softmax_1_loss: 1.6598 - val_branch_softmax_2_loss: 0.7319 - val_classification_accuracy: 0.9371 - val_branch_softmax_accuracy: 0.3526 - val_branch_softmax_1_accuracy: 0.4409 - val_branch_softmax_2_accuracy: 0.7488 - val_branch_softmax_evidence: 0.0165 - val_branch_softmax_mean_ev_succ: 0.1958 - val_branch_softmax_mean_ev_fail: 0.1492 - val_branch_softmax_1_evidence: 0.0218 - val_branch_softmax_1_mean_ev_succ: 0.2543 - val_branch_softmax_1_mean_ev_fail: 0.1900 - val_branch_softmax_2_evidence: 0.0166 - val_branch_softmax_2_mean_ev_succ: 0.1966 - val_branch_softmax_2_mean_ev_fail: 0.0724\n",
      "\n",
      "Epoch 00004: saving model to models\\mobileNetv2_evidence_flat.hdf5\n",
      "Epoch 5/12\n",
      "1406/1406 [==============================] - 437s 301ms/step - loss: 4.7691 - classification_loss: 2.1890 - branch_softmax_loss: 1.1288 - branch_softmax_1_loss: 0.9318 - branch_softmax_2_loss: 0.5195 - classification_accuracy: 0.9811 - branch_softmax_accuracy: 0.5996 - branch_softmax_1_accuracy: 0.6674 - branch_softmax_2_accuracy: 0.8176 - branch_softmax_evidence: 0.0096 - branch_softmax_mean_ev_succ: 0.1088 - branch_softmax_mean_ev_fail: 0.0757 - branch_softmax_1_evidence: 0.0121 - branch_softmax_1_mean_ev_succ: 0.1457 - branch_softmax_1_mean_ev_fail: 0.0705 - branch_softmax_2_evidence: 0.0269 - branch_softmax_2_mean_ev_succ: 0.3143 - branch_softmax_2_mean_ev_fail: 0.0620 - val_loss: 7.1135 - val_classification_loss: 2.2463 - val_branch_softmax_loss: 2.0656 - val_branch_softmax_1_loss: 2.0083 - val_branch_softmax_2_loss: 0.7932 - val_classification_accuracy: 0.9291 - val_branch_softmax_accuracy: 0.3820 - val_branch_softmax_1_accuracy: 0.4187 - val_branch_softmax_2_accuracy: 0.7310 - val_branch_softmax_evidence: 0.0023 - val_branch_softmax_mean_ev_succ: 0.0276 - val_branch_softmax_mean_ev_fail: 0.0193 - val_branch_softmax_1_evidence: 0.0107 - val_branch_softmax_1_mean_ev_succ: 0.1566 - val_branch_softmax_1_mean_ev_fail: 0.0712 - val_branch_softmax_2_evidence: 0.0209 - val_branch_softmax_2_mean_ev_succ: 0.2655 - val_branch_softmax_2_mean_ev_fail: 0.0556\n",
      "\n",
      "Epoch 00005: saving model to models\\mobileNetv2_evidence_flat.hdf5\n",
      "Epoch 6/12\n",
      "1406/1406 [==============================] - 427s 294ms/step - loss: 4.6170 - classification_loss: 2.1850 - branch_softmax_loss: 1.0888 - branch_softmax_1_loss: 0.8782 - branch_softmax_2_loss: 0.4651 - classification_accuracy: 0.9833 - branch_softmax_accuracy: 0.6131 - branch_softmax_1_accuracy: 0.6862 - branch_softmax_2_accuracy: 0.8376 - branch_softmax_evidence: 0.0089 - branch_softmax_mean_ev_succ: 0.1026 - branch_softmax_mean_ev_fail: 0.0658 - branch_softmax_1_evidence: 0.0121 - branch_softmax_1_mean_ev_succ: 0.1471 - branch_softmax_1_mean_ev_fail: 0.0638 - branch_softmax_2_evidence: 0.0298 - branch_softmax_2_mean_ev_succ: 0.3450 - branch_softmax_2_mean_ev_fail: 0.0578 - val_loss: 7.7427 - val_classification_loss: 2.2830 - val_branch_softmax_loss: 2.3087 - val_branch_softmax_1_loss: 1.8724 - val_branch_softmax_2_loss: 1.2786 - val_classification_accuracy: 0.8968 - val_branch_softmax_accuracy: 0.3379 - val_branch_softmax_1_accuracy: 0.4107 - val_branch_softmax_2_accuracy: 0.5960 - val_branch_softmax_evidence: 0.0015 - val_branch_softmax_mean_ev_succ: 0.0221 - val_branch_softmax_mean_ev_fail: 0.0108 - val_branch_softmax_1_evidence: 0.0078 - val_branch_softmax_1_mean_ev_succ: 0.1430 - val_branch_softmax_1_mean_ev_fail: 0.0317 - val_branch_softmax_2_evidence: 0.0230 - val_branch_softmax_2_mean_ev_succ: 0.3492 - val_branch_softmax_2_mean_ev_fail: 0.0529\n",
      "\n",
      "Epoch 00006: saving model to models\\mobileNetv2_evidence_flat.hdf5\n",
      "Epoch 7/12\n",
      "1406/1406 [==============================] - 419s 288ms/step - loss: 4.5283 - classification_loss: 2.1835 - branch_softmax_loss: 1.0683 - branch_softmax_1_loss: 0.8458 - branch_softmax_2_loss: 0.4308 - classification_accuracy: 0.9852 - branch_softmax_accuracy: 0.6201 - branch_softmax_1_accuracy: 0.6986 - branch_softmax_2_accuracy: 0.8496 - branch_softmax_evidence: 0.0087 - branch_softmax_mean_ev_succ: 0.1026 - branch_softmax_mean_ev_fail: 0.0603 - branch_softmax_1_evidence: 0.0120 - branch_softmax_1_mean_ev_succ: 0.1477 - branch_softmax_1_mean_ev_fail: 0.0561 - branch_softmax_2_evidence: 0.0330 - branch_softmax_2_mean_ev_succ: 0.3782 - branch_softmax_2_mean_ev_fail: 0.0550 - val_loss: 5.9963 - val_classification_loss: 2.2483 - val_branch_softmax_loss: 1.6955 - val_branch_softmax_1_loss: 1.3625 - val_branch_softmax_2_loss: 0.6900 - val_classification_accuracy: 0.9263 - val_branch_softmax_accuracy: 0.4137 - val_branch_softmax_1_accuracy: 0.5461 - val_branch_softmax_2_accuracy: 0.7654 - val_branch_softmax_evidence: 0.0015 - val_branch_softmax_mean_ev_succ: 0.0192 - val_branch_softmax_mean_ev_fail: 0.0117 - val_branch_softmax_1_evidence: 0.0085 - val_branch_softmax_1_mean_ev_succ: 0.1144 - val_branch_softmax_1_mean_ev_fail: 0.0505 - val_branch_softmax_2_evidence: 0.0238 - val_branch_softmax_2_mean_ev_succ: 0.2949 - val_branch_softmax_2_mean_ev_fail: 0.0504\n",
      "\n",
      "Epoch 00007: saving model to models\\mobileNetv2_evidence_flat.hdf5\n",
      "Epoch 8/12\n",
      "1406/1406 [==============================] - 424s 291ms/step - loss: 4.3967 - classification_loss: 2.1795 - branch_softmax_loss: 1.0348 - branch_softmax_1_loss: 0.8040 - branch_softmax_2_loss: 0.3783 - classification_accuracy: 0.9882 - branch_softmax_accuracy: 0.6269 - branch_softmax_1_accuracy: 0.7098 - branch_softmax_2_accuracy: 0.8664 - branch_softmax_evidence: 0.0084 - branch_softmax_mean_ev_succ: 0.1008 - branch_softmax_mean_ev_fail: 0.0569 - branch_softmax_1_evidence: 0.0116 - branch_softmax_1_mean_ev_succ: 0.1421 - branch_softmax_1_mean_ev_fail: 0.0502 - branch_softmax_2_evidence: 0.0355 - branch_softmax_2_mean_ev_succ: 0.4018 - branch_softmax_2_mean_ev_fail: 0.0517 - val_loss: 5.5059 - val_classification_loss: 2.2388 - val_branch_softmax_loss: 1.3927 - val_branch_softmax_1_loss: 1.2391 - val_branch_softmax_2_loss: 0.6353 - val_classification_accuracy: 0.9351 - val_branch_softmax_accuracy: 0.5066 - val_branch_softmax_1_accuracy: 0.5737 - val_branch_softmax_2_accuracy: 0.7903 - val_branch_softmax_evidence: 0.0058 - val_branch_softmax_mean_ev_succ: 0.0743 - val_branch_softmax_mean_ev_fail: 0.0412 - val_branch_softmax_1_evidence: 0.0086 - val_branch_softmax_1_mean_ev_succ: 0.1193 - val_branch_softmax_1_mean_ev_fail: 0.0401 - val_branch_softmax_2_evidence: 0.0161 - val_branch_softmax_2_mean_ev_succ: 0.1977 - val_branch_softmax_2_mean_ev_fail: 0.0186\n",
      "\n",
      "Epoch 00008: saving model to models\\mobileNetv2_evidence_flat.hdf5\n",
      "Epoch 9/12\n",
      "1406/1406 [==============================] - 424s 292ms/step - loss: 4.3078 - classification_loss: 2.1783 - branch_softmax_loss: 1.0023 - branch_softmax_1_loss: 0.7757 - branch_softmax_2_loss: 0.3515 - classification_accuracy: 0.9884 - branch_softmax_accuracy: 0.6427 - branch_softmax_1_accuracy: 0.7232 - branch_softmax_2_accuracy: 0.8773 - branch_softmax_evidence: 0.0082 - branch_softmax_mean_ev_succ: 0.0983 - branch_softmax_mean_ev_fail: 0.0529 - branch_softmax_1_evidence: 0.0116 - branch_softmax_1_mean_ev_succ: 0.1423 - branch_softmax_1_mean_ev_fail: 0.0470 - branch_softmax_2_evidence: 0.0383 - branch_softmax_2_mean_ev_succ: 0.4295 - branch_softmax_2_mean_ev_fail: 0.0493 - val_loss: 5.7404 - val_classification_loss: 2.2376 - val_branch_softmax_loss: 1.8731 - val_branch_softmax_1_loss: 1.0934 - val_branch_softmax_2_loss: 0.5362 - val_classification_accuracy: 0.9351 - val_branch_softmax_accuracy: 0.4313 - val_branch_softmax_1_accuracy: 0.6186 - val_branch_softmax_2_accuracy: 0.8143 - val_branch_softmax_evidence: 0.0092 - val_branch_softmax_mean_ev_succ: 0.1597 - val_branch_softmax_mean_ev_fail: 0.0400 - val_branch_softmax_1_evidence: 0.0081 - val_branch_softmax_1_mean_ev_succ: 0.1037 - val_branch_softmax_1_mean_ev_fail: 0.0437 - val_branch_softmax_2_evidence: 0.0258 - val_branch_softmax_2_mean_ev_succ: 0.3074 - val_branch_softmax_2_mean_ev_fail: 0.0391\n",
      "\n",
      "Epoch 00009: saving model to models\\mobileNetv2_evidence_flat.hdf5\n",
      "Epoch 10/12\n",
      "1406/1406 [==============================] - 426s 292ms/step - loss: 4.2523 - classification_loss: 2.1770 - branch_softmax_loss: 0.9913 - branch_softmax_1_loss: 0.7595 - branch_softmax_2_loss: 0.3244 - classification_accuracy: 0.9890 - branch_softmax_accuracy: 0.6465 - branch_softmax_1_accuracy: 0.7278 - branch_softmax_2_accuracy: 0.8857 - branch_softmax_evidence: 0.0081 - branch_softmax_mean_ev_succ: 0.0978 - branch_softmax_mean_ev_fail: 0.0504 - branch_softmax_1_evidence: 0.0113 - branch_softmax_1_mean_ev_succ: 0.1396 - branch_softmax_1_mean_ev_fail: 0.0428 - branch_softmax_2_evidence: 0.0417 - branch_softmax_2_mean_ev_succ: 0.4657 - branch_softmax_2_mean_ev_fail: 0.0463 - val_loss: 5.7652 - val_classification_loss: 2.2363 - val_branch_softmax_loss: 1.4624 - val_branch_softmax_1_loss: 1.3392 - val_branch_softmax_2_loss: 0.7272 - val_classification_accuracy: 0.9373 - val_branch_softmax_accuracy: 0.5038 - val_branch_softmax_1_accuracy: 0.5655 - val_branch_softmax_2_accuracy: 0.7698 - val_branch_softmax_evidence: 0.0082 - val_branch_softmax_mean_ev_succ: 0.1223 - val_branch_softmax_mean_ev_fail: 0.0424 - val_branch_softmax_1_evidence: 0.0169 - val_branch_softmax_1_mean_ev_succ: 0.2601 - val_branch_softmax_1_mean_ev_fail: 0.0501 - val_branch_softmax_2_evidence: 0.0569 - val_branch_softmax_2_mean_ev_succ: 0.7184 - val_branch_softmax_2_mean_ev_fail: 0.0750\n",
      "\n",
      "Epoch 00010: saving model to models\\mobileNetv2_evidence_flat.hdf5\n",
      "Epoch 11/12\n",
      "1406/1406 [==============================] - 426s 293ms/step - loss: 4.1541 - classification_loss: 2.1748 - branch_softmax_loss: 0.9619 - branch_softmax_1_loss: 0.7232 - branch_softmax_2_loss: 0.2942 - classification_accuracy: 0.9910 - branch_softmax_accuracy: 0.6556 - branch_softmax_1_accuracy: 0.7409 - branch_softmax_2_accuracy: 0.8986 - branch_softmax_evidence: 0.0078 - branch_softmax_mean_ev_succ: 0.0956 - branch_softmax_mean_ev_fail: 0.0464 - branch_softmax_1_evidence: 0.0117 - branch_softmax_1_mean_ev_succ: 0.1445 - branch_softmax_1_mean_ev_fail: 0.0404 - branch_softmax_2_evidence: 0.0450 - branch_softmax_2_mean_ev_succ: 0.4973 - branch_softmax_2_mean_ev_fail: 0.0456 - val_loss: 6.3853 - val_classification_loss: 2.2410 - val_branch_softmax_loss: 1.8898 - val_branch_softmax_1_loss: 1.5186 - val_branch_softmax_2_loss: 0.7360 - val_classification_accuracy: 0.9345 - val_branch_softmax_accuracy: 0.4313 - val_branch_softmax_1_accuracy: 0.5136 - val_branch_softmax_2_accuracy: 0.7630 - val_branch_softmax_evidence: 0.0050 - val_branch_softmax_mean_ev_succ: 0.0665 - val_branch_softmax_mean_ev_fail: 0.0370 - val_branch_softmax_1_evidence: 0.0125 - val_branch_softmax_1_mean_ev_succ: 0.1959 - val_branch_softmax_1_mean_ev_fail: 0.0463 - val_branch_softmax_2_evidence: 0.0274 - val_branch_softmax_2_mean_ev_succ: 0.3476 - val_branch_softmax_2_mean_ev_fail: 0.0350\n",
      "\n",
      "Epoch 00011: saving model to models\\mobileNetv2_evidence_flat.hdf5\n",
      "Epoch 12/12\n",
      "1406/1406 [==============================] - 427s 293ms/step - loss: 4.1010 - classification_loss: 2.1733 - branch_softmax_loss: 0.9516 - branch_softmax_1_loss: 0.7073 - branch_softmax_2_loss: 0.2688 - classification_accuracy: 0.9923 - branch_softmax_accuracy: 0.6568 - branch_softmax_1_accuracy: 0.7472 - branch_softmax_2_accuracy: 0.9076 - branch_softmax_evidence: 0.0079 - branch_softmax_mean_ev_succ: 0.0962 - branch_softmax_mean_ev_fail: 0.0459 - branch_softmax_1_evidence: 0.0119 - branch_softmax_1_mean_ev_succ: 0.1470 - branch_softmax_1_mean_ev_fail: 0.0395 - branch_softmax_2_evidence: 0.0473 - branch_softmax_2_mean_ev_succ: 0.5178 - branch_softmax_2_mean_ev_fail: 0.0438 - val_loss: 6.4497 - val_classification_loss: 2.2329 - val_branch_softmax_loss: 1.7099 - val_branch_softmax_1_loss: 1.8391 - val_branch_softmax_2_loss: 0.6677 - val_classification_accuracy: 0.9409 - val_branch_softmax_accuracy: 0.4385 - val_branch_softmax_1_accuracy: 0.4776 - val_branch_softmax_2_accuracy: 0.7933 - val_branch_softmax_evidence: 0.0060 - val_branch_softmax_mean_ev_succ: 0.0907 - val_branch_softmax_mean_ev_fail: 0.0362 - val_branch_softmax_1_evidence: 0.0052 - val_branch_softmax_1_mean_ev_succ: 0.0865 - val_branch_softmax_1_mean_ev_fail: 0.0194 - val_branch_softmax_2_evidence: 0.0460 - val_branch_softmax_2_mean_ev_succ: 0.5699 - val_branch_softmax_2_mean_ev_fail: 0.0376\n",
      "\n",
      "Epoch 00012: saving model to models\\mobileNetv2_evidence_flat.hdf5\n",
      "<tensorflow.python.keras.callbacks.History object at 0x000002134E2C3D30>\n",
      "312/312 - 29s - loss: 6.4907 - classification_loss: 2.2433 - branch_softmax_loss: 1.7052 - branch_softmax_1_loss: 1.8078 - branch_softmax_2_loss: 0.7344 - classification_accuracy: 0.9300 - branch_softmax_accuracy: 0.4363 - branch_softmax_1_accuracy: 0.4856 - branch_softmax_2_accuracy: 0.7763 - branch_softmax_evidence: 0.0065 - branch_softmax_mean_ev_succ: 0.0976 - branch_softmax_mean_ev_fail: 0.0399 - branch_softmax_1_evidence: 0.0055 - branch_softmax_1_mean_ev_succ: 0.0908 - branch_softmax_1_mean_ev_fail: 0.0207 - branch_softmax_2_evidence: 0.0470 - branch_softmax_2_mean_ev_succ: 0.5917 - branch_softmax_2_mean_ev_fail: 0.0428\n",
      "overall loss: 6.490667343139648\n"
     ]
    }
   ],
   "source": [
    "model = brevis.trainTransfer(12, loss=loss_fn, optimizer=tf.optimizers.SGD(), transfer=False, customOptions=\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting Main Layers  and branch layers training to true\n",
      "\n",
      "customOption: Other\n",
      "https://app.neptune.ai/cailen01/branchingDNN/e/BRAN-408\n",
      "Remember to stop your run once youve finished logging your metadata (https://docs.neptune.ai/api-reference/run#stop). It will be stopped automatically only when the notebook kernel/interactive console is terminated.\n",
      "Epoch 1/18\n",
      "1406/1406 [==============================] - ETA: 0s - loss: 4.0479 - classification_loss: 2.1721 - branch_softmax_loss: 0.9353 - branch_softmax_1_loss: 0.6901 - branch_softmax_2_loss: 0.2503 - classification_accuracy: 0.9931 - branch_softmax_accuracy: 0.6646 - branch_softmax_1_accuracy: 0.7542 - branch_softmax_2_accuracy: 0.9116 - branch_softmax_evidence: 0.0078 - branch_softmax_mean_ev_succ: 0.0957 - branch_softmax_mean_ev_fail: 0.0440 - branch_softmax_1_evidence: 0.0120 - branch_softmax_1_mean_ev_succ: 0.1475 - branch_softmax_1_mean_ev_fail: 0.0366 - branch_softmax_2_evidence: 0.0522 - branch_softmax_2_mean_ev_succ: 0.5697 - branch_softmax_2_mean_ev_fail: 0.0424"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: string series 'monitoring/stdout' value was longer than 1000 characters and was truncated. This warning is printed only once per series.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1406/1406 [==============================] - 436s 299ms/step - loss: 4.0479 - classification_loss: 2.1721 - branch_softmax_loss: 0.9353 - branch_softmax_1_loss: 0.6902 - branch_softmax_2_loss: 0.2503 - classification_accuracy: 0.9931 - branch_softmax_accuracy: 0.6646 - branch_softmax_1_accuracy: 0.7541 - branch_softmax_2_accuracy: 0.9116 - branch_softmax_evidence: 0.0078 - branch_softmax_mean_ev_succ: 0.0957 - branch_softmax_mean_ev_fail: 0.0440 - branch_softmax_1_evidence: 0.0120 - branch_softmax_1_mean_ev_succ: 0.1475 - branch_softmax_1_mean_ev_fail: 0.0366 - branch_softmax_2_evidence: 0.0522 - branch_softmax_2_mean_ev_succ: 0.5697 - branch_softmax_2_mean_ev_fail: 0.0424 - val_loss: 4.9971 - val_classification_loss: 2.2329 - val_branch_softmax_loss: 1.3407 - val_branch_softmax_1_loss: 0.9056 - val_branch_softmax_2_loss: 0.5179 - val_classification_accuracy: 0.9389 - val_branch_softmax_accuracy: 0.5371 - val_branch_softmax_1_accuracy: 0.6617 - val_branch_softmax_2_accuracy: 0.8223 - val_branch_softmax_evidence: 0.0013 - val_branch_softmax_mean_ev_succ: 0.0178 - val_branch_softmax_mean_ev_fail: 0.0079 - val_branch_softmax_1_evidence: 0.0080 - val_branch_softmax_1_mean_ev_succ: 0.1062 - val_branch_softmax_1_mean_ev_fail: 0.0282 - val_branch_softmax_2_evidence: 0.0578 - val_branch_softmax_2_mean_ev_succ: 0.6899 - val_branch_softmax_2_mean_ev_fail: 0.0613\n",
      "\n",
      "Epoch 00001: saving model to models\\mobileNetv2_evidence_flat_30.hdf5\n",
      "Epoch 2/18\n",
      "1406/1406 [==============================] - 438s 301ms/step - loss: 4.0207 - classification_loss: 2.1714 - branch_softmax_loss: 0.9310 - branch_softmax_1_loss: 0.6832 - branch_softmax_2_loss: 0.2350 - classification_accuracy: 0.9931 - branch_softmax_accuracy: 0.6657 - branch_softmax_1_accuracy: 0.7566 - branch_softmax_2_accuracy: 0.9168 - branch_softmax_evidence: 0.0077 - branch_softmax_mean_ev_succ: 0.0933 - branch_softmax_mean_ev_fail: 0.0430 - branch_softmax_1_evidence: 0.0120 - branch_softmax_1_mean_ev_succ: 0.1474 - branch_softmax_1_mean_ev_fail: 0.0351 - branch_softmax_2_evidence: 0.0571 - branch_softmax_2_mean_ev_succ: 0.6210 - branch_softmax_2_mean_ev_fail: 0.0414 - val_loss: 6.6217 - val_classification_loss: 2.2348 - val_branch_softmax_loss: 2.2761 - val_branch_softmax_1_loss: 1.5014 - val_branch_softmax_2_loss: 0.6094 - val_classification_accuracy: 0.9379 - val_branch_softmax_accuracy: 0.3502 - val_branch_softmax_1_accuracy: 0.5379 - val_branch_softmax_2_accuracy: 0.8139 - val_branch_softmax_evidence: 0.0095 - val_branch_softmax_mean_ev_succ: 0.1774 - val_branch_softmax_mean_ev_fail: 0.0498 - val_branch_softmax_1_evidence: 0.0039 - val_branch_softmax_1_mean_ev_succ: 0.0602 - val_branch_softmax_1_mean_ev_fail: 0.0135 - val_branch_softmax_2_evidence: 0.0353 - val_branch_softmax_2_mean_ev_succ: 0.4267 - val_branch_softmax_2_mean_ev_fail: 0.0328\n",
      "\n",
      "Epoch 00002: saving model to models\\mobileNetv2_evidence_flat_30.hdf5\n",
      "Epoch 3/18\n",
      "1406/1406 [==============================] - 435s 299ms/step - loss: 3.9640 - classification_loss: 2.1699 - branch_softmax_loss: 0.9145 - branch_softmax_1_loss: 0.6637 - branch_softmax_2_loss: 0.2159 - classification_accuracy: 0.9948 - branch_softmax_accuracy: 0.6756 - branch_softmax_1_accuracy: 0.7642 - branch_softmax_2_accuracy: 0.9213 - branch_softmax_evidence: 0.0076 - branch_softmax_mean_ev_succ: 0.0932 - branch_softmax_mean_ev_fail: 0.0418 - branch_softmax_1_evidence: 0.0118 - branch_softmax_1_mean_ev_succ: 0.1441 - branch_softmax_1_mean_ev_fail: 0.0350 - branch_softmax_2_evidence: 0.0594 - branch_softmax_2_mean_ev_succ: 0.6415 - branch_softmax_2_mean_ev_fail: 0.0404 - val_loss: 6.6065 - val_classification_loss: 2.2308 - val_branch_softmax_loss: 2.5246 - val_branch_softmax_1_loss: 1.2034 - val_branch_softmax_2_loss: 0.6477 - val_classification_accuracy: 0.9431 - val_branch_softmax_accuracy: 0.3498 - val_branch_softmax_1_accuracy: 0.6102 - val_branch_softmax_2_accuracy: 0.7999 - val_branch_softmax_evidence: 0.0099 - val_branch_softmax_mean_ev_succ: 0.1914 - val_branch_softmax_mean_ev_fail: 0.0510 - val_branch_softmax_1_evidence: 0.0098 - val_branch_softmax_1_mean_ev_succ: 0.1371 - val_branch_softmax_1_mean_ev_fail: 0.0369 - val_branch_softmax_2_evidence: 0.0308 - val_branch_softmax_2_mean_ev_succ: 0.3799 - val_branch_softmax_2_mean_ev_fail: 0.0147\n",
      "\n",
      "Epoch 00003: saving model to models\\mobileNetv2_evidence_flat_30.hdf5\n",
      "Epoch 4/18\n",
      "1406/1406 [==============================] - 438s 301ms/step - loss: 3.9130 - classification_loss: 2.1704 - branch_softmax_loss: 0.8972 - branch_softmax_1_loss: 0.6459 - branch_softmax_2_loss: 0.1995 - classification_accuracy: 0.9939 - branch_softmax_accuracy: 0.6791 - branch_softmax_1_accuracy: 0.7719 - branch_softmax_2_accuracy: 0.9312 - branch_softmax_evidence: 0.0077 - branch_softmax_mean_ev_succ: 0.0946 - branch_softmax_mean_ev_fail: 0.0410 - branch_softmax_1_evidence: 0.0121 - branch_softmax_1_mean_ev_succ: 0.1482 - branch_softmax_1_mean_ev_fail: 0.0336 - branch_softmax_2_evidence: 0.0621 - branch_softmax_2_mean_ev_succ: 0.6676 - branch_softmax_2_mean_ev_fail: 0.0349 - val_loss: 6.7025 - val_classification_loss: 2.2374 - val_branch_softmax_loss: 2.0700 - val_branch_softmax_1_loss: 1.3728 - val_branch_softmax_2_loss: 1.0223 - val_classification_accuracy: 0.9345 - val_branch_softmax_accuracy: 0.3778 - val_branch_softmax_1_accuracy: 0.5769 - val_branch_softmax_2_accuracy: 0.7276 - val_branch_softmax_evidence: 0.0091 - val_branch_softmax_mean_ev_succ: 0.1218 - val_branch_softmax_mean_ev_fail: 0.0729 - val_branch_softmax_1_evidence: 0.0022 - val_branch_softmax_1_mean_ev_succ: 0.0340 - val_branch_softmax_1_mean_ev_fail: 0.0052 - val_branch_softmax_2_evidence: 0.0505 - val_branch_softmax_2_mean_ev_succ: 0.6783 - val_branch_softmax_2_mean_ev_fail: 0.0555\n",
      "\n",
      "Epoch 00004: saving model to models\\mobileNetv2_evidence_flat_30.hdf5\n",
      "Epoch 5/18\n",
      "1406/1406 [==============================] - 446s 306ms/step - loss: 3.8842 - classification_loss: 2.1701 - branch_softmax_loss: 0.8919 - branch_softmax_1_loss: 0.6374 - branch_softmax_2_loss: 0.1848 - classification_accuracy: 0.9939 - branch_softmax_accuracy: 0.6809 - branch_softmax_1_accuracy: 0.7720 - branch_softmax_2_accuracy: 0.9362 - branch_softmax_evidence: 0.0075 - branch_softmax_mean_ev_succ: 0.0920 - branch_softmax_mean_ev_fail: 0.0400 - branch_softmax_1_evidence: 0.0124 - branch_softmax_1_mean_ev_succ: 0.1511 - branch_softmax_1_mean_ev_fail: 0.0335 - branch_softmax_2_evidence: 0.0674 - branch_softmax_2_mean_ev_succ: 0.7197 - branch_softmax_2_mean_ev_fail: 0.0352 - val_loss: 6.5865 - val_classification_loss: 2.2386 - val_branch_softmax_loss: 2.5430 - val_branch_softmax_1_loss: 1.1410 - val_branch_softmax_2_loss: 0.6639 - val_classification_accuracy: 0.9325 - val_branch_softmax_accuracy: 0.3255 - val_branch_softmax_1_accuracy: 0.6216 - val_branch_softmax_2_accuracy: 0.7997 - val_branch_softmax_evidence: 0.0123 - val_branch_softmax_mean_ev_succ: 0.2148 - val_branch_softmax_mean_ev_fail: 0.0768 - val_branch_softmax_1_evidence: 0.0076 - val_branch_softmax_1_mean_ev_succ: 0.1117 - val_branch_softmax_1_mean_ev_fail: 0.0161 - val_branch_softmax_2_evidence: 0.0621 - val_branch_softmax_2_mean_ev_succ: 0.7652 - val_branch_softmax_2_mean_ev_fail: 0.0396\n",
      "\n",
      "Epoch 00005: saving model to models\\mobileNetv2_evidence_flat_30.hdf5\n",
      "Epoch 6/18\n",
      "1406/1406 [==============================] - 449s 308ms/step - loss: 3.8795 - classification_loss: 2.1696 - branch_softmax_loss: 0.8986 - branch_softmax_1_loss: 0.6373 - branch_softmax_2_loss: 0.1739 - classification_accuracy: 0.9947 - branch_softmax_accuracy: 0.6762 - branch_softmax_1_accuracy: 0.7693 - branch_softmax_2_accuracy: 0.9385 - branch_softmax_evidence: 0.0076 - branch_softmax_mean_ev_succ: 0.0935 - branch_softmax_mean_ev_fail: 0.0391 - branch_softmax_1_evidence: 0.0118 - branch_softmax_1_mean_ev_succ: 0.1442 - branch_softmax_1_mean_ev_fail: 0.0316 - branch_softmax_2_evidence: 0.0720 - branch_softmax_2_mean_ev_succ: 0.7660 - branch_softmax_2_mean_ev_fail: 0.0351 - val_loss: 5.9625 - val_classification_loss: 2.2354 - val_branch_softmax_loss: 1.8395 - val_branch_softmax_1_loss: 1.3828 - val_branch_softmax_2_loss: 0.5048 - val_classification_accuracy: 0.9365 - val_branch_softmax_accuracy: 0.4002 - val_branch_softmax_1_accuracy: 0.5543 - val_branch_softmax_2_accuracy: 0.8452 - val_branch_softmax_evidence: 0.0082 - val_branch_softmax_mean_ev_succ: 0.1232 - val_branch_softmax_mean_ev_fail: 0.0520 - val_branch_softmax_1_evidence: 0.0109 - val_branch_softmax_1_mean_ev_succ: 0.1697 - val_branch_softmax_1_mean_ev_fail: 0.0303 - val_branch_softmax_2_evidence: 0.0593 - val_branch_softmax_2_mean_ev_succ: 0.6958 - val_branch_softmax_2_mean_ev_fail: 0.0244\n",
      "\n",
      "Epoch 00006: saving model to models\\mobileNetv2_evidence_flat_30.hdf5\n",
      "Epoch 7/18\n",
      "1406/1406 [==============================] - 443s 304ms/step - loss: 3.8216 - classification_loss: 2.1684 - branch_softmax_loss: 0.8764 - branch_softmax_1_loss: 0.6115 - branch_softmax_2_loss: 0.1653 - classification_accuracy: 0.9958 - branch_softmax_accuracy: 0.6880 - branch_softmax_1_accuracy: 0.7788 - branch_softmax_2_accuracy: 0.9398 - branch_softmax_evidence: 0.0077 - branch_softmax_mean_ev_succ: 0.0942 - branch_softmax_mean_ev_fail: 0.0379 - branch_softmax_1_evidence: 0.0123 - branch_softmax_1_mean_ev_succ: 0.1500 - branch_softmax_1_mean_ev_fail: 0.0313 - branch_softmax_2_evidence: 0.0781 - branch_softmax_2_mean_ev_succ: 0.8288 - branch_softmax_2_mean_ev_fail: 0.0327 - val_loss: 5.4435 - val_classification_loss: 2.2315 - val_branch_softmax_loss: 1.7073 - val_branch_softmax_1_loss: 0.8821 - val_branch_softmax_2_loss: 0.6226 - val_classification_accuracy: 0.9403 - val_branch_softmax_accuracy: 0.4884 - val_branch_softmax_1_accuracy: 0.6869 - val_branch_softmax_2_accuracy: 0.8111 - val_branch_softmax_evidence: 0.0067 - val_branch_softmax_mean_ev_succ: 0.1089 - val_branch_softmax_mean_ev_fail: 0.0269 - val_branch_softmax_1_evidence: 0.0115 - val_branch_softmax_1_mean_ev_succ: 0.1532 - val_branch_softmax_1_mean_ev_fail: 0.0291 - val_branch_softmax_2_evidence: 0.0662 - val_branch_softmax_2_mean_ev_succ: 0.8001 - val_branch_softmax_2_mean_ev_fail: 0.0564\n",
      "\n",
      "Epoch 00007: saving model to models\\mobileNetv2_evidence_flat_30.hdf5\n",
      "Epoch 8/18\n",
      "1406/1406 [==============================] - 447s 307ms/step - loss: 3.7996 - classification_loss: 2.1687 - branch_softmax_loss: 0.8668 - branch_softmax_1_loss: 0.6096 - branch_softmax_2_loss: 0.1545 - classification_accuracy: 0.9950 - branch_softmax_accuracy: 0.6872 - branch_softmax_1_accuracy: 0.7820 - branch_softmax_2_accuracy: 0.9446 - branch_softmax_evidence: 0.0075 - branch_softmax_mean_ev_succ: 0.0922 - branch_softmax_mean_ev_fail: 0.0366 - branch_softmax_1_evidence: 0.0121 - branch_softmax_1_mean_ev_succ: 0.1477 - branch_softmax_1_mean_ev_fail: 0.0296 - branch_softmax_2_evidence: 0.0801 - branch_softmax_2_mean_ev_succ: 0.8477 - branch_softmax_2_mean_ev_fail: 0.0315 - val_loss: 6.2962 - val_classification_loss: 2.2418 - val_branch_softmax_loss: 2.2518 - val_branch_softmax_1_loss: 1.1212 - val_branch_softmax_2_loss: 0.6814 - val_classification_accuracy: 0.9321 - val_branch_softmax_accuracy: 0.3756 - val_branch_softmax_1_accuracy: 0.6166 - val_branch_softmax_2_accuracy: 0.8053 - val_branch_softmax_evidence: 0.0049 - val_branch_softmax_mean_ev_succ: 0.0688 - val_branch_softmax_mean_ev_fail: 0.0380 - val_branch_softmax_1_evidence: 0.0101 - val_branch_softmax_1_mean_ev_succ: 0.1466 - val_branch_softmax_1_mean_ev_fail: 0.0282 - val_branch_softmax_2_evidence: 0.0502 - val_branch_softmax_2_mean_ev_succ: 0.6126 - val_branch_softmax_2_mean_ev_fail: 0.0430\n",
      "\n",
      "Epoch 00008: saving model to models\\mobileNetv2_evidence_flat_30.hdf5\n",
      "Epoch 9/18\n",
      "1406/1406 [==============================] - 449s 308ms/step - loss: 3.7728 - classification_loss: 2.1682 - branch_softmax_loss: 0.8656 - branch_softmax_1_loss: 0.5956 - branch_softmax_2_loss: 0.1434 - classification_accuracy: 0.9955 - branch_softmax_accuracy: 0.6874 - branch_softmax_1_accuracy: 0.7877 - branch_softmax_2_accuracy: 0.9488 - branch_softmax_evidence: 0.0074 - branch_softmax_mean_ev_succ: 0.0911 - branch_softmax_mean_ev_fail: 0.0365 - branch_softmax_1_evidence: 0.0126 - branch_softmax_1_mean_ev_succ: 0.1532 - branch_softmax_1_mean_ev_fail: 0.0301 - branch_softmax_2_evidence: 0.0831 - branch_softmax_2_mean_ev_succ: 0.8764 - branch_softmax_2_mean_ev_fail: 0.0337 - val_loss: 5.0491 - val_classification_loss: 2.2341 - val_branch_softmax_loss: 1.1446 - val_branch_softmax_1_loss: 1.1137 - val_branch_softmax_2_loss: 0.5567 - val_classification_accuracy: 0.9395 - val_branch_softmax_accuracy: 0.6070 - val_branch_softmax_1_accuracy: 0.6350 - val_branch_softmax_2_accuracy: 0.8417 - val_branch_softmax_evidence: 0.0080 - val_branch_softmax_mean_ev_succ: 0.1137 - val_branch_softmax_mean_ev_fail: 0.0284 - val_branch_softmax_1_evidence: 0.0102 - val_branch_softmax_1_mean_ev_succ: 0.1450 - val_branch_softmax_1_mean_ev_fail: 0.0299 - val_branch_softmax_2_evidence: 0.0624 - val_branch_softmax_2_mean_ev_succ: 0.7337 - val_branch_softmax_2_mean_ev_fail: 0.0377\n",
      "\n",
      "Epoch 00009: saving model to models\\mobileNetv2_evidence_flat_30.hdf5\n",
      "Epoch 10/18\n",
      "1406/1406 [==============================] - 451s 310ms/step - loss: 3.7362 - classification_loss: 2.1679 - branch_softmax_loss: 0.8512 - branch_softmax_1_loss: 0.5850 - branch_softmax_2_loss: 0.1321 - classification_accuracy: 0.9957 - branch_softmax_accuracy: 0.6925 - branch_softmax_1_accuracy: 0.7889 - branch_softmax_2_accuracy: 0.9526 - branch_softmax_evidence: 0.0075 - branch_softmax_mean_ev_succ: 0.0921 - branch_softmax_mean_ev_fail: 0.0359 - branch_softmax_1_evidence: 0.0134 - branch_softmax_1_mean_ev_succ: 0.1620 - branch_softmax_1_mean_ev_fail: 0.0315 - branch_softmax_2_evidence: 0.0894 - branch_softmax_2_mean_ev_succ: 0.9390 - branch_softmax_2_mean_ev_fail: 0.0276 - val_loss: 5.7113 - val_classification_loss: 2.2342 - val_branch_softmax_loss: 1.5631 - val_branch_softmax_1_loss: 1.2649 - val_branch_softmax_2_loss: 0.6491 - val_classification_accuracy: 0.9375 - val_branch_softmax_accuracy: 0.4796 - val_branch_softmax_1_accuracy: 0.6038 - val_branch_softmax_2_accuracy: 0.8271 - val_branch_softmax_evidence: 0.0019 - val_branch_softmax_mean_ev_succ: 0.0277 - val_branch_softmax_mean_ev_fail: 0.0108 - val_branch_softmax_1_evidence: 0.0064 - val_branch_softmax_1_mean_ev_succ: 0.0921 - val_branch_softmax_1_mean_ev_fail: 0.0201 - val_branch_softmax_2_evidence: 0.0767 - val_branch_softmax_2_mean_ev_succ: 0.9152 - val_branch_softmax_2_mean_ev_fail: 0.0394\n",
      "\n",
      "Epoch 00010: saving model to models\\mobileNetv2_evidence_flat_30.hdf5\n",
      "Epoch 11/18\n",
      "1406/1406 [==============================] - 454s 311ms/step - loss: 3.7362 - classification_loss: 2.1672 - branch_softmax_loss: 0.8584 - branch_softmax_1_loss: 0.5854 - branch_softmax_2_loss: 0.1253 - classification_accuracy: 0.9962 - branch_softmax_accuracy: 0.6919 - branch_softmax_1_accuracy: 0.7885 - branch_softmax_2_accuracy: 0.9544 - branch_softmax_evidence: 0.0076 - branch_softmax_mean_ev_succ: 0.0934 - branch_softmax_mean_ev_fail: 0.0356 - branch_softmax_1_evidence: 0.0131 - branch_softmax_1_mean_ev_succ: 0.1581 - branch_softmax_1_mean_ev_fail: 0.0300 - branch_softmax_2_evidence: 0.0963 - branch_softmax_2_mean_ev_succ: 1.0097 - branch_softmax_2_mean_ev_fail: 0.0282 - val_loss: 5.7361 - val_classification_loss: 2.2342 - val_branch_softmax_loss: 1.9385 - val_branch_softmax_1_loss: 0.8891 - val_branch_softmax_2_loss: 0.6743 - val_classification_accuracy: 0.9397 - val_branch_softmax_accuracy: 0.3886 - val_branch_softmax_1_accuracy: 0.6943 - val_branch_softmax_2_accuracy: 0.8113 - val_branch_softmax_evidence: 0.0073 - val_branch_softmax_mean_ev_succ: 0.1283 - val_branch_softmax_mean_ev_fail: 0.0384 - val_branch_softmax_1_evidence: 0.0134 - val_branch_softmax_1_mean_ev_succ: 0.1779 - val_branch_softmax_1_mean_ev_fail: 0.0327 - val_branch_softmax_2_evidence: 0.0611 - val_branch_softmax_2_mean_ev_succ: 0.7378 - val_branch_softmax_2_mean_ev_fail: 0.0412\n",
      "\n",
      "Epoch 00011: saving model to models\\mobileNetv2_evidence_flat_30.hdf5\n",
      "Epoch 12/18\n",
      "1406/1406 [==============================] - 455s 312ms/step - loss: 3.7174 - classification_loss: 2.1664 - branch_softmax_loss: 0.8518 - branch_softmax_1_loss: 0.5781 - branch_softmax_2_loss: 0.1211 - classification_accuracy: 0.9968 - branch_softmax_accuracy: 0.6918 - branch_softmax_1_accuracy: 0.7913 - branch_softmax_2_accuracy: 0.9573 - branch_softmax_evidence: 0.0074 - branch_softmax_mean_ev_succ: 0.0922 - branch_softmax_mean_ev_fail: 0.0347 - branch_softmax_1_evidence: 0.0131 - branch_softmax_1_mean_ev_succ: 0.1580 - branch_softmax_1_mean_ev_fail: 0.0276 - branch_softmax_2_evidence: 0.0996 - branch_softmax_2_mean_ev_succ: 1.0412 - branch_softmax_2_mean_ev_fail: 0.0270 - val_loss: 5.9228 - val_classification_loss: 2.2427 - val_branch_softmax_loss: 1.8608 - val_branch_softmax_1_loss: 1.0908 - val_branch_softmax_2_loss: 0.7285 - val_classification_accuracy: 0.9299 - val_branch_softmax_accuracy: 0.4211 - val_branch_softmax_1_accuracy: 0.6440 - val_branch_softmax_2_accuracy: 0.8221 - val_branch_softmax_evidence: 0.0036 - val_branch_softmax_mean_ev_succ: 0.0609 - val_branch_softmax_mean_ev_fail: 0.0183 - val_branch_softmax_1_evidence: 0.0069 - val_branch_softmax_1_mean_ev_succ: 0.0966 - val_branch_softmax_1_mean_ev_fail: 0.0200 - val_branch_softmax_2_evidence: 0.0651 - val_branch_softmax_2_mean_ev_succ: 0.7778 - val_branch_softmax_2_mean_ev_fail: 0.0600\n",
      "\n",
      "Epoch 00012: saving model to models\\mobileNetv2_evidence_flat_30.hdf5\n",
      "Epoch 13/18\n",
      "1406/1406 [==============================] - 455s 312ms/step - loss: 3.6929 - classification_loss: 2.1676 - branch_softmax_loss: 0.8432 - branch_softmax_1_loss: 0.5687 - branch_softmax_2_loss: 0.1134 - classification_accuracy: 0.9956 - branch_softmax_accuracy: 0.6975 - branch_softmax_1_accuracy: 0.7948 - branch_softmax_2_accuracy: 0.9589 - branch_softmax_evidence: 0.0074 - branch_softmax_mean_ev_succ: 0.0921 - branch_softmax_mean_ev_fail: 0.0329 - branch_softmax_1_evidence: 0.0142 - branch_softmax_1_mean_ev_succ: 0.1707 - branch_softmax_1_mean_ev_fail: 0.0268 - branch_softmax_2_evidence: 0.1029 - branch_softmax_2_mean_ev_succ: 1.0748 - branch_softmax_2_mean_ev_fail: 0.0243 - val_loss: 5.2304 - val_classification_loss: 2.2336 - val_branch_softmax_loss: 1.2607 - val_branch_softmax_1_loss: 1.0181 - val_branch_softmax_2_loss: 0.7179 - val_classification_accuracy: 0.9381 - val_branch_softmax_accuracy: 0.5669 - val_branch_softmax_1_accuracy: 0.6657 - val_branch_softmax_2_accuracy: 0.8145 - val_branch_softmax_evidence: 0.0013 - val_branch_softmax_mean_ev_succ: 0.0195 - val_branch_softmax_mean_ev_fail: 0.0042 - val_branch_softmax_1_evidence: 0.0038 - val_branch_softmax_1_mean_ev_succ: 0.0536 - val_branch_softmax_1_mean_ev_fail: 0.0069 - val_branch_softmax_2_evidence: 0.0908 - val_branch_softmax_2_mean_ev_succ: 1.1044 - val_branch_softmax_2_mean_ev_fail: 0.0440\n",
      "\n",
      "Epoch 00013: saving model to models\\mobileNetv2_evidence_flat_30.hdf5\n",
      "Epoch 14/18\n",
      "1406/1406 [==============================] - 458s 314ms/step - loss: 3.6655 - classification_loss: 2.1662 - branch_softmax_loss: 0.8380 - branch_softmax_1_loss: 0.5609 - branch_softmax_2_loss: 0.1004 - classification_accuracy: 0.9969 - branch_softmax_accuracy: 0.7003 - branch_softmax_1_accuracy: 0.7983 - branch_softmax_2_accuracy: 0.9650 - branch_softmax_evidence: 0.0073 - branch_softmax_mean_ev_succ: 0.0904 - branch_softmax_mean_ev_fail: 0.0328 - branch_softmax_1_evidence: 0.0139 - branch_softmax_1_mean_ev_succ: 0.1673 - branch_softmax_1_mean_ev_fail: 0.0276 - branch_softmax_2_evidence: 0.1057 - branch_softmax_2_mean_ev_succ: 1.0997 - branch_softmax_2_mean_ev_fail: 0.0245 - val_loss: 5.7111 - val_classification_loss: 2.2382 - val_branch_softmax_loss: 1.5831 - val_branch_softmax_1_loss: 1.0337 - val_branch_softmax_2_loss: 0.8562 - val_classification_accuracy: 0.9345 - val_branch_softmax_accuracy: 0.4730 - val_branch_softmax_1_accuracy: 0.6637 - val_branch_softmax_2_accuracy: 0.7997 - val_branch_softmax_evidence: 0.0023 - val_branch_softmax_mean_ev_succ: 0.0312 - val_branch_softmax_mean_ev_fail: 0.0144 - val_branch_softmax_1_evidence: 0.0073 - val_branch_softmax_1_mean_ev_succ: 0.0967 - val_branch_softmax_1_mean_ev_fail: 0.0239 - val_branch_softmax_2_evidence: 0.0693 - val_branch_softmax_2_mean_ev_succ: 0.8608 - val_branch_softmax_2_mean_ev_fail: 0.0290\n",
      "\n",
      "Epoch 00014: saving model to models\\mobileNetv2_evidence_flat_30.hdf5\n",
      "Epoch 15/18\n",
      "1406/1406 [==============================] - 460s 316ms/step - loss: 3.6452 - classification_loss: 2.1664 - branch_softmax_loss: 0.8319 - branch_softmax_1_loss: 0.5497 - branch_softmax_2_loss: 0.0973 - classification_accuracy: 0.9965 - branch_softmax_accuracy: 0.7025 - branch_softmax_1_accuracy: 0.8031 - branch_softmax_2_accuracy: 0.9651 - branch_softmax_evidence: 0.0075 - branch_softmax_mean_ev_succ: 0.0938 - branch_softmax_mean_ev_fail: 0.0325 - branch_softmax_1_evidence: 0.0145 - branch_softmax_1_mean_ev_succ: 0.1740 - branch_softmax_1_mean_ev_fail: 0.0279 - branch_softmax_2_evidence: 0.1097 - branch_softmax_2_mean_ev_succ: 1.1377 - branch_softmax_2_mean_ev_fail: 0.0251 - val_loss: 6.0709 - val_classification_loss: 2.2390 - val_branch_softmax_loss: 2.2015 - val_branch_softmax_1_loss: 0.9251 - val_branch_softmax_2_loss: 0.7053 - val_classification_accuracy: 0.9339 - val_branch_softmax_accuracy: 0.3922 - val_branch_softmax_1_accuracy: 0.6749 - val_branch_softmax_2_accuracy: 0.8183 - val_branch_softmax_evidence: 0.0051 - val_branch_softmax_mean_ev_succ: 0.0833 - val_branch_softmax_mean_ev_fail: 0.0309 - val_branch_softmax_1_evidence: 0.0056 - val_branch_softmax_1_mean_ev_succ: 0.0744 - val_branch_softmax_1_mean_ev_fail: 0.0157 - val_branch_softmax_2_evidence: 0.0436 - val_branch_softmax_2_mean_ev_succ: 0.5320 - val_branch_softmax_2_mean_ev_fail: 0.0073\n",
      "\n",
      "Epoch 00015: saving model to models\\mobileNetv2_evidence_flat_30.hdf5\n",
      "Epoch 16/18\n",
      "1406/1406 [==============================] - 465s 319ms/step - loss: 3.6431 - classification_loss: 2.1665 - branch_softmax_loss: 0.8298 - branch_softmax_1_loss: 0.5489 - branch_softmax_2_loss: 0.0979 - classification_accuracy: 0.9965 - branch_softmax_accuracy: 0.7001 - branch_softmax_1_accuracy: 0.8025 - branch_softmax_2_accuracy: 0.9656 - branch_softmax_evidence: 0.0075 - branch_softmax_mean_ev_succ: 0.0925 - branch_softmax_mean_ev_fail: 0.0322 - branch_softmax_1_evidence: 0.0144 - branch_softmax_1_mean_ev_succ: 0.1726 - branch_softmax_1_mean_ev_fail: 0.0271 - branch_softmax_2_evidence: 0.1127 - branch_softmax_2_mean_ev_succ: 1.1683 - branch_softmax_2_mean_ev_fail: 0.0235 - val_loss: 5.7565 - val_classification_loss: 2.2426 - val_branch_softmax_loss: 1.8371 - val_branch_softmax_1_loss: 0.9761 - val_branch_softmax_2_loss: 0.7007 - val_classification_accuracy: 0.9299 - val_branch_softmax_accuracy: 0.4425 - val_branch_softmax_1_accuracy: 0.6673 - val_branch_softmax_2_accuracy: 0.8223 - val_branch_softmax_evidence: 0.0031 - val_branch_softmax_mean_ev_succ: 0.0479 - val_branch_softmax_mean_ev_fail: 0.0170 - val_branch_softmax_1_evidence: 0.0090 - val_branch_softmax_1_mean_ev_succ: 0.1240 - val_branch_softmax_1_mean_ev_fail: 0.0216 - val_branch_softmax_2_evidence: 0.0896 - val_branch_softmax_2_mean_ev_succ: 1.0767 - val_branch_softmax_2_mean_ev_fail: 0.0523\n",
      "\n",
      "Epoch 00016: saving model to models\\mobileNetv2_evidence_flat_30.hdf5\n",
      "Epoch 17/18\n",
      "1406/1406 [==============================] - 467s 320ms/step - loss: 3.6160 - classification_loss: 2.1661 - branch_softmax_loss: 0.8194 - branch_softmax_1_loss: 0.5396 - branch_softmax_2_loss: 0.0909 - classification_accuracy: 0.9968 - branch_softmax_accuracy: 0.7054 - branch_softmax_1_accuracy: 0.8059 - branch_softmax_2_accuracy: 0.9677 - branch_softmax_evidence: 0.0074 - branch_softmax_mean_ev_succ: 0.0918 - branch_softmax_mean_ev_fail: 0.0324 - branch_softmax_1_evidence: 0.0141 - branch_softmax_1_mean_ev_succ: 0.1682 - branch_softmax_1_mean_ev_fail: 0.0271 - branch_softmax_2_evidence: 0.1169 - branch_softmax_2_mean_ev_succ: 1.2106 - branch_softmax_2_mean_ev_fail: 0.0206 - val_loss: 6.4047 - val_classification_loss: 2.2426 - val_branch_softmax_loss: 1.8262 - val_branch_softmax_1_loss: 1.2901 - val_branch_softmax_2_loss: 1.0457 - val_classification_accuracy: 0.9289 - val_branch_softmax_accuracy: 0.4473 - val_branch_softmax_1_accuracy: 0.5841 - val_branch_softmax_2_accuracy: 0.7610 - val_branch_softmax_evidence: 0.0046 - val_branch_softmax_mean_ev_succ: 0.0776 - val_branch_softmax_mean_ev_fail: 0.0209 - val_branch_softmax_1_evidence: 0.0081 - val_branch_softmax_1_mean_ev_succ: 0.1259 - val_branch_softmax_1_mean_ev_fail: 0.0193 - val_branch_softmax_2_evidence: 0.1044 - val_branch_softmax_2_mean_ev_succ: 1.3244 - val_branch_softmax_2_mean_ev_fail: 0.1580\n",
      "\n",
      "Epoch 00017: saving model to models\\mobileNetv2_evidence_flat_30.hdf5\n",
      "Epoch 18/18\n",
      "1406/1406 [==============================] - 467s 320ms/step - loss: 3.5920 - classification_loss: 2.1662 - branch_softmax_loss: 0.8119 - branch_softmax_1_loss: 0.5265 - branch_softmax_2_loss: 0.0873 - classification_accuracy: 0.9967 - branch_softmax_accuracy: 0.7079 - branch_softmax_1_accuracy: 0.8103 - branch_softmax_2_accuracy: 0.9692 - branch_softmax_evidence: 0.0072 - branch_softmax_mean_ev_succ: 0.0889 - branch_softmax_mean_ev_fail: 0.0306 - branch_softmax_1_evidence: 0.0141 - branch_softmax_1_mean_ev_succ: 0.1689 - branch_softmax_1_mean_ev_fail: 0.0266 - branch_softmax_2_evidence: 0.1195 - branch_softmax_2_mean_ev_succ: 1.2359 - branch_softmax_2_mean_ev_fail: 0.0230 - val_loss: 5.7685 - val_classification_loss: 2.2355 - val_branch_softmax_loss: 1.3524 - val_branch_softmax_1_loss: 1.0625 - val_branch_softmax_2_loss: 1.1181 - val_classification_accuracy: 0.9363 - val_branch_softmax_accuracy: 0.5166 - val_branch_softmax_1_accuracy: 0.6526 - val_branch_softmax_2_accuracy: 0.7600 - val_branch_softmax_evidence: 0.0045 - val_branch_softmax_mean_ev_succ: 0.0632 - val_branch_softmax_mean_ev_fail: 0.0256 - val_branch_softmax_1_evidence: 0.0121 - val_branch_softmax_1_mean_ev_succ: 0.1655 - val_branch_softmax_1_mean_ev_fail: 0.0360 - val_branch_softmax_2_evidence: 0.0818 - val_branch_softmax_2_mean_ev_succ: 1.0555 - val_branch_softmax_2_mean_ev_fail: 0.0609\n",
      "\n",
      "Epoch 00018: saving model to models\\mobileNetv2_evidence_flat_30.hdf5\n",
      "<tensorflow.python.keras.callbacks.History object at 0x00000218A5E52E10>\n",
      "312/312 - 30s - loss: 5.8555 - classification_loss: 2.2355 - branch_softmax_loss: 1.3805 - branch_softmax_1_loss: 1.0826 - branch_softmax_2_loss: 1.1569 - classification_accuracy: 0.9356 - branch_softmax_accuracy: 0.5152 - branch_softmax_1_accuracy: 0.6510 - branch_softmax_2_accuracy: 0.7536 - branch_softmax_evidence: 0.0049 - branch_softmax_mean_ev_succ: 0.0677 - branch_softmax_mean_ev_fail: 0.0285 - branch_softmax_1_evidence: 0.0124 - branch_softmax_1_mean_ev_succ: 0.1704 - branch_softmax_1_mean_ev_fail: 0.0375 - branch_softmax_2_evidence: 0.0833 - branch_softmax_2_mean_ev_succ: 1.0832 - branch_softmax_2_mean_ev_fail: 0.0642\n",
      "overall loss: 5.855493545532227\n"
     ]
    }
   ],
   "source": [
    "brevis.saveName=(\"mobileNetv2_evidence_flat_30\")\n",
    "model = brevis.trainTransfer(18, loss=loss_fn, optimizer=tf.optimizers.SGD(), transfer=False, customOptions=\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "targets already present?  False\n",
      "added targets\n",
      "Matching Branchpoint by id number\n",
      "Matching Branchpoint by name\n",
      "add Branch to branch point  block_2_add\n",
      "add Branch to branch point  block_4_add\n",
      "add Branch to branch point  block_8_add\n",
      "<branchingdnn.core.BranchingDnn.branched_model object at 0x0000021329855E10>\n"
     ]
    }
   ],
   "source": [
    "tf.compat.v1.reset_default_graph() #reset the graph to reset the naming scheme for the new branched model\n",
    "brevis = (branching.core.branched_model(modelName=\"models/mobileNetv2_finetuned.hdf5\",saveName=\"mobileNetv2_entropy_conv2d\",transfer=True,custom_objects=\"\")\n",
    "            .add_branches(branching.branches.branch.branch_conv2d_entropy,branchPoints, target_input=True)\n",
    "            .set_dataset(dataset)\n",
    "            \n",
    "            )\n",
    "# brevis.evaluate(test_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting Main Layers  and branch layers training to true\n",
      "\n",
      "customOption: Other\n",
      "https://app.neptune.ai/cailen01/branchingDNN/e/BRAN-409\n",
      "Remember to stop your run once youve finished logging your metadata (https://docs.neptune.ai/api-reference/run#stop). It will be stopped automatically only when the notebook kernel/interactive console is terminated.\n",
      "Epoch 1/12\n",
      "1406/1406 [==============================] - 428s 291ms/step - loss: 5.0702 - classification_loss: 0.0886 - branch_softmax_loss: 2.2019 - branch_softmax_1_loss: 1.5525 - branch_softmax_2_loss: 1.2272 - classification_accuracy: 0.9690 - branch_softmax_accuracy: 0.2074 - branch_softmax_1_accuracy: 0.5233 - branch_softmax_2_accuracy: 0.5822 - val_loss: 5.6773 - val_classification_loss: 0.3886 - val_branch_softmax_loss: 2.3974 - val_branch_softmax_1_loss: 1.7095 - val_branch_softmax_2_loss: 1.1818 - val_classification_accuracy: 0.8790 - val_branch_softmax_accuracy: 0.2668 - val_branch_softmax_1_accuracy: 0.5292 - val_branch_softmax_2_accuracy: 0.6366\n",
      "\n",
      "Epoch 00001: saving model to models\\mobileNetv2_entropy_conv2d.hdf5\n",
      "Epoch 2/12\n",
      "1406/1406 [==============================] - 425s 290ms/step - loss: 3.2581 - classification_loss: 0.0605 - branch_softmax_loss: 1.8628 - branch_softmax_1_loss: 0.7811 - branch_softmax_2_loss: 0.5537 - classification_accuracy: 0.9802 - branch_softmax_accuracy: 0.3715 - branch_softmax_1_accuracy: 0.7595 - branch_softmax_2_accuracy: 0.8090 - val_loss: 7.9485 - val_classification_loss: 0.7255 - val_branch_softmax_loss: 2.7616 - val_branch_softmax_1_loss: 2.2221 - val_branch_softmax_2_loss: 2.2393 - val_classification_accuracy: 0.7835 - val_branch_softmax_accuracy: 0.3027 - val_branch_softmax_1_accuracy: 0.4389 - val_branch_softmax_2_accuracy: 0.3972\n",
      "\n",
      "Epoch 00002: saving model to models\\mobileNetv2_entropy_conv2d.hdf5\n",
      "Epoch 3/12\n",
      "1406/1406 [==============================] - 429s 292ms/step - loss: 2.4277 - classification_loss: 0.0396 - branch_softmax_loss: 1.4595 - branch_softmax_1_loss: 0.5002 - branch_softmax_2_loss: 0.4283 - classification_accuracy: 0.9874 - branch_softmax_accuracy: 0.5469 - branch_softmax_1_accuracy: 0.8381 - branch_softmax_2_accuracy: 0.8543 - val_loss: 4.0806 - val_classification_loss: 0.2709 - val_branch_softmax_loss: 1.9124 - val_branch_softmax_1_loss: 1.2024 - val_branch_softmax_2_loss: 0.6950 - val_classification_accuracy: 0.9137 - val_branch_softmax_accuracy: 0.5012 - val_branch_softmax_1_accuracy: 0.6478 - val_branch_softmax_2_accuracy: 0.7666\n",
      "\n",
      "Epoch 00003: saving model to models\\mobileNetv2_entropy_conv2d.hdf5\n",
      "Epoch 4/12\n",
      "1406/1406 [==============================] - 431s 293ms/step - loss: 1.8130 - classification_loss: 0.0320 - branch_softmax_loss: 1.0844 - branch_softmax_1_loss: 0.3587 - branch_softmax_2_loss: 0.3380 - classification_accuracy: 0.9893 - branch_softmax_accuracy: 0.6614 - branch_softmax_1_accuracy: 0.8878 - branch_softmax_2_accuracy: 0.8859 - val_loss: 3.4130 - val_classification_loss: 0.2040 - val_branch_softmax_loss: 1.5147 - val_branch_softmax_1_loss: 1.0388 - val_branch_softmax_2_loss: 0.6556 - val_classification_accuracy: 0.9391 - val_branch_softmax_accuracy: 0.6142 - val_branch_softmax_1_accuracy: 0.6977 - val_branch_softmax_2_accuracy: 0.7800\n",
      "\n",
      "Epoch 00004: saving model to models\\mobileNetv2_entropy_conv2d.hdf5\n",
      "Epoch 5/12\n",
      "1406/1406 [==============================] - 432s 294ms/step - loss: 1.3203 - classification_loss: 0.0229 - branch_softmax_loss: 0.7721 - branch_softmax_1_loss: 0.2584 - branch_softmax_2_loss: 0.2669 - classification_accuracy: 0.9928 - branch_softmax_accuracy: 0.7586 - branch_softmax_1_accuracy: 0.9225 - branch_softmax_2_accuracy: 0.9092 - val_loss: 3.5543 - val_classification_loss: 0.2271 - val_branch_softmax_loss: 1.4515 - val_branch_softmax_1_loss: 1.1891 - val_branch_softmax_2_loss: 0.6866 - val_classification_accuracy: 0.9355 - val_branch_softmax_accuracy: 0.6358 - val_branch_softmax_1_accuracy: 0.6675 - val_branch_softmax_2_accuracy: 0.7782\n",
      "\n",
      "Epoch 00005: saving model to models\\mobileNetv2_entropy_conv2d.hdf5\n",
      "Epoch 6/12\n",
      "1406/1406 [==============================] - 434s 295ms/step - loss: 0.9300 - classification_loss: 0.0165 - branch_softmax_loss: 0.5245 - branch_softmax_1_loss: 0.1820 - branch_softmax_2_loss: 0.2071 - classification_accuracy: 0.9948 - branch_softmax_accuracy: 0.8296 - branch_softmax_1_accuracy: 0.9488 - branch_softmax_2_accuracy: 0.9330 - val_loss: 3.4036 - val_classification_loss: 0.2067 - val_branch_softmax_loss: 1.3393 - val_branch_softmax_1_loss: 1.2128 - val_branch_softmax_2_loss: 0.6448 - val_classification_accuracy: 0.9425 - val_branch_softmax_accuracy: 0.6454 - val_branch_softmax_1_accuracy: 0.6823 - val_branch_softmax_2_accuracy: 0.8017\n",
      "\n",
      "Epoch 00006: saving model to models\\mobileNetv2_entropy_conv2d.hdf5\n",
      "Epoch 7/12\n",
      "1406/1406 [==============================] - 437s 298ms/step - loss: 0.6918 - classification_loss: 0.0140 - branch_softmax_loss: 0.3859 - branch_softmax_1_loss: 0.1319 - branch_softmax_2_loss: 0.1600 - classification_accuracy: 0.9962 - branch_softmax_accuracy: 0.8706 - branch_softmax_1_accuracy: 0.9645 - branch_softmax_2_accuracy: 0.9482 - val_loss: 3.1310 - val_classification_loss: 0.1985 - val_branch_softmax_loss: 1.3282 - val_branch_softmax_1_loss: 1.0259 - val_branch_softmax_2_loss: 0.5784 - val_classification_accuracy: 0.9433 - val_branch_softmax_accuracy: 0.6731 - val_branch_softmax_1_accuracy: 0.7328 - val_branch_softmax_2_accuracy: 0.8263\n",
      "\n",
      "Epoch 00007: saving model to models\\mobileNetv2_entropy_conv2d.hdf5\n",
      "Epoch 8/12\n",
      "1406/1406 [==============================] - 439s 299ms/step - loss: 0.5320 - classification_loss: 0.0121 - branch_softmax_loss: 0.2973 - branch_softmax_1_loss: 0.0971 - branch_softmax_2_loss: 0.1256 - classification_accuracy: 0.9965 - branch_softmax_accuracy: 0.8974 - branch_softmax_1_accuracy: 0.9738 - branch_softmax_2_accuracy: 0.9627 - val_loss: 3.5375 - val_classification_loss: 0.2125 - val_branch_softmax_loss: 1.5516 - val_branch_softmax_1_loss: 1.1488 - val_branch_softmax_2_loss: 0.6246 - val_classification_accuracy: 0.9439 - val_branch_softmax_accuracy: 0.6342 - val_branch_softmax_1_accuracy: 0.7119 - val_branch_softmax_2_accuracy: 0.8073\n",
      "\n",
      "Epoch 00008: saving model to models\\mobileNetv2_entropy_conv2d.hdf5\n",
      "Epoch 9/12\n",
      "1406/1406 [==============================] - 443s 301ms/step - loss: 0.4230 - classification_loss: 0.0115 - branch_softmax_loss: 0.2453 - branch_softmax_1_loss: 0.0743 - branch_softmax_2_loss: 0.0920 - classification_accuracy: 0.9965 - branch_softmax_accuracy: 0.9105 - branch_softmax_1_accuracy: 0.9817 - branch_softmax_2_accuracy: 0.9730 - val_loss: 3.6496 - val_classification_loss: 0.2054 - val_branch_softmax_loss: 1.4232 - val_branch_softmax_1_loss: 1.2559 - val_branch_softmax_2_loss: 0.7652 - val_classification_accuracy: 0.9445 - val_branch_softmax_accuracy: 0.6735 - val_branch_softmax_1_accuracy: 0.6999 - val_branch_softmax_2_accuracy: 0.7887\n",
      "\n",
      "Epoch 00009: saving model to models\\mobileNetv2_entropy_conv2d.hdf5\n",
      "Epoch 10/12\n",
      "1406/1406 [==============================] - 447s 304ms/step - loss: 0.3655 - classification_loss: 0.0074 - branch_softmax_loss: 0.2193 - branch_softmax_1_loss: 0.0609 - branch_softmax_2_loss: 0.0779 - classification_accuracy: 0.9979 - branch_softmax_accuracy: 0.9178 - branch_softmax_1_accuracy: 0.9839 - branch_softmax_2_accuracy: 0.9784 - val_loss: 3.0772 - val_classification_loss: 0.1984 - val_branch_softmax_loss: 1.2887 - val_branch_softmax_1_loss: 1.0418 - val_branch_softmax_2_loss: 0.5482 - val_classification_accuracy: 0.9521 - val_branch_softmax_accuracy: 0.6959 - val_branch_softmax_1_accuracy: 0.7534 - val_branch_softmax_2_accuracy: 0.8425\n",
      "\n",
      "Epoch 00010: saving model to models\\mobileNetv2_entropy_conv2d.hdf5\n",
      "Epoch 11/12\n",
      "1406/1406 [==============================] - 449s 305ms/step - loss: 0.3295 - classification_loss: 0.0082 - branch_softmax_loss: 0.2072 - branch_softmax_1_loss: 0.0545 - branch_softmax_2_loss: 0.0595 - classification_accuracy: 0.9979 - branch_softmax_accuracy: 0.9214 - branch_softmax_1_accuracy: 0.9846 - branch_softmax_2_accuracy: 0.9850 - val_loss: 3.1530 - val_classification_loss: 0.1790 - val_branch_softmax_loss: 1.3624 - val_branch_softmax_1_loss: 1.0766 - val_branch_softmax_2_loss: 0.5350 - val_classification_accuracy: 0.9547 - val_branch_softmax_accuracy: 0.6929 - val_branch_softmax_1_accuracy: 0.7526 - val_branch_softmax_2_accuracy: 0.8520\n",
      "\n",
      "Epoch 00011: saving model to models\\mobileNetv2_entropy_conv2d.hdf5\n",
      "Epoch 12/12\n",
      "1406/1406 [==============================] - 451s 307ms/step - loss: 0.2940 - classification_loss: 0.0063 - branch_softmax_loss: 0.1941 - branch_softmax_1_loss: 0.0484 - branch_softmax_2_loss: 0.0452 - classification_accuracy: 0.9984 - branch_softmax_accuracy: 0.9260 - branch_softmax_1_accuracy: 0.9858 - branch_softmax_2_accuracy: 0.9891 - val_loss: 3.1911 - val_classification_loss: 0.1558 - val_branch_softmax_loss: 1.4133 - val_branch_softmax_1_loss: 1.0844 - val_branch_softmax_2_loss: 0.5376 - val_classification_accuracy: 0.9599 - val_branch_softmax_accuracy: 0.6853 - val_branch_softmax_1_accuracy: 0.7486 - val_branch_softmax_2_accuracy: 0.8514\n",
      "\n",
      "Epoch 00012: saving model to models\\mobileNetv2_entropy_conv2d.hdf5\n",
      "<tensorflow.python.keras.callbacks.History object at 0x0000021959286BE0>\n",
      "312/312 - 22s - loss: 3.4081 - classification_loss: 0.1985 - branch_softmax_loss: 1.4660 - branch_softmax_1_loss: 1.1414 - branch_softmax_2_loss: 0.6022 - classification_accuracy: 0.9498 - branch_softmax_accuracy: 0.6828 - branch_softmax_1_accuracy: 0.7340 - branch_softmax_2_accuracy: 0.8436\n",
      "overall loss: 3.4081459045410156\n"
     ]
    }
   ],
   "source": [
    "model = brevis.trainTransfer(12, loss=\"categorical_crossentropy\", optimizer=tf.optimizers.SGD(), transfer=False, customOptions=\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting Main Layers  and branch layers training to true\n",
      "\n",
      "customOption: Other\n",
      "https://app.neptune.ai/cailen01/branchingDNN/e/BRAN-410\n",
      "Remember to stop your run once youve finished logging your metadata (https://docs.neptune.ai/api-reference/run#stop). It will be stopped automatically only when the notebook kernel/interactive console is terminated.\n",
      "Epoch 1/18\n",
      "1406/1406 [==============================] - 456s 309ms/step - loss: 0.2773 - classification_loss: 0.0039 - branch_softmax_loss: 0.1914 - branch_softmax_1_loss: 0.0460 - branch_softmax_2_loss: 0.0360 - classification_accuracy: 0.9990 - branch_softmax_accuracy: 0.9261 - branch_softmax_1_accuracy: 0.9859 - branch_softmax_2_accuracy: 0.9920 - val_loss: 3.2821 - val_classification_loss: 0.1798 - val_branch_softmax_loss: 1.3646 - val_branch_softmax_1_loss: 1.1361 - val_branch_softmax_2_loss: 0.6016 - val_classification_accuracy: 0.9587 - val_branch_softmax_accuracy: 0.6967 - val_branch_softmax_1_accuracy: 0.7472 - val_branch_softmax_2_accuracy: 0.8458\n",
      "\n",
      "Epoch 00001: saving model to models\\mobileNetv2_entropy_conv2d_30.hdf5\n",
      "Epoch 2/18\n",
      "1406/1406 [==============================] - 456s 309ms/step - loss: 0.2552 - classification_loss: 0.0043 - branch_softmax_loss: 0.1819 - branch_softmax_1_loss: 0.0422 - branch_softmax_2_loss: 0.0269 - classification_accuracy: 0.9989 - branch_softmax_accuracy: 0.9287 - branch_softmax_1_accuracy: 0.9867 - branch_softmax_2_accuracy: 0.9949 - val_loss: 3.4134 - val_classification_loss: 0.1859 - val_branch_softmax_loss: 1.4341 - val_branch_softmax_1_loss: 1.1575 - val_branch_softmax_2_loss: 0.6360 - val_classification_accuracy: 0.9533 - val_branch_softmax_accuracy: 0.6967 - val_branch_softmax_1_accuracy: 0.7482 - val_branch_softmax_2_accuracy: 0.8395\n",
      "\n",
      "Epoch 00002: saving model to models\\mobileNetv2_entropy_conv2d_30.hdf5\n",
      "Epoch 3/18\n",
      "1406/1406 [==============================] - 452s 307ms/step - loss: 0.2476 - classification_loss: 0.0040 - branch_softmax_loss: 0.1781 - branch_softmax_1_loss: 0.0396 - branch_softmax_2_loss: 0.0259 - classification_accuracy: 0.9991 - branch_softmax_accuracy: 0.9300 - branch_softmax_1_accuracy: 0.9871 - branch_softmax_2_accuracy: 0.9946 - val_loss: 3.2744 - val_classification_loss: 0.1710 - val_branch_softmax_loss: 1.3982 - val_branch_softmax_1_loss: 1.1223 - val_branch_softmax_2_loss: 0.5830 - val_classification_accuracy: 0.9555 - val_branch_softmax_accuracy: 0.6957 - val_branch_softmax_1_accuracy: 0.7490 - val_branch_softmax_2_accuracy: 0.8498\n",
      "\n",
      "Epoch 00003: saving model to models\\mobileNetv2_entropy_conv2d_30.hdf5\n",
      "Epoch 4/18\n",
      "1406/1406 [==============================] - 454s 308ms/step - loss: 0.2375 - classification_loss: 0.0044 - branch_softmax_loss: 0.1721 - branch_softmax_1_loss: 0.0408 - branch_softmax_2_loss: 0.0203 - classification_accuracy: 0.9986 - branch_softmax_accuracy: 0.9328 - branch_softmax_1_accuracy: 0.9865 - branch_softmax_2_accuracy: 0.9964 - val_loss: 3.3474 - val_classification_loss: 0.1788 - val_branch_softmax_loss: 1.4212 - val_branch_softmax_1_loss: 1.1372 - val_branch_softmax_2_loss: 0.6103 - val_classification_accuracy: 0.9569 - val_branch_softmax_accuracy: 0.6943 - val_branch_softmax_1_accuracy: 0.7562 - val_branch_softmax_2_accuracy: 0.8494\n",
      "\n",
      "Epoch 00004: saving model to models\\mobileNetv2_entropy_conv2d_30.hdf5\n",
      "Epoch 5/18\n",
      "1406/1406 [==============================] - 456s 310ms/step - loss: 0.2294 - classification_loss: 0.0041 - branch_softmax_loss: 0.1687 - branch_softmax_1_loss: 0.0375 - branch_softmax_2_loss: 0.0192 - classification_accuracy: 0.9992 - branch_softmax_accuracy: 0.9336 - branch_softmax_1_accuracy: 0.9877 - branch_softmax_2_accuracy: 0.9962 - val_loss: 3.3554 - val_classification_loss: 0.1806 - val_branch_softmax_loss: 1.4339 - val_branch_softmax_1_loss: 1.1373 - val_branch_softmax_2_loss: 0.6036 - val_classification_accuracy: 0.9555 - val_branch_softmax_accuracy: 0.6981 - val_branch_softmax_1_accuracy: 0.7556 - val_branch_softmax_2_accuracy: 0.8522\n",
      "\n",
      "Epoch 00005: saving model to models\\mobileNetv2_entropy_conv2d_30.hdf5\n",
      "Epoch 6/18\n",
      "1406/1406 [==============================] - 457s 310ms/step - loss: 0.2227 - classification_loss: 0.0027 - branch_softmax_loss: 0.1692 - branch_softmax_1_loss: 0.0360 - branch_softmax_2_loss: 0.0149 - classification_accuracy: 0.9996 - branch_softmax_accuracy: 0.9338 - branch_softmax_1_accuracy: 0.9882 - branch_softmax_2_accuracy: 0.9977 - val_loss: 3.4040 - val_classification_loss: 0.1742 - val_branch_softmax_loss: 1.4378 - val_branch_softmax_1_loss: 1.1602 - val_branch_softmax_2_loss: 0.6317 - val_classification_accuracy: 0.9573 - val_branch_softmax_accuracy: 0.6981 - val_branch_softmax_1_accuracy: 0.7510 - val_branch_softmax_2_accuracy: 0.8423\n",
      "\n",
      "Epoch 00006: saving model to models\\mobileNetv2_entropy_conv2d_30.hdf5\n",
      "Epoch 7/18\n",
      "1406/1406 [==============================] - 459s 312ms/step - loss: 0.2190 - classification_loss: 0.0026 - branch_softmax_loss: 0.1655 - branch_softmax_1_loss: 0.0374 - branch_softmax_2_loss: 0.0135 - classification_accuracy: 0.9995 - branch_softmax_accuracy: 0.9354 - branch_softmax_1_accuracy: 0.9870 - branch_softmax_2_accuracy: 0.9977 - val_loss: 3.4276 - val_classification_loss: 0.1712 - val_branch_softmax_loss: 1.4506 - val_branch_softmax_1_loss: 1.1571 - val_branch_softmax_2_loss: 0.6487 - val_classification_accuracy: 0.9591 - val_branch_softmax_accuracy: 0.7009 - val_branch_softmax_1_accuracy: 0.7578 - val_branch_softmax_2_accuracy: 0.8524\n",
      "\n",
      "Epoch 00007: saving model to models\\mobileNetv2_entropy_conv2d_30.hdf5\n",
      "Epoch 8/18\n",
      "1406/1406 [==============================] - 460s 312ms/step - loss: 0.2134 - classification_loss: 0.0021 - branch_softmax_loss: 0.1640 - branch_softmax_1_loss: 0.0358 - branch_softmax_2_loss: 0.0115 - classification_accuracy: 0.9996 - branch_softmax_accuracy: 0.9354 - branch_softmax_1_accuracy: 0.9874 - branch_softmax_2_accuracy: 0.9983 - val_loss: 3.4348 - val_classification_loss: 0.1755 - val_branch_softmax_loss: 1.4277 - val_branch_softmax_1_loss: 1.1929 - val_branch_softmax_2_loss: 0.6387 - val_classification_accuracy: 0.9609 - val_branch_softmax_accuracy: 0.6971 - val_branch_softmax_1_accuracy: 0.7488 - val_branch_softmax_2_accuracy: 0.8514\n",
      "\n",
      "Epoch 00008: saving model to models\\mobileNetv2_entropy_conv2d_30.hdf5\n",
      "Epoch 9/18\n",
      "1406/1406 [==============================] - 463s 314ms/step - loss: 0.2024 - classification_loss: 0.0020 - branch_softmax_loss: 0.1575 - branch_softmax_1_loss: 0.0331 - branch_softmax_2_loss: 0.0097 - classification_accuracy: 0.9995 - branch_softmax_accuracy: 0.9380 - branch_softmax_1_accuracy: 0.9884 - branch_softmax_2_accuracy: 0.9987 - val_loss: 3.4231 - val_classification_loss: 0.1689 - val_branch_softmax_loss: 1.4763 - val_branch_softmax_1_loss: 1.1762 - val_branch_softmax_2_loss: 0.6017 - val_classification_accuracy: 0.9607 - val_branch_softmax_accuracy: 0.6991 - val_branch_softmax_1_accuracy: 0.7558 - val_branch_softmax_2_accuracy: 0.8542\n",
      "\n",
      "Epoch 00009: saving model to models\\mobileNetv2_entropy_conv2d_30.hdf5\n",
      "Epoch 10/18\n",
      "1406/1406 [==============================] - 463s 314ms/step - loss: 0.2091 - classification_loss: 0.0016 - branch_softmax_loss: 0.1618 - branch_softmax_1_loss: 0.0353 - branch_softmax_2_loss: 0.0104 - classification_accuracy: 0.9997 - branch_softmax_accuracy: 0.9366 - branch_softmax_1_accuracy: 0.9874 - branch_softmax_2_accuracy: 0.9982 - val_loss: 3.5103 - val_classification_loss: 0.1864 - val_branch_softmax_loss: 1.5019 - val_branch_softmax_1_loss: 1.2044 - val_branch_softmax_2_loss: 0.6177 - val_classification_accuracy: 0.9555 - val_branch_softmax_accuracy: 0.6933 - val_branch_softmax_1_accuracy: 0.7482 - val_branch_softmax_2_accuracy: 0.8554\n",
      "\n",
      "Epoch 00010: saving model to models\\mobileNetv2_entropy_conv2d_30.hdf5\n",
      "Epoch 11/18\n",
      "1406/1406 [==============================] - 465s 315ms/step - loss: 0.2003 - classification_loss: 0.0024 - branch_softmax_loss: 0.1556 - branch_softmax_1_loss: 0.0332 - branch_softmax_2_loss: 0.0091 - classification_accuracy: 0.9994 - branch_softmax_accuracy: 0.9386 - branch_softmax_1_accuracy: 0.9882 - branch_softmax_2_accuracy: 0.9986 - val_loss: 3.5115 - val_classification_loss: 0.1851 - val_branch_softmax_loss: 1.4788 - val_branch_softmax_1_loss: 1.2211 - val_branch_softmax_2_loss: 0.6265 - val_classification_accuracy: 0.9573 - val_branch_softmax_accuracy: 0.7001 - val_branch_softmax_1_accuracy: 0.7532 - val_branch_softmax_2_accuracy: 0.8532\n",
      "\n",
      "Epoch 00011: saving model to models\\mobileNetv2_entropy_conv2d_30.hdf5\n",
      "Epoch 12/18\n",
      "1406/1406 [==============================] - 466s 316ms/step - loss: 0.1993 - classification_loss: 0.0019 - branch_softmax_loss: 0.1538 - branch_softmax_1_loss: 0.0346 - branch_softmax_2_loss: 0.0089 - classification_accuracy: 0.9996 - branch_softmax_accuracy: 0.9393 - branch_softmax_1_accuracy: 0.9878 - branch_softmax_2_accuracy: 0.9983 - val_loss: 3.5650 - val_classification_loss: 0.1864 - val_branch_softmax_loss: 1.5372 - val_branch_softmax_1_loss: 1.2084 - val_branch_softmax_2_loss: 0.6330 - val_classification_accuracy: 0.9571 - val_branch_softmax_accuracy: 0.6879 - val_branch_softmax_1_accuracy: 0.7506 - val_branch_softmax_2_accuracy: 0.8536\n",
      "\n",
      "Epoch 00012: saving model to models\\mobileNetv2_entropy_conv2d_30.hdf5\n",
      "Epoch 13/18\n",
      "1406/1406 [==============================] - 468s 317ms/step - loss: 0.1986 - classification_loss: 0.0016 - branch_softmax_loss: 0.1551 - branch_softmax_1_loss: 0.0335 - branch_softmax_2_loss: 0.0084 - classification_accuracy: 0.9997 - branch_softmax_accuracy: 0.9388 - branch_softmax_1_accuracy: 0.9883 - branch_softmax_2_accuracy: 0.9986 - val_loss: 3.5028 - val_classification_loss: 0.1774 - val_branch_softmax_loss: 1.4956 - val_branch_softmax_1_loss: 1.2112 - val_branch_softmax_2_loss: 0.6187 - val_classification_accuracy: 0.9613 - val_branch_softmax_accuracy: 0.6985 - val_branch_softmax_1_accuracy: 0.7590 - val_branch_softmax_2_accuracy: 0.8532\n",
      "\n",
      "Epoch 00013: saving model to models\\mobileNetv2_entropy_conv2d_30.hdf5\n",
      "Epoch 14/18\n",
      "1406/1406 [==============================] - 469s 318ms/step - loss: 0.1926 - classification_loss: 0.0018 - branch_softmax_loss: 0.1500 - branch_softmax_1_loss: 0.0331 - branch_softmax_2_loss: 0.0077 - classification_accuracy: 0.9996 - branch_softmax_accuracy: 0.9408 - branch_softmax_1_accuracy: 0.9882 - branch_softmax_2_accuracy: 0.9987 - val_loss: 3.5206 - val_classification_loss: 0.1797 - val_branch_softmax_loss: 1.4696 - val_branch_softmax_1_loss: 1.2299 - val_branch_softmax_2_loss: 0.6413 - val_classification_accuracy: 0.9577 - val_branch_softmax_accuracy: 0.7013 - val_branch_softmax_1_accuracy: 0.7570 - val_branch_softmax_2_accuracy: 0.8572\n",
      "\n",
      "Epoch 00014: saving model to models\\mobileNetv2_entropy_conv2d_30.hdf5\n",
      "Epoch 15/18\n",
      "1406/1406 [==============================] - 472s 320ms/step - loss: 0.1929 - classification_loss: 0.0019 - branch_softmax_loss: 0.1525 - branch_softmax_1_loss: 0.0321 - branch_softmax_2_loss: 0.0063 - classification_accuracy: 0.9995 - branch_softmax_accuracy: 0.9394 - branch_softmax_1_accuracy: 0.9885 - branch_softmax_2_accuracy: 0.9992 - val_loss: 3.5609 - val_classification_loss: 0.1831 - val_branch_softmax_loss: 1.5154 - val_branch_softmax_1_loss: 1.2117 - val_branch_softmax_2_loss: 0.6506 - val_classification_accuracy: 0.9571 - val_branch_softmax_accuracy: 0.6995 - val_branch_softmax_1_accuracy: 0.7612 - val_branch_softmax_2_accuracy: 0.8576\n",
      "\n",
      "Epoch 00015: saving model to models\\mobileNetv2_entropy_conv2d_30.hdf5\n",
      "Epoch 16/18\n",
      "1406/1406 [==============================] - 472s 320ms/step - loss: 0.1905 - classification_loss: 0.0015 - branch_softmax_loss: 0.1490 - branch_softmax_1_loss: 0.0331 - branch_softmax_2_loss: 0.0068 - classification_accuracy: 0.9997 - branch_softmax_accuracy: 0.9405 - branch_softmax_1_accuracy: 0.9878 - branch_softmax_2_accuracy: 0.9990 - val_loss: 3.5773 - val_classification_loss: 0.1682 - val_branch_softmax_loss: 1.5288 - val_branch_softmax_1_loss: 1.2351 - val_branch_softmax_2_loss: 0.6451 - val_classification_accuracy: 0.9597 - val_branch_softmax_accuracy: 0.7027 - val_branch_softmax_1_accuracy: 0.7534 - val_branch_softmax_2_accuracy: 0.8574\n",
      "\n",
      "Epoch 00016: saving model to models\\mobileNetv2_entropy_conv2d_30.hdf5\n",
      "Epoch 17/18\n",
      "1406/1406 [==============================] - 472s 319ms/step - loss: 0.1872 - classification_loss: 0.0013 - branch_softmax_loss: 0.1476 - branch_softmax_1_loss: 0.0325 - branch_softmax_2_loss: 0.0058 - classification_accuracy: 0.9997 - branch_softmax_accuracy: 0.9417 - branch_softmax_1_accuracy: 0.9884 - branch_softmax_2_accuracy: 0.9990 - val_loss: 3.6047 - val_classification_loss: 0.1923 - val_branch_softmax_loss: 1.5197 - val_branch_softmax_1_loss: 1.2303 - val_branch_softmax_2_loss: 0.6624 - val_classification_accuracy: 0.9555 - val_branch_softmax_accuracy: 0.6977 - val_branch_softmax_1_accuracy: 0.7608 - val_branch_softmax_2_accuracy: 0.8582\n",
      "\n",
      "Epoch 00017: saving model to models\\mobileNetv2_entropy_conv2d_30.hdf5\n",
      "Epoch 18/18\n",
      "1406/1406 [==============================] - 474s 321ms/step - loss: 0.1904 - classification_loss: 0.0019 - branch_softmax_loss: 0.1504 - branch_softmax_1_loss: 0.0322 - branch_softmax_2_loss: 0.0059 - classification_accuracy: 0.9996 - branch_softmax_accuracy: 0.9404 - branch_softmax_1_accuracy: 0.9881 - branch_softmax_2_accuracy: 0.9991 - val_loss: 3.5804 - val_classification_loss: 0.1750 - val_branch_softmax_loss: 1.5379 - val_branch_softmax_1_loss: 1.2137 - val_branch_softmax_2_loss: 0.6537 - val_classification_accuracy: 0.9613 - val_branch_softmax_accuracy: 0.6983 - val_branch_softmax_1_accuracy: 0.7600 - val_branch_softmax_2_accuracy: 0.8574\n",
      "\n",
      "Epoch 00018: saving model to models\\mobileNetv2_entropy_conv2d_30.hdf5\n",
      "<tensorflow.python.keras.callbacks.History object at 0x00000219D30D03C8>\n",
      "312/312 - 22s - loss: 3.8150 - classification_loss: 0.2225 - branch_softmax_loss: 1.6262 - branch_softmax_1_loss: 1.2726 - branch_softmax_2_loss: 0.6937 - classification_accuracy: 0.9539 - branch_softmax_accuracy: 0.6916 - branch_softmax_1_accuracy: 0.7468 - branch_softmax_2_accuracy: 0.8567\n",
      "overall loss: 3.8149566650390625\n"
     ]
    }
   ],
   "source": [
    "brevis.saveName=(\"mobileNetv2_entropy_conv2d_30\")\n",
    "model = brevis.trainTransfer(18, loss=\"categorical_crossentropy\", optimizer=tf.optimizers.SGD(), transfer=False, customOptions=\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "targets already present?  False\n",
      "added targets\n",
      "Matching Branchpoint by id number\n",
      "Matching Branchpoint by name\n",
      "add Branch to branch point  block_2_add\n",
      "TensorShape([None, 56, 56, 24])\n",
      "TensorShape([None, 54, 54, 32])\n",
      "TensorShape([None, 52, 52, 32])\n",
      "TensorShape([None, 86528])\n",
      "inputShape TensorShape([None, 86528])\n",
      "add Branch to branch point  block_4_add\n",
      "TensorShape([None, 28, 28, 32])\n",
      "TensorShape([None, 26, 26, 32])\n",
      "TensorShape([None, 24, 24, 32])\n",
      "TensorShape([None, 18432])\n",
      "inputShape TensorShape([None, 18432])\n",
      "add Branch to branch point  block_8_add\n",
      "TensorShape([None, 14, 14, 64])\n",
      "TensorShape([None, 12, 12, 32])\n",
      "TensorShape([None, 10, 10, 32])\n",
      "TensorShape([None, 3200])\n",
      "inputShape TensorShape([None, 3200])\n",
      "<branchingdnn.core.BranchingDnn.branched_model object at 0x000002132319F7B8>\n"
     ]
    }
   ],
   "source": [
    "loss_fn = loss_function()\n",
    "tf.compat.v1.reset_default_graph()\n",
    "brevis = (branching.core.branched_model(modelName=\"models/mobileNetv2_finetuned.hdf5\",saveName=\"mobileNetv2_evidence_conv2d\",transfer=True,custom_objects=\"\")\n",
    "            .add_branches(branching.branches.branch.branch_conv2d_inception_evidence,branchPoints, target_input=True)\n",
    "            .set_dataset(dataset)\n",
    "            \n",
    "            )\n",
    "# brevis.evaluate(test_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_branched\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 224, 224, 3) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "Conv1 (Conv2D)                  (None, 112, 112, 32) 864         input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "bn_Conv1 (BatchNormalization)   (None, 112, 112, 32) 128         Conv1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "Conv1_relu (ReLU)               (None, 112, 112, 32) 0           bn_Conv1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "expanded_conv_depthwise (Depthw (None, 112, 112, 32) 288         Conv1_relu[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "expanded_conv_depthwise_BN (Bat (None, 112, 112, 32) 128         expanded_conv_depthwise[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "expanded_conv_depthwise_relu (R (None, 112, 112, 32) 0           expanded_conv_depthwise_BN[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "expanded_conv_project (Conv2D)  (None, 112, 112, 16) 512         expanded_conv_depthwise_relu[0][0\n",
      "__________________________________________________________________________________________________\n",
      "expanded_conv_project_BN (Batch (None, 112, 112, 16) 64          expanded_conv_project[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "block_1_expand (Conv2D)         (None, 112, 112, 96) 1536        expanded_conv_project_BN[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "block_1_expand_BN (BatchNormali (None, 112, 112, 96) 384         block_1_expand[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "block_1_expand_relu (ReLU)      (None, 112, 112, 96) 0           block_1_expand_BN[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "block_1_pad (ZeroPadding2D)     (None, 113, 113, 96) 0           block_1_expand_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block_1_depthwise (DepthwiseCon (None, 56, 56, 96)   864         block_1_pad[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "block_1_depthwise_BN (BatchNorm (None, 56, 56, 96)   384         block_1_depthwise[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "block_1_depthwise_relu (ReLU)   (None, 56, 56, 96)   0           block_1_depthwise_BN[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "block_1_project (Conv2D)        (None, 56, 56, 24)   2304        block_1_depthwise_relu[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "block_1_project_BN (BatchNormal (None, 56, 56, 24)   96          block_1_project[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block_2_expand (Conv2D)         (None, 56, 56, 144)  3456        block_1_project_BN[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block_2_expand_BN (BatchNormali (None, 56, 56, 144)  576         block_2_expand[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "block_2_expand_relu (ReLU)      (None, 56, 56, 144)  0           block_2_expand_BN[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "block_2_depthwise (DepthwiseCon (None, 56, 56, 144)  1296        block_2_expand_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block_2_depthwise_BN (BatchNorm (None, 56, 56, 144)  576         block_2_depthwise[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "block_2_depthwise_relu (ReLU)   (None, 56, 56, 144)  0           block_2_depthwise_BN[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "block_2_project (Conv2D)        (None, 56, 56, 24)   3456        block_2_depthwise_relu[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "block_2_project_BN (BatchNormal (None, 56, 56, 24)   96          block_2_project[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block_2_add (Add)               (None, 56, 56, 24)   0           block_1_project_BN[0][0]         \n",
      "                                                                 block_2_project_BN[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block_3_expand (Conv2D)         (None, 56, 56, 144)  3456        block_2_add[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "block_3_expand_BN (BatchNormali (None, 56, 56, 144)  576         block_3_expand[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "block_3_expand_relu (ReLU)      (None, 56, 56, 144)  0           block_3_expand_BN[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "block_3_pad (ZeroPadding2D)     (None, 57, 57, 144)  0           block_3_expand_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block_3_depthwise (DepthwiseCon (None, 28, 28, 144)  1296        block_3_pad[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "block_3_depthwise_BN (BatchNorm (None, 28, 28, 144)  576         block_3_depthwise[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "block_3_depthwise_relu (ReLU)   (None, 28, 28, 144)  0           block_3_depthwise_BN[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "block_3_project (Conv2D)        (None, 28, 28, 32)   4608        block_3_depthwise_relu[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "block_3_project_BN (BatchNormal (None, 28, 28, 32)   128         block_3_project[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block_4_expand (Conv2D)         (None, 28, 28, 192)  6144        block_3_project_BN[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block_4_expand_BN (BatchNormali (None, 28, 28, 192)  768         block_4_expand[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "block_4_expand_relu (ReLU)      (None, 28, 28, 192)  0           block_4_expand_BN[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "block_4_depthwise (DepthwiseCon (None, 28, 28, 192)  1728        block_4_expand_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block_4_depthwise_BN (BatchNorm (None, 28, 28, 192)  768         block_4_depthwise[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "block_4_depthwise_relu (ReLU)   (None, 28, 28, 192)  0           block_4_depthwise_BN[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "block_4_project (Conv2D)        (None, 28, 28, 32)   6144        block_4_depthwise_relu[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "block_4_project_BN (BatchNormal (None, 28, 28, 32)   128         block_4_project[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block_4_add (Add)               (None, 28, 28, 32)   0           block_3_project_BN[0][0]         \n",
      "                                                                 block_4_project_BN[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block_5_expand (Conv2D)         (None, 28, 28, 192)  6144        block_4_add[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "block_5_expand_BN (BatchNormali (None, 28, 28, 192)  768         block_5_expand[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "block_5_expand_relu (ReLU)      (None, 28, 28, 192)  0           block_5_expand_BN[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "block_5_depthwise (DepthwiseCon (None, 28, 28, 192)  1728        block_5_expand_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block_5_depthwise_BN (BatchNorm (None, 28, 28, 192)  768         block_5_depthwise[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "block_5_depthwise_relu (ReLU)   (None, 28, 28, 192)  0           block_5_depthwise_BN[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "block_5_project (Conv2D)        (None, 28, 28, 32)   6144        block_5_depthwise_relu[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "block_5_project_BN (BatchNormal (None, 28, 28, 32)   128         block_5_project[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block_5_add (Add)               (None, 28, 28, 32)   0           block_4_add[0][0]                \n",
      "                                                                 block_5_project_BN[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block_6_expand (Conv2D)         (None, 28, 28, 192)  6144        block_5_add[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "block_6_expand_BN (BatchNormali (None, 28, 28, 192)  768         block_6_expand[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "block_6_expand_relu (ReLU)      (None, 28, 28, 192)  0           block_6_expand_BN[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "block_6_pad (ZeroPadding2D)     (None, 29, 29, 192)  0           block_6_expand_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block_6_depthwise (DepthwiseCon (None, 14, 14, 192)  1728        block_6_pad[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "block_6_depthwise_BN (BatchNorm (None, 14, 14, 192)  768         block_6_depthwise[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "block_6_depthwise_relu (ReLU)   (None, 14, 14, 192)  0           block_6_depthwise_BN[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "block_6_project (Conv2D)        (None, 14, 14, 64)   12288       block_6_depthwise_relu[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "block_6_project_BN (BatchNormal (None, 14, 14, 64)   256         block_6_project[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block_7_expand (Conv2D)         (None, 14, 14, 384)  24576       block_6_project_BN[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block_7_expand_BN (BatchNormali (None, 14, 14, 384)  1536        block_7_expand[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "block_7_expand_relu (ReLU)      (None, 14, 14, 384)  0           block_7_expand_BN[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "block_7_depthwise (DepthwiseCon (None, 14, 14, 384)  3456        block_7_expand_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block_7_depthwise_BN (BatchNorm (None, 14, 14, 384)  1536        block_7_depthwise[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "block_7_depthwise_relu (ReLU)   (None, 14, 14, 384)  0           block_7_depthwise_BN[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "block_7_project (Conv2D)        (None, 14, 14, 64)   24576       block_7_depthwise_relu[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "block_7_project_BN (BatchNormal (None, 14, 14, 64)   256         block_7_project[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block_7_add (Add)               (None, 14, 14, 64)   0           block_6_project_BN[0][0]         \n",
      "                                                                 block_7_project_BN[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block_8_expand (Conv2D)         (None, 14, 14, 384)  24576       block_7_add[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "block_8_expand_BN (BatchNormali (None, 14, 14, 384)  1536        block_8_expand[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "block_8_expand_relu (ReLU)      (None, 14, 14, 384)  0           block_8_expand_BN[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "block_8_depthwise (DepthwiseCon (None, 14, 14, 384)  3456        block_8_expand_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block_8_depthwise_BN (BatchNorm (None, 14, 14, 384)  1536        block_8_depthwise[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "block_8_depthwise_relu (ReLU)   (None, 14, 14, 384)  0           block_8_depthwise_BN[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "block_8_project (Conv2D)        (None, 14, 14, 64)   24576       block_8_depthwise_relu[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "block_8_project_BN (BatchNormal (None, 14, 14, 64)   256         block_8_project[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block_8_add (Add)               (None, 14, 14, 64)   0           block_7_add[0][0]                \n",
      "                                                                 block_8_project_BN[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block_9_expand (Conv2D)         (None, 14, 14, 384)  24576       block_8_add[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "block_9_expand_BN (BatchNormali (None, 14, 14, 384)  1536        block_9_expand[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "block_9_expand_relu (ReLU)      (None, 14, 14, 384)  0           block_9_expand_BN[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "block_9_depthwise (DepthwiseCon (None, 14, 14, 384)  3456        block_9_expand_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block_9_depthwise_BN (BatchNorm (None, 14, 14, 384)  1536        block_9_depthwise[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "block_9_depthwise_relu (ReLU)   (None, 14, 14, 384)  0           block_9_depthwise_BN[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "block_9_project (Conv2D)        (None, 14, 14, 64)   24576       block_9_depthwise_relu[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "block_9_project_BN (BatchNormal (None, 14, 14, 64)   256         block_9_project[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block_9_add (Add)               (None, 14, 14, 64)   0           block_8_add[0][0]                \n",
      "                                                                 block_9_project_BN[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block_10_expand (Conv2D)        (None, 14, 14, 384)  24576       block_9_add[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "block_10_expand_BN (BatchNormal (None, 14, 14, 384)  1536        block_10_expand[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block_10_expand_relu (ReLU)     (None, 14, 14, 384)  0           block_10_expand_BN[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block_10_depthwise (DepthwiseCo (None, 14, 14, 384)  3456        block_10_expand_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "block_10_depthwise_BN (BatchNor (None, 14, 14, 384)  1536        block_10_depthwise[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block_10_depthwise_relu (ReLU)  (None, 14, 14, 384)  0           block_10_depthwise_BN[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "block_10_project (Conv2D)       (None, 14, 14, 96)   36864       block_10_depthwise_relu[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "block_10_project_BN (BatchNorma (None, 14, 14, 96)   384         block_10_project[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "block_11_expand (Conv2D)        (None, 14, 14, 576)  55296       block_10_project_BN[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block_11_expand_BN (BatchNormal (None, 14, 14, 576)  2304        block_11_expand[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block_11_expand_relu (ReLU)     (None, 14, 14, 576)  0           block_11_expand_BN[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block_11_depthwise (DepthwiseCo (None, 14, 14, 576)  5184        block_11_expand_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "block_11_depthwise_BN (BatchNor (None, 14, 14, 576)  2304        block_11_depthwise[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block_11_depthwise_relu (ReLU)  (None, 14, 14, 576)  0           block_11_depthwise_BN[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "block_11_project (Conv2D)       (None, 14, 14, 96)   55296       block_11_depthwise_relu[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "block_11_project_BN (BatchNorma (None, 14, 14, 96)   384         block_11_project[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "block_11_add (Add)              (None, 14, 14, 96)   0           block_10_project_BN[0][0]        \n",
      "                                                                 block_11_project_BN[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block_12_expand (Conv2D)        (None, 14, 14, 576)  55296       block_11_add[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "block_12_expand_BN (BatchNormal (None, 14, 14, 576)  2304        block_12_expand[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block_12_expand_relu (ReLU)     (None, 14, 14, 576)  0           block_12_expand_BN[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block_12_depthwise (DepthwiseCo (None, 14, 14, 576)  5184        block_12_expand_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "block_12_depthwise_BN (BatchNor (None, 14, 14, 576)  2304        block_12_depthwise[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block_12_depthwise_relu (ReLU)  (None, 14, 14, 576)  0           block_12_depthwise_BN[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "block_12_project (Conv2D)       (None, 14, 14, 96)   55296       block_12_depthwise_relu[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "block_12_project_BN (BatchNorma (None, 14, 14, 96)   384         block_12_project[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "block_12_add (Add)              (None, 14, 14, 96)   0           block_11_add[0][0]               \n",
      "                                                                 block_12_project_BN[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block_13_expand (Conv2D)        (None, 14, 14, 576)  55296       block_12_add[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "block_13_expand_BN (BatchNormal (None, 14, 14, 576)  2304        block_13_expand[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block_13_expand_relu (ReLU)     (None, 14, 14, 576)  0           block_13_expand_BN[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block_13_pad (ZeroPadding2D)    (None, 15, 15, 576)  0           block_13_expand_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "block_13_depthwise (DepthwiseCo (None, 7, 7, 576)    5184        block_13_pad[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "block_13_depthwise_BN (BatchNor (None, 7, 7, 576)    2304        block_13_depthwise[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block_13_depthwise_relu (ReLU)  (None, 7, 7, 576)    0           block_13_depthwise_BN[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "block_13_project (Conv2D)       (None, 7, 7, 160)    92160       block_13_depthwise_relu[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "block_13_project_BN (BatchNorma (None, 7, 7, 160)    640         block_13_project[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "block_14_expand (Conv2D)        (None, 7, 7, 960)    153600      block_13_project_BN[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block_14_expand_BN (BatchNormal (None, 7, 7, 960)    3840        block_14_expand[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block_14_expand_relu (ReLU)     (None, 7, 7, 960)    0           block_14_expand_BN[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block_14_depthwise (DepthwiseCo (None, 7, 7, 960)    8640        block_14_expand_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "block_14_depthwise_BN (BatchNor (None, 7, 7, 960)    3840        block_14_depthwise[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block_14_depthwise_relu (ReLU)  (None, 7, 7, 960)    0           block_14_depthwise_BN[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "block_14_project (Conv2D)       (None, 7, 7, 160)    153600      block_14_depthwise_relu[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "block_14_project_BN (BatchNorma (None, 7, 7, 160)    640         block_14_project[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "block_14_add (Add)              (None, 7, 7, 160)    0           block_13_project_BN[0][0]        \n",
      "                                                                 block_14_project_BN[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block_15_expand (Conv2D)        (None, 7, 7, 960)    153600      block_14_add[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "block_15_expand_BN (BatchNormal (None, 7, 7, 960)    3840        block_15_expand[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block_15_expand_relu (ReLU)     (None, 7, 7, 960)    0           block_15_expand_BN[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block_15_depthwise (DepthwiseCo (None, 7, 7, 960)    8640        block_15_expand_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "block_15_depthwise_BN (BatchNor (None, 7, 7, 960)    3840        block_15_depthwise[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block_15_depthwise_relu (ReLU)  (None, 7, 7, 960)    0           block_15_depthwise_BN[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "block_15_project (Conv2D)       (None, 7, 7, 160)    153600      block_15_depthwise_relu[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "block_15_project_BN (BatchNorma (None, 7, 7, 160)    640         block_15_project[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "block_15_add (Add)              (None, 7, 7, 160)    0           block_14_add[0][0]               \n",
      "                                                                 block_15_project_BN[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block_16_expand (Conv2D)        (None, 7, 7, 960)    153600      block_15_add[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "block_16_expand_BN (BatchNormal (None, 7, 7, 960)    3840        block_16_expand[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block_16_expand_relu (ReLU)     (None, 7, 7, 960)    0           block_16_expand_BN[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block_16_depthwise (DepthwiseCo (None, 7, 7, 960)    8640        block_16_expand_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "block_16_depthwise_BN (BatchNor (None, 7, 7, 960)    3840        block_16_depthwise[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block_16_depthwise_relu (ReLU)  (None, 7, 7, 960)    0           block_16_depthwise_BN[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "block_16_project (Conv2D)       (None, 7, 7, 320)    307200      block_16_depthwise_relu[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "block_16_project_BN (BatchNorma (None, 7, 7, 320)    1280        block_16_project[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "branch_conv2d (Conv2D)          (None, 54, 54, 32)   6944        block_2_add[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "branch_conv2d_2 (Conv2D)        (None, 26, 26, 32)   9248        block_4_add[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "branch_conv2d_4 (Conv2D)        (None, 12, 12, 32)   18464       block_8_add[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "Conv_1 (Conv2D)                 (None, 7, 7, 1280)   409600      block_16_project_BN[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "branch_batchnorm (BatchNormaliz (None, 54, 54, 32)   128         branch_conv2d[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "branch_batchnorm_2 (BatchNormal (None, 26, 26, 32)   128         branch_conv2d_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "branch_batchnorm_4 (BatchNormal (None, 12, 12, 32)   128         branch_conv2d_4[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "Conv_1_bn (BatchNormalization)  (None, 7, 7, 1280)   5120        Conv_1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "branch_conv2d_1 (Conv2D)        (None, 52, 52, 32)   9248        branch_batchnorm[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "branch_conv2d_3 (Conv2D)        (None, 24, 24, 32)   9248        branch_batchnorm_2[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "branch_conv2d_5 (Conv2D)        (None, 10, 10, 32)   9248        branch_batchnorm_4[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "out_relu (ReLU)                 (None, 7, 7, 1280)   0           Conv_1_bn[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "branch_batchnorm_1 (BatchNormal (None, 52, 52, 32)   128         branch_conv2d_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "branch_batchnorm_3 (BatchNormal (None, 24, 24, 32)   128         branch_conv2d_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "branch_batchnorm_5 (BatchNormal (None, 10, 10, 32)   128         branch_conv2d_5[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling2d (Globa (None, 1280)         0           out_relu[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "branch_flatten (Flatten)        (None, 86528)        0           branch_batchnorm_1[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "targets (InputLayer)            [(None, 10)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "branch_flatten_1 (Flatten)      (None, 18432)        0           branch_batchnorm_3[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "branch_flatten_2 (Flatten)      (None, 3200)         0           branch_batchnorm_5[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "classification (Dense)          (None, 10)           12810       global_average_pooling2d[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "branch_softmax (CrossEntropyEnd (None, 10)           865280      branch_flatten[0][0]             \n",
      "                                                                 targets[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "branch_softmax_1 (CrossEntropyE (None, 10)           184320      branch_flatten_1[0][0]           \n",
      "                                                                 targets[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "branch_softmax_2 (CrossEntropyE (None, 10)           32000       branch_flatten_2[0][0]           \n",
      "                                                                 targets[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 3,415,562\n",
      "Trainable params: 3,381,066\n",
      "Non-trainable params: 34,496\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "brevis.model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting Main Layers  and branch layers training to true\n",
      "\n",
      "customOption: Other\n",
      "https://app.neptune.ai/cailen01/branchingDNN/e/BRAN-411\n",
      "Remember to stop your run once youve finished logging your metadata (https://docs.neptune.ai/api-reference/run#stop). It will be stopped automatically only when the notebook kernel/interactive console is terminated.\n",
      "Epoch 1/12\n",
      "1406/1406 [==============================] - ETA: 0s - loss: 7.9523 - classification_loss: 2.2225 - branch_softmax_loss: 2.7894 - branch_softmax_1_loss: 1.6485 - branch_softmax_2_loss: 1.2919 - classification_accuracy: 0.9588 - branch_softmax_accuracy: 0.4855 - branch_softmax_1_accuracy: 0.5406 - branch_softmax_2_accuracy: 0.5867 - branch_softmax_evidence: 0.0284 - branch_softmax_mean_ev_succ: 0.4075 - branch_softmax_mean_ev_fail: 0.1427 - branch_softmax_1_evidence: 0.0317 - branch_softmax_1_mean_ev_succ: 0.4360 - branch_softmax_1_mean_ev_fail: 0.1362 - branch_softmax_2_evidence: 0.0353 - branch_softmax_2_mean_ev_succ: 0.4337 - branch_softmax_2_mean_ev_fail: 0.1782"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: string series 'monitoring/stdout' value was longer than 1000 characters and was truncated. This warning is printed only once per series.\n",
      "Warning: string series 'monitoring/stdout' value was longer than 1000 characters and was truncated. This warning is printed only once per series.\n",
      "Warning: string series 'monitoring/stdout' value was longer than 1000 characters and was truncated. This warning is printed only once per series.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1406/1406 [==============================] - 549s 373ms/step - loss: 7.9511 - classification_loss: 2.2224 - branch_softmax_loss: 2.7887 - branch_softmax_1_loss: 1.6483 - branch_softmax_2_loss: 1.2916 - classification_accuracy: 0.9588 - branch_softmax_accuracy: 0.4856 - branch_softmax_1_accuracy: 0.5407 - branch_softmax_2_accuracy: 0.5868 - branch_softmax_evidence: 0.0284 - branch_softmax_mean_ev_succ: 0.4075 - branch_softmax_mean_ev_fail: 0.1427 - branch_softmax_1_evidence: 0.0317 - branch_softmax_1_mean_ev_succ: 0.4360 - branch_softmax_1_mean_ev_fail: 0.1362 - branch_softmax_2_evidence: 0.0353 - branch_softmax_2_mean_ev_succ: 0.4337 - branch_softmax_2_mean_ev_fail: 0.1782 - val_loss: 8.7184 - val_classification_loss: 2.3282 - val_branch_softmax_loss: 2.0600 - val_branch_softmax_1_loss: 2.7810 - val_branch_softmax_2_loss: 1.5493 - val_classification_accuracy: 0.8634 - val_branch_softmax_accuracy: 0.4333 - val_branch_softmax_1_accuracy: 0.4461 - val_branch_softmax_2_accuracy: 0.5501 - val_branch_softmax_evidence: 0.0569 - val_branch_softmax_mean_ev_succ: 0.8114 - val_branch_softmax_mean_ev_fail: 0.3842 - val_branch_softmax_1_evidence: 0.0671 - val_branch_softmax_1_mean_ev_succ: 1.3060 - val_branch_softmax_1_mean_ev_fail: 0.1588 - val_branch_softmax_2_evidence: 0.0308 - val_branch_softmax_2_mean_ev_succ: 0.5068 - val_branch_softmax_2_mean_ev_fail: 0.0649\n",
      "\n",
      "Epoch 00001: saving model to models\\mobileNetv2_evidence_conv2d.hdf5\n",
      "Epoch 2/12\n",
      "1406/1406 [==============================] - 544s 371ms/step - loss: 3.8918 - classification_loss: 2.1966 - branch_softmax_loss: 0.5262 - branch_softmax_1_loss: 0.5996 - branch_softmax_2_loss: 0.5694 - classification_accuracy: 0.9766 - branch_softmax_accuracy: 0.8251 - branch_softmax_1_accuracy: 0.7923 - branch_softmax_2_accuracy: 0.8035 - branch_softmax_evidence: 0.0220 - branch_softmax_mean_ev_succ: 0.2583 - branch_softmax_mean_ev_fail: 0.0441 - branch_softmax_1_evidence: 0.0277 - branch_softmax_1_mean_ev_succ: 0.3373 - branch_softmax_1_mean_ev_fail: 0.0503 - branch_softmax_2_evidence: 0.0343 - branch_softmax_2_mean_ev_succ: 0.4123 - branch_softmax_2_mean_ev_fail: 0.0597 - val_loss: 5.5108 - val_classification_loss: 2.2411 - val_branch_softmax_loss: 1.4230 - val_branch_softmax_1_loss: 1.1004 - val_branch_softmax_2_loss: 0.7463 - val_classification_accuracy: 0.9373 - val_branch_softmax_accuracy: 0.5950 - val_branch_softmax_1_accuracy: 0.6536 - val_branch_softmax_2_accuracy: 0.7468 - val_branch_softmax_evidence: 0.0588 - val_branch_softmax_mean_ev_succ: 0.7960 - val_branch_softmax_mean_ev_fail: 0.2851 - val_branch_softmax_1_evidence: 0.0325 - val_branch_softmax_1_mean_ev_succ: 0.4381 - val_branch_softmax_1_mean_ev_fail: 0.1103 - val_branch_softmax_2_evidence: 0.0283 - val_branch_softmax_2_mean_ev_succ: 0.3576 - val_branch_softmax_2_mean_ev_fail: 0.0579\n",
      "\n",
      "Epoch 00002: saving model to models\\mobileNetv2_evidence_conv2d.hdf5\n",
      "Epoch 3/12\n",
      "1406/1406 [==============================] - 541s 368ms/step - loss: 3.2531 - classification_loss: 2.1876 - branch_softmax_loss: 0.2444 - branch_softmax_1_loss: 0.3956 - branch_softmax_2_loss: 0.4255 - classification_accuracy: 0.9830 - branch_softmax_accuracy: 0.9303 - branch_softmax_1_accuracy: 0.8675 - branch_softmax_2_accuracy: 0.8533 - branch_softmax_evidence: 0.0287 - branch_softmax_mean_ev_succ: 0.3094 - branch_softmax_mean_ev_fail: 0.0235 - branch_softmax_1_evidence: 0.0354 - branch_softmax_1_mean_ev_succ: 0.4035 - branch_softmax_1_mean_ev_fail: 0.0464 - branch_softmax_2_evidence: 0.0442 - branch_softmax_2_mean_ev_succ: 0.5104 - branch_softmax_2_mean_ev_fail: 0.0554 - val_loss: 5.2768 - val_classification_loss: 2.2302 - val_branch_softmax_loss: 1.2997 - val_branch_softmax_1_loss: 0.9526 - val_branch_softmax_2_loss: 0.7943 - val_classification_accuracy: 0.9451 - val_branch_softmax_accuracy: 0.6428 - val_branch_softmax_1_accuracy: 0.7001 - val_branch_softmax_2_accuracy: 0.7486 - val_branch_softmax_evidence: 0.0314 - val_branch_softmax_mean_ev_succ: 0.4324 - val_branch_softmax_mean_ev_fail: 0.1020 - val_branch_softmax_1_evidence: 0.0425 - val_branch_softmax_1_mean_ev_succ: 0.5425 - val_branch_softmax_1_mean_ev_fail: 0.1468 - val_branch_softmax_2_evidence: 0.0456 - val_branch_softmax_2_mean_ev_succ: 0.5698 - val_branch_softmax_2_mean_ev_fail: 0.1168\n",
      "\n",
      "Epoch 00003: saving model to models\\mobileNetv2_evidence_conv2d.hdf5\n",
      "Epoch 4/12\n",
      "1406/1406 [==============================] - 543s 370ms/step - loss: 2.8630 - classification_loss: 2.1822 - branch_softmax_loss: 0.0816 - branch_softmax_1_loss: 0.2627 - branch_softmax_2_loss: 0.3363 - classification_accuracy: 0.9870 - branch_softmax_accuracy: 0.9826 - branch_softmax_1_accuracy: 0.9164 - branch_softmax_2_accuracy: 0.8847 - branch_softmax_evidence: 0.0369 - branch_softmax_mean_ev_succ: 0.3751 - branch_softmax_mean_ev_fail: 0.0071 - branch_softmax_1_evidence: 0.0412 - branch_softmax_1_mean_ev_succ: 0.4499 - branch_softmax_1_mean_ev_fail: 0.0426 - branch_softmax_2_evidence: 0.0502 - branch_softmax_2_mean_ev_succ: 0.5631 - branch_softmax_2_mean_ev_fail: 0.0485 - val_loss: 5.3252 - val_classification_loss: 2.2280 - val_branch_softmax_loss: 1.4983 - val_branch_softmax_1_loss: 0.9695 - val_branch_softmax_2_loss: 0.6294 - val_classification_accuracy: 0.9469 - val_branch_softmax_accuracy: 0.6516 - val_branch_softmax_1_accuracy: 0.7161 - val_branch_softmax_2_accuracy: 0.8009 - val_branch_softmax_evidence: 0.0281 - val_branch_softmax_mean_ev_succ: 0.3941 - val_branch_softmax_mean_ev_fail: 0.0764 - val_branch_softmax_1_evidence: 0.0317 - val_branch_softmax_1_mean_ev_succ: 0.4157 - val_branch_softmax_1_mean_ev_fail: 0.0673 - val_branch_softmax_2_evidence: 0.0388 - val_branch_softmax_2_mean_ev_succ: 0.4655 - val_branch_softmax_2_mean_ev_fail: 0.0711\n",
      "\n",
      "Epoch 00004: saving model to models\\mobileNetv2_evidence_conv2d.hdf5\n",
      "Epoch 5/12\n",
      "1406/1406 [==============================] - 544s 370ms/step - loss: 2.6499 - classification_loss: 2.1766 - branch_softmax_loss: 0.0279 - branch_softmax_1_loss: 0.1797 - branch_softmax_2_loss: 0.2656 - classification_accuracy: 0.9909 - branch_softmax_accuracy: 0.9971 - branch_softmax_1_accuracy: 0.9453 - branch_softmax_2_accuracy: 0.9092 - branch_softmax_evidence: 0.0406 - branch_softmax_mean_ev_succ: 0.4074 - branch_softmax_mean_ev_fail: 0.0014 - branch_softmax_1_evidence: 0.0465 - branch_softmax_1_mean_ev_succ: 0.4924 - branch_softmax_1_mean_ev_fail: 0.0329 - branch_softmax_2_evidence: 0.0586 - branch_softmax_2_mean_ev_succ: 0.6441 - branch_softmax_2_mean_ev_fail: 0.0460 - val_loss: 5.1472 - val_classification_loss: 2.2267 - val_branch_softmax_loss: 1.4313 - val_branch_softmax_1_loss: 0.9673 - val_branch_softmax_2_loss: 0.5220 - val_classification_accuracy: 0.9445 - val_branch_softmax_accuracy: 0.6789 - val_branch_softmax_1_accuracy: 0.7240 - val_branch_softmax_2_accuracy: 0.8329 - val_branch_softmax_evidence: 0.0459 - val_branch_softmax_mean_ev_succ: 0.6275 - val_branch_softmax_mean_ev_fail: 0.1034 - val_branch_softmax_1_evidence: 0.0549 - val_branch_softmax_1_mean_ev_succ: 0.7219 - val_branch_softmax_1_mean_ev_fail: 0.0883 - val_branch_softmax_2_evidence: 0.0421 - val_branch_softmax_2_mean_ev_succ: 0.4964 - val_branch_softmax_2_mean_ev_fail: 0.0364\n",
      "\n",
      "Epoch 00005: saving model to models\\mobileNetv2_evidence_conv2d.hdf5\n",
      "Epoch 6/12\n",
      "1406/1406 [==============================] - 546s 372ms/step - loss: 2.5107 - classification_loss: 2.1754 - branch_softmax_loss: 0.0122 - branch_softmax_1_loss: 0.1165 - branch_softmax_2_loss: 0.2067 - classification_accuracy: 0.9914 - branch_softmax_accuracy: 0.9997 - branch_softmax_1_accuracy: 0.9691 - branch_softmax_2_accuracy: 0.9313 - branch_softmax_evidence: 0.0445 - branch_softmax_mean_ev_succ: 0.4453 - branch_softmax_mean_ev_fail: 2.0746e-04 - branch_softmax_1_evidence: 0.0512 - branch_softmax_1_mean_ev_succ: 0.5294 - branch_softmax_1_mean_ev_fail: 0.0227 - branch_softmax_2_evidence: 0.0651 - branch_softmax_2_mean_ev_succ: 0.6999 - branch_softmax_2_mean_ev_fail: 0.0412 - val_loss: 5.3491 - val_classification_loss: 2.2261 - val_branch_softmax_loss: 1.4266 - val_branch_softmax_1_loss: 1.1117 - val_branch_softmax_2_loss: 0.5847 - val_classification_accuracy: 0.9471 - val_branch_softmax_accuracy: 0.6823 - val_branch_softmax_1_accuracy: 0.7175 - val_branch_softmax_2_accuracy: 0.8233 - val_branch_softmax_evidence: 0.0296 - val_branch_softmax_mean_ev_succ: 0.4069 - val_branch_softmax_mean_ev_fail: 0.0519 - val_branch_softmax_1_evidence: 0.0657 - val_branch_softmax_1_mean_ev_succ: 0.8754 - val_branch_softmax_1_mean_ev_fail: 0.0906 - val_branch_softmax_2_evidence: 0.0731 - val_branch_softmax_2_mean_ev_succ: 0.8713 - val_branch_softmax_2_mean_ev_fail: 0.0624\n",
      "\n",
      "Epoch 00006: saving model to models\\mobileNetv2_evidence_conv2d.hdf5\n",
      "Epoch 7/12\n",
      "1406/1406 [==============================] - 545s 371ms/step - loss: 2.4216 - classification_loss: 2.1720 - branch_softmax_loss: 0.0073 - branch_softmax_1_loss: 0.0768 - branch_softmax_2_loss: 0.1656 - classification_accuracy: 0.9935 - branch_softmax_accuracy: 0.9999 - branch_softmax_1_accuracy: 0.9824 - branch_softmax_2_accuracy: 0.9440 - branch_softmax_evidence: 0.0443 - branch_softmax_mean_ev_succ: 0.4427 - branch_softmax_mean_ev_fail: 4.2491e-06 - branch_softmax_1_evidence: 0.0573 - branch_softmax_1_mean_ev_succ: 0.5841 - branch_softmax_1_mean_ev_fail: 0.0123 - branch_softmax_2_evidence: 0.0725 - branch_softmax_2_mean_ev_succ: 0.7690 - branch_softmax_2_mean_ev_fail: 0.0327 - val_loss: 5.2639 - val_classification_loss: 2.2169 - val_branch_softmax_loss: 1.4756 - val_branch_softmax_1_loss: 1.0662 - val_branch_softmax_2_loss: 0.5052 - val_classification_accuracy: 0.9543 - val_branch_softmax_accuracy: 0.6813 - val_branch_softmax_1_accuracy: 0.7300 - val_branch_softmax_2_accuracy: 0.8415 - val_branch_softmax_evidence: 0.0394 - val_branch_softmax_mean_ev_succ: 0.5379 - val_branch_softmax_mean_ev_fail: 0.0803 - val_branch_softmax_1_evidence: 0.0425 - val_branch_softmax_1_mean_ev_succ: 0.5639 - val_branch_softmax_1_mean_ev_fail: 0.0443 - val_branch_softmax_2_evidence: 0.0783 - val_branch_softmax_2_mean_ev_succ: 0.9161 - val_branch_softmax_2_mean_ev_fail: 0.0666\n",
      "\n",
      "Epoch 00007: saving model to models\\mobileNetv2_evidence_conv2d.hdf5\n",
      "Epoch 8/12\n",
      "1406/1406 [==============================] - 547s 372ms/step - loss: 2.3564 - classification_loss: 2.1711 - branch_softmax_loss: 0.0050 - branch_softmax_1_loss: 0.0511 - branch_softmax_2_loss: 0.1292 - classification_accuracy: 0.9944 - branch_softmax_accuracy: 1.0000 - branch_softmax_1_accuracy: 0.9897 - branch_softmax_2_accuracy: 0.9592 - branch_softmax_evidence: 0.0482 - branch_softmax_mean_ev_succ: 0.4817 - branch_softmax_mean_ev_fail: 0.0000e+00 - branch_softmax_1_evidence: 0.0593 - branch_softmax_1_mean_ev_succ: 0.5989 - branch_softmax_1_mean_ev_fail: 0.0080 - branch_softmax_2_evidence: 0.0802 - branch_softmax_2_mean_ev_succ: 0.8368 - branch_softmax_2_mean_ev_fail: 0.0337 - val_loss: 5.8074 - val_classification_loss: 2.2226 - val_branch_softmax_loss: 1.7292 - val_branch_softmax_1_loss: 1.2447 - val_branch_softmax_2_loss: 0.6108 - val_classification_accuracy: 0.9489 - val_branch_softmax_accuracy: 0.6581 - val_branch_softmax_1_accuracy: 0.7051 - val_branch_softmax_2_accuracy: 0.8221 - val_branch_softmax_evidence: 0.0578 - val_branch_softmax_mean_ev_succ: 0.8100 - val_branch_softmax_mean_ev_fail: 0.1214 - val_branch_softmax_1_evidence: 0.0676 - val_branch_softmax_1_mean_ev_succ: 0.9049 - val_branch_softmax_1_mean_ev_fail: 0.1186 - val_branch_softmax_2_evidence: 0.1018 - val_branch_softmax_2_mean_ev_succ: 1.2139 - val_branch_softmax_2_mean_ev_fail: 0.0866\n",
      "\n",
      "Epoch 00008: saving model to models\\mobileNetv2_evidence_conv2d.hdf5\n",
      "Epoch 9/12\n",
      "1406/1406 [==============================] - 555s 377ms/step - loss: 2.3079 - classification_loss: 2.1691 - branch_softmax_loss: 0.0041 - branch_softmax_1_loss: 0.0361 - branch_softmax_2_loss: 0.0986 - classification_accuracy: 0.9955 - branch_softmax_accuracy: 1.0000 - branch_softmax_1_accuracy: 0.9947 - branch_softmax_2_accuracy: 0.9712 - branch_softmax_evidence: 0.0497 - branch_softmax_mean_ev_succ: 0.4967 - branch_softmax_mean_ev_fail: 0.0000e+00 - branch_softmax_1_evidence: 0.0655 - branch_softmax_1_mean_ev_succ: 0.6595 - branch_softmax_1_mean_ev_fail: 0.0049 - branch_softmax_2_evidence: 0.0900 - branch_softmax_2_mean_ev_succ: 0.9289 - branch_softmax_2_mean_ev_fail: 0.0229 - val_loss: 5.6923 - val_classification_loss: 2.2226 - val_branch_softmax_loss: 1.5409 - val_branch_softmax_1_loss: 1.2044 - val_branch_softmax_2_loss: 0.7244 - val_classification_accuracy: 0.9475 - val_branch_softmax_accuracy: 0.6855 - val_branch_softmax_1_accuracy: 0.7192 - val_branch_softmax_2_accuracy: 0.8073 - val_branch_softmax_evidence: 0.0547 - val_branch_softmax_mean_ev_succ: 0.7431 - val_branch_softmax_mean_ev_fail: 0.1090 - val_branch_softmax_1_evidence: 0.0540 - val_branch_softmax_1_mean_ev_succ: 0.7215 - val_branch_softmax_1_mean_ev_fail: 0.0709 - val_branch_softmax_2_evidence: 0.0822 - val_branch_softmax_2_mean_ev_succ: 0.9916 - val_branch_softmax_2_mean_ev_fail: 0.0979\n",
      "\n",
      "Epoch 00009: saving model to models\\mobileNetv2_evidence_conv2d.hdf5\n",
      "Epoch 10/12\n",
      "1406/1406 [==============================] - 561s 382ms/step - loss: 2.2747 - classification_loss: 2.1685 - branch_softmax_loss: 0.0032 - branch_softmax_1_loss: 0.0268 - branch_softmax_2_loss: 0.0762 - classification_accuracy: 0.9958 - branch_softmax_accuracy: 1.0000 - branch_softmax_1_accuracy: 0.9963 - branch_softmax_2_accuracy: 0.9782 - branch_softmax_evidence: 0.0495 - branch_softmax_mean_ev_succ: 0.4953 - branch_softmax_mean_ev_fail: 0.0000e+00 - branch_softmax_1_evidence: 0.0645 - branch_softmax_1_mean_ev_succ: 0.6476 - branch_softmax_1_mean_ev_fail: 0.0017 - branch_softmax_2_evidence: 0.0955 - branch_softmax_2_mean_ev_succ: 0.9775 - branch_softmax_2_mean_ev_fail: 0.0157 - val_loss: 5.5986 - val_classification_loss: 2.2216 - val_branch_softmax_loss: 1.5579 - val_branch_softmax_1_loss: 1.2203 - val_branch_softmax_2_loss: 0.5988 - val_classification_accuracy: 0.9495 - val_branch_softmax_accuracy: 0.6873 - val_branch_softmax_1_accuracy: 0.7300 - val_branch_softmax_2_accuracy: 0.8351 - val_branch_softmax_evidence: 0.0558 - val_branch_softmax_mean_ev_succ: 0.7653 - val_branch_softmax_mean_ev_fail: 0.0950 - val_branch_softmax_1_evidence: 0.0477 - val_branch_softmax_1_mean_ev_succ: 0.6333 - val_branch_softmax_1_mean_ev_fail: 0.0561 - val_branch_softmax_2_evidence: 0.1003 - val_branch_softmax_2_mean_ev_succ: 1.1758 - val_branch_softmax_2_mean_ev_fail: 0.1139\n",
      "\n",
      "Epoch 00010: saving model to models\\mobileNetv2_evidence_conv2d.hdf5\n",
      "Epoch 11/12\n",
      "1406/1406 [==============================] - 558s 379ms/step - loss: 2.2450 - classification_loss: 2.1674 - branch_softmax_loss: 0.0026 - branch_softmax_1_loss: 0.0182 - branch_softmax_2_loss: 0.0566 - classification_accuracy: 0.9967 - branch_softmax_accuracy: 1.0000 - branch_softmax_1_accuracy: 0.9985 - branch_softmax_2_accuracy: 0.9852 - branch_softmax_evidence: 0.0516 - branch_softmax_mean_ev_succ: 0.5162 - branch_softmax_mean_ev_fail: 0.0000e+00 - branch_softmax_1_evidence: 0.0688 - branch_softmax_1_mean_ev_succ: 0.6889 - branch_softmax_1_mean_ev_fail: 8.1532e-04 - branch_softmax_2_evidence: 0.1029 - branch_softmax_2_mean_ev_succ: 1.0459 - branch_softmax_2_mean_ev_fail: 0.0119 - val_loss: 5.5372 - val_classification_loss: 2.2181 - val_branch_softmax_loss: 1.5212 - val_branch_softmax_1_loss: 1.1738 - val_branch_softmax_2_loss: 0.6241 - val_classification_accuracy: 0.9519 - val_branch_softmax_accuracy: 0.6907 - val_branch_softmax_1_accuracy: 0.7356 - val_branch_softmax_2_accuracy: 0.8335 - val_branch_softmax_evidence: 0.0428 - val_branch_softmax_mean_ev_succ: 0.5850 - val_branch_softmax_mean_ev_fail: 0.0662 - val_branch_softmax_1_evidence: 0.0679 - val_branch_softmax_1_mean_ev_succ: 0.8939 - val_branch_softmax_1_mean_ev_fail: 0.0646 - val_branch_softmax_2_evidence: 0.1108 - val_branch_softmax_2_mean_ev_succ: 1.3097 - val_branch_softmax_2_mean_ev_fail: 0.0784\n",
      "\n",
      "Epoch 00011: saving model to models\\mobileNetv2_evidence_conv2d.hdf5\n",
      "Epoch 12/12\n",
      "1406/1406 [==============================] - 556s 378ms/step - loss: 2.2315 - classification_loss: 2.1671 - branch_softmax_loss: 0.0023 - branch_softmax_1_loss: 0.0144 - branch_softmax_2_loss: 0.0476 - classification_accuracy: 0.9964 - branch_softmax_accuracy: 1.0000 - branch_softmax_1_accuracy: 0.9987 - branch_softmax_2_accuracy: 0.9881 - branch_softmax_evidence: 0.0519 - branch_softmax_mean_ev_succ: 0.5192 - branch_softmax_mean_ev_fail: 0.0000e+00 - branch_softmax_1_evidence: 0.0673 - branch_softmax_1_mean_ev_succ: 0.6738 - branch_softmax_1_mean_ev_fail: 4.0388e-04 - branch_softmax_2_evidence: 0.1131 - branch_softmax_2_mean_ev_succ: 1.1447 - branch_softmax_2_mean_ev_fail: 0.0091 - val_loss: 5.5867 - val_classification_loss: 2.2187 - val_branch_softmax_loss: 1.5510 - val_branch_softmax_1_loss: 1.2173 - val_branch_softmax_2_loss: 0.5997 - val_classification_accuracy: 0.9531 - val_branch_softmax_accuracy: 0.6945 - val_branch_softmax_1_accuracy: 0.7416 - val_branch_softmax_2_accuracy: 0.8480 - val_branch_softmax_evidence: 0.0523 - val_branch_softmax_mean_ev_succ: 0.7108 - val_branch_softmax_mean_ev_fail: 0.0939 - val_branch_softmax_1_evidence: 0.0684 - val_branch_softmax_1_mean_ev_succ: 0.8993 - val_branch_softmax_1_mean_ev_fail: 0.0563 - val_branch_softmax_2_evidence: 0.1226 - val_branch_softmax_2_mean_ev_succ: 1.4293 - val_branch_softmax_2_mean_ev_fail: 0.0719\n",
      "\n",
      "Epoch 00012: saving model to models\\mobileNetv2_evidence_conv2d.hdf5\n",
      "<tensorflow.python.keras.callbacks.History object at 0x0000021A4AE1B710>\n",
      "312/312 - 33s - loss: 5.8005 - classification_loss: 2.2241 - branch_softmax_loss: 1.6498 - branch_softmax_1_loss: 1.2493 - branch_softmax_2_loss: 0.6774 - classification_accuracy: 0.9471 - branch_softmax_accuracy: 0.6828 - branch_softmax_1_accuracy: 0.7321 - branch_softmax_2_accuracy: 0.8322 - branch_softmax_evidence: 0.0515 - branch_softmax_mean_ev_succ: 0.7156 - branch_softmax_mean_ev_fail: 0.0853 - branch_softmax_1_evidence: 0.0687 - branch_softmax_1_mean_ev_succ: 0.9106 - branch_softmax_1_mean_ev_fail: 0.0688 - branch_softmax_2_evidence: 0.1219 - branch_softmax_2_mean_ev_succ: 1.4491 - branch_softmax_2_mean_ev_fail: 0.0791\n",
      "overall loss: 5.800529479980469\n"
     ]
    }
   ],
   "source": [
    "model = brevis.trainTransfer(12, loss=loss_fn, optimizer=tf.optimizers.SGD(), transfer=False, customOptions=\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting Main Layers  and branch layers training to true\n",
      "\n",
      "customOption: Other\n",
      "https://app.neptune.ai/cailen01/branchingDNN/e/BRAN-412\n",
      "Remember to stop your run once youve finished logging your metadata (https://docs.neptune.ai/api-reference/run#stop). It will be stopped automatically only when the notebook kernel/interactive console is terminated.\n",
      "Epoch 1/18\n",
      "1406/1406 [==============================] - ETA: 0s - loss: 2.2155 - classification_loss: 2.1658 - branch_softmax_loss: 0.0019 - branch_softmax_1_loss: 0.0107 - branch_softmax_2_loss: 0.0371 - classification_accuracy: 0.9978 - branch_softmax_accuracy: 1.0000 - branch_softmax_1_accuracy: 0.9998 - branch_softmax_2_accuracy: 0.9910 - branch_softmax_evidence: 0.0505 - branch_softmax_mean_ev_succ: 0.5053 - branch_softmax_mean_ev_fail: 0.0000e+00 - branch_softmax_1_evidence: 0.0698 - branch_softmax_1_mean_ev_succ: 0.6978 - branch_softmax_1_mean_ev_fail: 3.0467e-05 - branch_softmax_2_evidence: 0.1162 - branch_softmax_2_mean_ev_succ: 1.1719 - branch_softmax_2_mean_ev_fail: 0.0136"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: string series 'monitoring/stdout' value was longer than 1000 characters and was truncated. This warning is printed only once per series.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1406/1406 [==============================] - 567s 385ms/step - loss: 2.2155 - classification_loss: 2.1658 - branch_softmax_loss: 0.0019 - branch_softmax_1_loss: 0.0107 - branch_softmax_2_loss: 0.0371 - classification_accuracy: 0.9978 - branch_softmax_accuracy: 1.0000 - branch_softmax_1_accuracy: 0.9998 - branch_softmax_2_accuracy: 0.9910 - branch_softmax_evidence: 0.0505 - branch_softmax_mean_ev_succ: 0.5053 - branch_softmax_mean_ev_fail: 0.0000e+00 - branch_softmax_1_evidence: 0.0698 - branch_softmax_1_mean_ev_succ: 0.6978 - branch_softmax_1_mean_ev_fail: 3.0467e-05 - branch_softmax_2_evidence: 0.1162 - branch_softmax_2_mean_ev_succ: 1.1719 - branch_softmax_2_mean_ev_fail: 0.0136 - val_loss: 5.6164 - val_classification_loss: 2.2175 - val_branch_softmax_loss: 1.5477 - val_branch_softmax_1_loss: 1.2428 - val_branch_softmax_2_loss: 0.6084 - val_classification_accuracy: 0.9535 - val_branch_softmax_accuracy: 0.6959 - val_branch_softmax_1_accuracy: 0.7396 - val_branch_softmax_2_accuracy: 0.8429 - val_branch_softmax_evidence: 0.0417 - val_branch_softmax_mean_ev_succ: 0.5676 - val_branch_softmax_mean_ev_fail: 0.0664 - val_branch_softmax_1_evidence: 0.0754 - val_branch_softmax_1_mean_ev_succ: 0.9848 - val_branch_softmax_1_mean_ev_fail: 0.0893 - val_branch_softmax_2_evidence: 0.1249 - val_branch_softmax_2_mean_ev_succ: 1.4653 - val_branch_softmax_2_mean_ev_fail: 0.0737\n",
      "\n",
      "Epoch 00001: saving model to models\\mobileNetv2_evidence_conv2d_30.hdf5\n",
      "Epoch 2/18\n",
      "1406/1406 [==============================] - 560s 381ms/step - loss: 2.2045 - classification_loss: 2.1656 - branch_softmax_loss: 0.0017 - branch_softmax_1_loss: 0.0088 - branch_softmax_2_loss: 0.0284 - classification_accuracy: 0.9975 - branch_softmax_accuracy: 1.0000 - branch_softmax_1_accuracy: 0.9997 - branch_softmax_2_accuracy: 0.9939 - branch_softmax_evidence: 0.0498 - branch_softmax_mean_ev_succ: 0.4982 - branch_softmax_mean_ev_fail: 0.0000e+00 - branch_softmax_1_evidence: 0.0715 - branch_softmax_1_mean_ev_succ: 0.7148 - branch_softmax_1_mean_ev_fail: 9.9876e-04 - branch_softmax_2_evidence: 0.1190 - branch_softmax_2_mean_ev_succ: 1.1974 - branch_softmax_2_mean_ev_fail: 0.0044 - val_loss: 5.6772 - val_classification_loss: 2.2160 - val_branch_softmax_loss: 1.5724 - val_branch_softmax_1_loss: 1.2329 - val_branch_softmax_2_loss: 0.6559 - val_classification_accuracy: 0.9553 - val_branch_softmax_accuracy: 0.6949 - val_branch_softmax_1_accuracy: 0.7408 - val_branch_softmax_2_accuracy: 0.8444 - val_branch_softmax_evidence: 0.0509 - val_branch_softmax_mean_ev_succ: 0.6965 - val_branch_softmax_mean_ev_fail: 0.0736 - val_branch_softmax_1_evidence: 0.0801 - val_branch_softmax_1_mean_ev_succ: 1.0510 - val_branch_softmax_1_mean_ev_fail: 0.0814 - val_branch_softmax_2_evidence: 0.1509 - val_branch_softmax_2_mean_ev_succ: 1.7611 - val_branch_softmax_2_mean_ev_fail: 0.1195\n",
      "\n",
      "Epoch 00002: saving model to models\\mobileNetv2_evidence_conv2d_30.hdf5\n",
      "Epoch 3/18\n",
      "1406/1406 [==============================] - 555s 377ms/step - loss: 2.1975 - classification_loss: 2.1647 - branch_softmax_loss: 0.0016 - branch_softmax_1_loss: 0.0073 - branch_softmax_2_loss: 0.0239 - classification_accuracy: 0.9983 - branch_softmax_accuracy: 1.0000 - branch_softmax_1_accuracy: 0.9998 - branch_softmax_2_accuracy: 0.9952 - branch_softmax_evidence: 0.0519 - branch_softmax_mean_ev_succ: 0.5188 - branch_softmax_mean_ev_fail: 1.3849e-09 - branch_softmax_1_evidence: 0.0732 - branch_softmax_1_mean_ev_succ: 0.7320 - branch_softmax_1_mean_ev_fail: 7.9452e-05 - branch_softmax_2_evidence: 0.1239 - branch_softmax_2_mean_ev_succ: 1.2455 - branch_softmax_2_mean_ev_fail: 0.0053 - val_loss: 5.6774 - val_classification_loss: 2.2175 - val_branch_softmax_loss: 1.5786 - val_branch_softmax_1_loss: 1.2551 - val_branch_softmax_2_loss: 0.6262 - val_classification_accuracy: 0.9529 - val_branch_softmax_accuracy: 0.6953 - val_branch_softmax_1_accuracy: 0.7386 - val_branch_softmax_2_accuracy: 0.8438 - val_branch_softmax_evidence: 0.0465 - val_branch_softmax_mean_ev_succ: 0.6330 - val_branch_softmax_mean_ev_fail: 0.0698 - val_branch_softmax_1_evidence: 0.0702 - val_branch_softmax_1_mean_ev_succ: 0.9211 - val_branch_softmax_1_mean_ev_fail: 0.0771 - val_branch_softmax_2_evidence: 0.1308 - val_branch_softmax_2_mean_ev_succ: 1.5288 - val_branch_softmax_2_mean_ev_fail: 0.0997\n",
      "\n",
      "Epoch 00003: saving model to models\\mobileNetv2_evidence_conv2d_30.hdf5\n",
      "Epoch 4/18\n",
      "1406/1406 [==============================] - 557s 379ms/step - loss: 2.1915 - classification_loss: 2.1647 - branch_softmax_loss: 0.0014 - branch_softmax_1_loss: 0.0069 - branch_softmax_2_loss: 0.0185 - classification_accuracy: 0.9982 - branch_softmax_accuracy: 1.0000 - branch_softmax_1_accuracy: 0.9999 - branch_softmax_2_accuracy: 0.9966 - branch_softmax_evidence: 0.0538 - branch_softmax_mean_ev_succ: 0.5379 - branch_softmax_mean_ev_fail: 0.0000e+00 - branch_softmax_1_evidence: 0.0770 - branch_softmax_1_mean_ev_succ: 0.7701 - branch_softmax_1_mean_ev_fail: 1.6342e-04 - branch_softmax_2_evidence: 0.1325 - branch_softmax_2_mean_ev_succ: 1.3301 - branch_softmax_2_mean_ev_fail: 0.0026 - val_loss: 5.7878 - val_classification_loss: 2.2179 - val_branch_softmax_loss: 1.5974 - val_branch_softmax_1_loss: 1.2935 - val_branch_softmax_2_loss: 0.6790 - val_classification_accuracy: 0.9517 - val_branch_softmax_accuracy: 0.6933 - val_branch_softmax_1_accuracy: 0.7348 - val_branch_softmax_2_accuracy: 0.8377 - val_branch_softmax_evidence: 0.0495 - val_branch_softmax_mean_ev_succ: 0.6734 - val_branch_softmax_mean_ev_fail: 0.0790 - val_branch_softmax_1_evidence: 0.0653 - val_branch_softmax_1_mean_ev_succ: 0.8621 - val_branch_softmax_1_mean_ev_fail: 0.0763 - val_branch_softmax_2_evidence: 0.1286 - val_branch_softmax_2_mean_ev_succ: 1.5081 - val_branch_softmax_2_mean_ev_fail: 0.0976\n",
      "\n",
      "Epoch 00004: saving model to models\\mobileNetv2_evidence_conv2d_30.hdf5\n",
      "Epoch 5/18\n",
      "1406/1406 [==============================] - 559s 380ms/step - loss: 2.1874 - classification_loss: 2.1642 - branch_softmax_loss: 0.0013 - branch_softmax_1_loss: 0.0054 - branch_softmax_2_loss: 0.0165 - classification_accuracy: 0.9987 - branch_softmax_accuracy: 1.0000 - branch_softmax_1_accuracy: 0.9999 - branch_softmax_2_accuracy: 0.9973 - branch_softmax_evidence: 0.0544 - branch_softmax_mean_ev_succ: 0.5438 - branch_softmax_mean_ev_fail: 0.0000e+00 - branch_softmax_1_evidence: 0.0754 - branch_softmax_1_mean_ev_succ: 0.7540 - branch_softmax_1_mean_ev_fail: 4.1211e-06 - branch_softmax_2_evidence: 0.1380 - branch_softmax_2_mean_ev_succ: 1.3834 - branch_softmax_2_mean_ev_fail: 0.0025 - val_loss: 5.7818 - val_classification_loss: 2.2177 - val_branch_softmax_loss: 1.6314 - val_branch_softmax_1_loss: 1.2933 - val_branch_softmax_2_loss: 0.6395 - val_classification_accuracy: 0.9529 - val_branch_softmax_accuracy: 0.6873 - val_branch_softmax_1_accuracy: 0.7450 - val_branch_softmax_2_accuracy: 0.8494 - val_branch_softmax_evidence: 0.0413 - val_branch_softmax_mean_ev_succ: 0.5657 - val_branch_softmax_mean_ev_fail: 0.0654 - val_branch_softmax_1_evidence: 0.0830 - val_branch_softmax_1_mean_ev_succ: 1.0799 - val_branch_softmax_1_mean_ev_fail: 0.0878 - val_branch_softmax_2_evidence: 0.1257 - val_branch_softmax_2_mean_ev_succ: 1.4661 - val_branch_softmax_2_mean_ev_fail: 0.0691\n",
      "\n",
      "Epoch 00005: saving model to models\\mobileNetv2_evidence_conv2d_30.hdf5\n",
      "Epoch 6/18\n",
      "1406/1406 [==============================] - 561s 381ms/step - loss: 2.1842 - classification_loss: 2.1641 - branch_softmax_loss: 0.0012 - branch_softmax_1_loss: 0.0051 - branch_softmax_2_loss: 0.0139 - classification_accuracy: 0.9987 - branch_softmax_accuracy: 1.0000 - branch_softmax_1_accuracy: 1.0000 - branch_softmax_2_accuracy: 0.9981 - branch_softmax_evidence: 0.0555 - branch_softmax_mean_ev_succ: 0.5549 - branch_softmax_mean_ev_fail: 0.0000e+00 - branch_softmax_1_evidence: 0.0770 - branch_softmax_1_mean_ev_succ: 0.7700 - branch_softmax_1_mean_ev_fail: 7.8074e-07 - branch_softmax_2_evidence: 0.1346 - branch_softmax_2_mean_ev_succ: 1.3489 - branch_softmax_2_mean_ev_fail: 9.9968e-04 - val_loss: 5.8388 - val_classification_loss: 2.2164 - val_branch_softmax_loss: 1.6229 - val_branch_softmax_1_loss: 1.3391 - val_branch_softmax_2_loss: 0.6603 - val_classification_accuracy: 0.9531 - val_branch_softmax_accuracy: 0.6929 - val_branch_softmax_1_accuracy: 0.7400 - val_branch_softmax_2_accuracy: 0.8474 - val_branch_softmax_evidence: 0.0418 - val_branch_softmax_mean_ev_succ: 0.5727 - val_branch_softmax_mean_ev_fail: 0.0587 - val_branch_softmax_1_evidence: 0.0696 - val_branch_softmax_1_mean_ev_succ: 0.9150 - val_branch_softmax_1_mean_ev_fail: 0.0686 - val_branch_softmax_2_evidence: 0.1520 - val_branch_softmax_2_mean_ev_succ: 1.7726 - val_branch_softmax_2_mean_ev_fail: 0.0831\n",
      "\n",
      "Epoch 00006: saving model to models\\mobileNetv2_evidence_conv2d_30.hdf5\n",
      "Epoch 7/18\n",
      "1406/1406 [==============================] - 557s 379ms/step - loss: 2.1813 - classification_loss: 2.1639 - branch_softmax_loss: 0.0011 - branch_softmax_1_loss: 0.0048 - branch_softmax_2_loss: 0.0114 - classification_accuracy: 0.9988 - branch_softmax_accuracy: 1.0000 - branch_softmax_1_accuracy: 0.9997 - branch_softmax_2_accuracy: 0.9985 - branch_softmax_evidence: 0.0580 - branch_softmax_mean_ev_succ: 0.5798 - branch_softmax_mean_ev_fail: 5.3826e-09 - branch_softmax_1_evidence: 0.0799 - branch_softmax_1_mean_ev_succ: 0.7988 - branch_softmax_1_mean_ev_fail: 5.2088e-06 - branch_softmax_2_evidence: 0.1413 - branch_softmax_2_mean_ev_succ: 1.4161 - branch_softmax_2_mean_ev_fail: 0.0011 - val_loss: 5.7574 - val_classification_loss: 2.2162 - val_branch_softmax_loss: 1.6169 - val_branch_softmax_1_loss: 1.2883 - val_branch_softmax_2_loss: 0.6360 - val_classification_accuracy: 0.9527 - val_branch_softmax_accuracy: 0.6983 - val_branch_softmax_1_accuracy: 0.7440 - val_branch_softmax_2_accuracy: 0.8530 - val_branch_softmax_evidence: 0.0492 - val_branch_softmax_mean_ev_succ: 0.6681 - val_branch_softmax_mean_ev_fail: 0.0719 - val_branch_softmax_1_evidence: 0.0828 - val_branch_softmax_1_mean_ev_succ: 1.0802 - val_branch_softmax_1_mean_ev_fail: 0.0808 - val_branch_softmax_2_evidence: 0.1460 - val_branch_softmax_2_mean_ev_succ: 1.6942 - val_branch_softmax_2_mean_ev_fail: 0.0651\n",
      "\n",
      "Epoch 00007: saving model to models\\mobileNetv2_evidence_conv2d_30.hdf5\n",
      "Epoch 8/18\n",
      "1406/1406 [==============================] - 560s 381ms/step - loss: 2.1784 - classification_loss: 2.1633 - branch_softmax_loss: 0.0010 - branch_softmax_1_loss: 0.0042 - branch_softmax_2_loss: 0.0099 - classification_accuracy: 0.9989 - branch_softmax_accuracy: 1.0000 - branch_softmax_1_accuracy: 1.0000 - branch_softmax_2_accuracy: 0.9988 - branch_softmax_evidence: 0.0572 - branch_softmax_mean_ev_succ: 0.5720 - branch_softmax_mean_ev_fail: 0.0000e+00 - branch_softmax_1_evidence: 0.0827 - branch_softmax_1_mean_ev_succ: 0.8272 - branch_softmax_1_mean_ev_fail: 1.2603e-06 - branch_softmax_2_evidence: 0.1451 - branch_softmax_2_mean_ev_succ: 1.4530 - branch_softmax_2_mean_ev_fail: 9.9482e-04 - val_loss: 5.8537 - val_classification_loss: 2.2153 - val_branch_softmax_loss: 1.6626 - val_branch_softmax_1_loss: 1.3107 - val_branch_softmax_2_loss: 0.6651 - val_classification_accuracy: 0.9543 - val_branch_softmax_accuracy: 0.6921 - val_branch_softmax_1_accuracy: 0.7418 - val_branch_softmax_2_accuracy: 0.8462 - val_branch_softmax_evidence: 0.0543 - val_branch_softmax_mean_ev_succ: 0.7456 - val_branch_softmax_mean_ev_fail: 0.0765 - val_branch_softmax_1_evidence: 0.0727 - val_branch_softmax_1_mean_ev_succ: 0.9550 - val_branch_softmax_1_mean_ev_fail: 0.0704 - val_branch_softmax_2_evidence: 0.1389 - val_branch_softmax_2_mean_ev_succ: 1.6221 - val_branch_softmax_2_mean_ev_fail: 0.0973\n",
      "\n",
      "Epoch 00008: saving model to models\\mobileNetv2_evidence_conv2d_30.hdf5\n",
      "Epoch 9/18\n",
      "1406/1406 [==============================] - 562s 382ms/step - loss: 2.1786 - classification_loss: 2.1633 - branch_softmax_loss: 9.5905e-04 - branch_softmax_1_loss: 0.0040 - branch_softmax_2_loss: 0.0103 - classification_accuracy: 0.9993 - branch_softmax_accuracy: 1.0000 - branch_softmax_1_accuracy: 0.9999 - branch_softmax_2_accuracy: 0.9984 - branch_softmax_evidence: 0.0580 - branch_softmax_mean_ev_succ: 0.5797 - branch_softmax_mean_ev_fail: 0.0000e+00 - branch_softmax_1_evidence: 0.0819 - branch_softmax_1_mean_ev_succ: 0.8187 - branch_softmax_1_mean_ev_fail: 9.1921e-08 - branch_softmax_2_evidence: 0.1401 - branch_softmax_2_mean_ev_succ: 1.4025 - branch_softmax_2_mean_ev_fail: 0.0019 - val_loss: 5.7891 - val_classification_loss: 2.2149 - val_branch_softmax_loss: 1.6162 - val_branch_softmax_1_loss: 1.3017 - val_branch_softmax_2_loss: 0.6562 - val_classification_accuracy: 0.9537 - val_branch_softmax_accuracy: 0.7013 - val_branch_softmax_1_accuracy: 0.7460 - val_branch_softmax_2_accuracy: 0.8548 - val_branch_softmax_evidence: 0.0577 - val_branch_softmax_mean_ev_succ: 0.7799 - val_branch_softmax_mean_ev_fail: 0.0876 - val_branch_softmax_1_evidence: 0.0863 - val_branch_softmax_1_mean_ev_succ: 1.1272 - val_branch_softmax_1_mean_ev_fail: 0.0910 - val_branch_softmax_2_evidence: 0.1670 - val_branch_softmax_2_mean_ev_succ: 1.9286 - val_branch_softmax_2_mean_ev_fail: 0.0970\n",
      "\n",
      "Epoch 00009: saving model to models\\mobileNetv2_evidence_conv2d_30.hdf5\n",
      "Epoch 10/18\n",
      "1406/1406 [==============================] - 560s 381ms/step - loss: 2.1763 - classification_loss: 2.1634 - branch_softmax_loss: 9.1915e-04 - branch_softmax_1_loss: 0.0035 - branch_softmax_2_loss: 0.0085 - classification_accuracy: 0.9989 - branch_softmax_accuracy: 1.0000 - branch_softmax_1_accuracy: 0.9999 - branch_softmax_2_accuracy: 0.9988 - branch_softmax_evidence: 0.0584 - branch_softmax_mean_ev_succ: 0.5840 - branch_softmax_mean_ev_fail: 0.0000e+00 - branch_softmax_1_evidence: 0.0851 - branch_softmax_1_mean_ev_succ: 0.8511 - branch_softmax_1_mean_ev_fail: 2.7015e-06 - branch_softmax_2_evidence: 0.1524 - branch_softmax_2_mean_ev_succ: 1.5263 - branch_softmax_2_mean_ev_fail: 0.0017 - val_loss: 5.8604 - val_classification_loss: 2.2140 - val_branch_softmax_loss: 1.6221 - val_branch_softmax_1_loss: 1.3375 - val_branch_softmax_2_loss: 0.6867 - val_classification_accuracy: 0.9541 - val_branch_softmax_accuracy: 0.6977 - val_branch_softmax_1_accuracy: 0.7422 - val_branch_softmax_2_accuracy: 0.8494 - val_branch_softmax_evidence: 0.0480 - val_branch_softmax_mean_ev_succ: 0.6513 - val_branch_softmax_mean_ev_fail: 0.0708 - val_branch_softmax_1_evidence: 0.0863 - val_branch_softmax_1_mean_ev_succ: 1.1297 - val_branch_softmax_1_mean_ev_fail: 0.0791 - val_branch_softmax_2_evidence: 0.1459 - val_branch_softmax_2_mean_ev_succ: 1.6999 - val_branch_softmax_2_mean_ev_fail: 0.0689\n",
      "\n",
      "Epoch 00010: saving model to models\\mobileNetv2_evidence_conv2d_30.hdf5\n",
      "Epoch 11/18\n",
      "1406/1406 [==============================] - 560s 381ms/step - loss: 2.1755 - classification_loss: 2.1634 - branch_softmax_loss: 8.7559e-04 - branch_softmax_1_loss: 0.0035 - branch_softmax_2_loss: 0.0077 - classification_accuracy: 0.9989 - branch_softmax_accuracy: 1.0000 - branch_softmax_1_accuracy: 1.0000 - branch_softmax_2_accuracy: 0.9991 - branch_softmax_evidence: 0.0567 - branch_softmax_mean_ev_succ: 0.5669 - branch_softmax_mean_ev_fail: 0.0000e+00 - branch_softmax_1_evidence: 0.0857 - branch_softmax_1_mean_ev_succ: 0.8571 - branch_softmax_1_mean_ev_fail: 7.0735e-09 - branch_softmax_2_evidence: 0.1487 - branch_softmax_2_mean_ev_succ: 1.4881 - branch_softmax_2_mean_ev_fail: 9.4612e-05 - val_loss: 5.8529 - val_classification_loss: 2.2147 - val_branch_softmax_loss: 1.6390 - val_branch_softmax_1_loss: 1.3371 - val_branch_softmax_2_loss: 0.6621 - val_classification_accuracy: 0.9553 - val_branch_softmax_accuracy: 0.6975 - val_branch_softmax_1_accuracy: 0.7454 - val_branch_softmax_2_accuracy: 0.8502 - val_branch_softmax_evidence: 0.0467 - val_branch_softmax_mean_ev_succ: 0.6363 - val_branch_softmax_mean_ev_fail: 0.0688 - val_branch_softmax_1_evidence: 0.0857 - val_branch_softmax_1_mean_ev_succ: 1.1200 - val_branch_softmax_1_mean_ev_fail: 0.0747 - val_branch_softmax_2_evidence: 0.1448 - val_branch_softmax_2_mean_ev_succ: 1.6887 - val_branch_softmax_2_mean_ev_fail: 0.0614\n",
      "\n",
      "Epoch 00011: saving model to models\\mobileNetv2_evidence_conv2d_30.hdf5\n",
      "Epoch 12/18\n",
      "1406/1406 [==============================] - 564s 383ms/step - loss: 2.1732 - classification_loss: 2.1630 - branch_softmax_loss: 8.0011e-04 - branch_softmax_1_loss: 0.0031 - branch_softmax_2_loss: 0.0063 - classification_accuracy: 0.9994 - branch_softmax_accuracy: 1.0000 - branch_softmax_1_accuracy: 1.0000 - branch_softmax_2_accuracy: 0.9993 - branch_softmax_evidence: 0.0570 - branch_softmax_mean_ev_succ: 0.5696 - branch_softmax_mean_ev_fail: 0.0000e+00 - branch_softmax_1_evidence: 0.0842 - branch_softmax_1_mean_ev_succ: 0.8422 - branch_softmax_1_mean_ev_fail: 6.3125e-09 - branch_softmax_2_evidence: 0.1567 - branch_softmax_2_mean_ev_succ: 1.5679 - branch_softmax_2_mean_ev_fail: 5.5567e-04 - val_loss: 5.8366 - val_classification_loss: 2.2139 - val_branch_softmax_loss: 1.6464 - val_branch_softmax_1_loss: 1.3240 - val_branch_softmax_2_loss: 0.6523 - val_classification_accuracy: 0.9569 - val_branch_softmax_accuracy: 0.7013 - val_branch_softmax_1_accuracy: 0.7438 - val_branch_softmax_2_accuracy: 0.8574 - val_branch_softmax_evidence: 0.0497 - val_branch_softmax_mean_ev_succ: 0.6776 - val_branch_softmax_mean_ev_fail: 0.0675 - val_branch_softmax_1_evidence: 0.0871 - val_branch_softmax_1_mean_ev_succ: 1.1408 - val_branch_softmax_1_mean_ev_fail: 0.0754 - val_branch_softmax_2_evidence: 0.1628 - val_branch_softmax_2_mean_ev_succ: 1.8787 - val_branch_softmax_2_mean_ev_fail: 0.0935\n",
      "\n",
      "Epoch 00012: saving model to models\\mobileNetv2_evidence_conv2d_30.hdf5\n",
      "Epoch 13/18\n",
      "1406/1406 [==============================] - 564s 383ms/step - loss: 2.1718 - classification_loss: 2.1627 - branch_softmax_loss: 7.4880e-04 - branch_softmax_1_loss: 0.0029 - branch_softmax_2_loss: 0.0055 - classification_accuracy: 0.9992 - branch_softmax_accuracy: 1.0000 - branch_softmax_1_accuracy: 1.0000 - branch_softmax_2_accuracy: 0.9996 - branch_softmax_evidence: 0.0562 - branch_softmax_mean_ev_succ: 0.5616 - branch_softmax_mean_ev_fail: 0.0000e+00 - branch_softmax_1_evidence: 0.0836 - branch_softmax_1_mean_ev_succ: 0.8359 - branch_softmax_1_mean_ev_fail: 3.9055e-08 - branch_softmax_2_evidence: 0.1507 - branch_softmax_2_mean_ev_succ: 1.5072 - branch_softmax_2_mean_ev_fail: 0.0012 - val_loss: 5.8200 - val_classification_loss: 2.2126 - val_branch_softmax_loss: 1.6363 - val_branch_softmax_1_loss: 1.3250 - val_branch_softmax_2_loss: 0.6462 - val_classification_accuracy: 0.9569 - val_branch_softmax_accuracy: 0.6983 - val_branch_softmax_1_accuracy: 0.7474 - val_branch_softmax_2_accuracy: 0.8594 - val_branch_softmax_evidence: 0.0461 - val_branch_softmax_mean_ev_succ: 0.6278 - val_branch_softmax_mean_ev_fail: 0.0665 - val_branch_softmax_1_evidence: 0.0902 - val_branch_softmax_1_mean_ev_succ: 1.1752 - val_branch_softmax_1_mean_ev_fail: 0.0844 - val_branch_softmax_2_evidence: 0.1749 - val_branch_softmax_2_mean_ev_succ: 2.0124 - val_branch_softmax_2_mean_ev_fail: 0.1036\n",
      "\n",
      "Epoch 00013: saving model to models\\mobileNetv2_evidence_conv2d_30.hdf5\n",
      "Epoch 14/18\n",
      "1406/1406 [==============================] - 569s 386ms/step - loss: 2.1712 - classification_loss: 2.1629 - branch_softmax_loss: 6.8445e-04 - branch_softmax_1_loss: 0.0026 - branch_softmax_2_loss: 0.0051 - classification_accuracy: 0.9992 - branch_softmax_accuracy: 1.0000 - branch_softmax_1_accuracy: 0.9999 - branch_softmax_2_accuracy: 0.9996 - branch_softmax_evidence: 0.0579 - branch_softmax_mean_ev_succ: 0.5793 - branch_softmax_mean_ev_fail: 0.0000e+00 - branch_softmax_1_evidence: 0.0834 - branch_softmax_1_mean_ev_succ: 0.8343 - branch_softmax_1_mean_ev_fail: 3.3515e-08 - branch_softmax_2_evidence: 0.1680 - branch_softmax_2_mean_ev_succ: 1.6806 - branch_softmax_2_mean_ev_fail: 3.1622e-05 - val_loss: 5.8451 - val_classification_loss: 2.2147 - val_branch_softmax_loss: 1.6483 - val_branch_softmax_1_loss: 1.3295 - val_branch_softmax_2_loss: 0.6527 - val_classification_accuracy: 0.9545 - val_branch_softmax_accuracy: 0.6989 - val_branch_softmax_1_accuracy: 0.7466 - val_branch_softmax_2_accuracy: 0.8580 - val_branch_softmax_evidence: 0.0538 - val_branch_softmax_mean_ev_succ: 0.7305 - val_branch_softmax_mean_ev_fail: 0.0799 - val_branch_softmax_1_evidence: 0.0754 - val_branch_softmax_1_mean_ev_succ: 0.9896 - val_branch_softmax_1_mean_ev_fail: 0.0518 - val_branch_softmax_2_evidence: 0.1712 - val_branch_softmax_2_mean_ev_succ: 1.9719 - val_branch_softmax_2_mean_ev_fail: 0.0990\n",
      "\n",
      "Epoch 00014: saving model to models\\mobileNetv2_evidence_conv2d_30.hdf5\n",
      "Epoch 15/18\n",
      "1406/1406 [==============================] - 566s 385ms/step - loss: 2.1704 - classification_loss: 2.1625 - branch_softmax_loss: 6.7241e-04 - branch_softmax_1_loss: 0.0025 - branch_softmax_2_loss: 0.0048 - classification_accuracy: 0.9994 - branch_softmax_accuracy: 1.0000 - branch_softmax_1_accuracy: 0.9999 - branch_softmax_2_accuracy: 0.9996 - branch_softmax_evidence: 0.0555 - branch_softmax_mean_ev_succ: 0.5548 - branch_softmax_mean_ev_fail: 0.0000e+00 - branch_softmax_1_evidence: 0.0873 - branch_softmax_1_mean_ev_succ: 0.8733 - branch_softmax_1_mean_ev_fail: 4.3608e-07 - branch_softmax_2_evidence: 0.1633 - branch_softmax_2_mean_ev_succ: 1.6335 - branch_softmax_2_mean_ev_fail: 3.7468e-05 - val_loss: 5.9232 - val_classification_loss: 2.2148 - val_branch_softmax_loss: 1.6858 - val_branch_softmax_1_loss: 1.3388 - val_branch_softmax_2_loss: 0.6838 - val_classification_accuracy: 0.9565 - val_branch_softmax_accuracy: 0.7001 - val_branch_softmax_1_accuracy: 0.7420 - val_branch_softmax_2_accuracy: 0.8556 - val_branch_softmax_evidence: 0.0488 - val_branch_softmax_mean_ev_succ: 0.6679 - val_branch_softmax_mean_ev_fail: 0.0619 - val_branch_softmax_1_evidence: 0.0804 - val_branch_softmax_1_mean_ev_succ: 1.0585 - val_branch_softmax_1_mean_ev_fail: 0.0625 - val_branch_softmax_2_evidence: 0.1775 - val_branch_softmax_2_mean_ev_succ: 2.0536 - val_branch_softmax_2_mean_ev_fail: 0.1041\n",
      "\n",
      "Epoch 00015: saving model to models\\mobileNetv2_evidence_conv2d_30.hdf5\n",
      "Epoch 16/18\n",
      "1406/1406 [==============================] - 570s 387ms/step - loss: 2.1695 - classification_loss: 2.1623 - branch_softmax_loss: 6.2707e-04 - branch_softmax_1_loss: 0.0021 - branch_softmax_2_loss: 0.0044 - classification_accuracy: 0.9995 - branch_softmax_accuracy: 1.0000 - branch_softmax_1_accuracy: 1.0000 - branch_softmax_2_accuracy: 0.9993 - branch_softmax_evidence: 0.0571 - branch_softmax_mean_ev_succ: 0.5715 - branch_softmax_mean_ev_fail: 0.0000e+00 - branch_softmax_1_evidence: 0.0878 - branch_softmax_1_mean_ev_succ: 0.8781 - branch_softmax_1_mean_ev_fail: 0.0000e+00 - branch_softmax_2_evidence: 0.1710 - branch_softmax_2_mean_ev_succ: 1.7109 - branch_softmax_2_mean_ev_fail: 2.8194e-05 - val_loss: 5.9160 - val_classification_loss: 2.2151 - val_branch_softmax_loss: 1.6671 - val_branch_softmax_1_loss: 1.3605 - val_branch_softmax_2_loss: 0.6732 - val_classification_accuracy: 0.9537 - val_branch_softmax_accuracy: 0.6981 - val_branch_softmax_1_accuracy: 0.7440 - val_branch_softmax_2_accuracy: 0.8532 - val_branch_softmax_evidence: 0.0507 - val_branch_softmax_mean_ev_succ: 0.6917 - val_branch_softmax_mean_ev_fail: 0.0711 - val_branch_softmax_1_evidence: 0.0827 - val_branch_softmax_1_mean_ev_succ: 1.0839 - val_branch_softmax_1_mean_ev_fail: 0.0713 - val_branch_softmax_2_evidence: 0.1779 - val_branch_softmax_2_mean_ev_succ: 2.0632 - val_branch_softmax_2_mean_ev_fail: 0.0859\n",
      "\n",
      "Epoch 00016: saving model to models\\mobileNetv2_evidence_conv2d_30.hdf5\n",
      "Epoch 17/18\n",
      "1406/1406 [==============================] - 570s 387ms/step - loss: 2.1692 - classification_loss: 2.1626 - branch_softmax_loss: 6.4245e-04 - branch_softmax_1_loss: 0.0021 - branch_softmax_2_loss: 0.0039 - classification_accuracy: 0.9994 - branch_softmax_accuracy: 1.0000 - branch_softmax_1_accuracy: 1.0000 - branch_softmax_2_accuracy: 0.9998 - branch_softmax_evidence: 0.0590 - branch_softmax_mean_ev_succ: 0.5895 - branch_softmax_mean_ev_fail: 0.0000e+00 - branch_softmax_1_evidence: 0.0850 - branch_softmax_1_mean_ev_succ: 0.8495 - branch_softmax_1_mean_ev_fail: 4.6159e-07 - branch_softmax_2_evidence: 0.1753 - branch_softmax_2_mean_ev_succ: 1.7536 - branch_softmax_2_mean_ev_fail: 1.3920e-04 - val_loss: 5.8945 - val_classification_loss: 2.2130 - val_branch_softmax_loss: 1.6643 - val_branch_softmax_1_loss: 1.3540 - val_branch_softmax_2_loss: 0.6632 - val_classification_accuracy: 0.9563 - val_branch_softmax_accuracy: 0.7005 - val_branch_softmax_1_accuracy: 0.7456 - val_branch_softmax_2_accuracy: 0.8574 - val_branch_softmax_evidence: 0.0551 - val_branch_softmax_mean_ev_succ: 0.7488 - val_branch_softmax_mean_ev_fail: 0.0765 - val_branch_softmax_1_evidence: 0.0935 - val_branch_softmax_1_mean_ev_succ: 1.2244 - val_branch_softmax_1_mean_ev_fail: 0.0816 - val_branch_softmax_2_evidence: 0.2029 - val_branch_softmax_2_mean_ev_succ: 2.3436 - val_branch_softmax_2_mean_ev_fail: 0.1044\n",
      "\n",
      "Epoch 00017: saving model to models\\mobileNetv2_evidence_conv2d_30.hdf5\n",
      "Epoch 18/18\n",
      "1406/1406 [==============================] - 574s 389ms/step - loss: 2.1693 - classification_loss: 2.1626 - branch_softmax_loss: 5.8977e-04 - branch_softmax_1_loss: 0.0022 - branch_softmax_2_loss: 0.0039 - classification_accuracy: 0.9994 - branch_softmax_accuracy: 1.0000 - branch_softmax_1_accuracy: 1.0000 - branch_softmax_2_accuracy: 0.9997 - branch_softmax_evidence: 0.0588 - branch_softmax_mean_ev_succ: 0.5876 - branch_softmax_mean_ev_fail: 0.0000e+00 - branch_softmax_1_evidence: 0.0869 - branch_softmax_1_mean_ev_succ: 0.8692 - branch_softmax_1_mean_ev_fail: 1.9435e-07 - branch_softmax_2_evidence: 0.1746 - branch_softmax_2_mean_ev_succ: 1.7462 - branch_softmax_2_mean_ev_fail: 1.7762e-05 - val_loss: 5.9232 - val_classification_loss: 2.2137 - val_branch_softmax_loss: 1.6801 - val_branch_softmax_1_loss: 1.3557 - val_branch_softmax_2_loss: 0.6736 - val_classification_accuracy: 0.9553 - val_branch_softmax_accuracy: 0.7009 - val_branch_softmax_1_accuracy: 0.7462 - val_branch_softmax_2_accuracy: 0.8626 - val_branch_softmax_evidence: 0.0529 - val_branch_softmax_mean_ev_succ: 0.7203 - val_branch_softmax_mean_ev_fail: 0.0701 - val_branch_softmax_1_evidence: 0.0845 - val_branch_softmax_1_mean_ev_succ: 1.1052 - val_branch_softmax_1_mean_ev_fail: 0.0695 - val_branch_softmax_2_evidence: 0.1690 - val_branch_softmax_2_mean_ev_succ: 1.9402 - val_branch_softmax_2_mean_ev_fail: 0.0797\n",
      "\n",
      "Epoch 00018: saving model to models\\mobileNetv2_evidence_conv2d_30.hdf5\n",
      "<tensorflow.python.keras.callbacks.History object at 0x0000021A7F9293C8>\n",
      "312/312 - 31s - loss: 6.1029 - classification_loss: 2.2189 - branch_softmax_loss: 1.7580 - branch_softmax_1_loss: 1.3846 - branch_softmax_2_loss: 0.7415 - classification_accuracy: 0.9509 - branch_softmax_accuracy: 0.6871 - branch_softmax_1_accuracy: 0.7399 - branch_softmax_2_accuracy: 0.8466 - branch_softmax_evidence: 0.0516 - branch_softmax_mean_ev_succ: 0.7210 - branch_softmax_mean_ev_fail: 0.0678 - branch_softmax_1_evidence: 0.0835 - branch_softmax_1_mean_ev_succ: 1.0941 - branch_softmax_1_mean_ev_fail: 0.0962 - branch_softmax_2_evidence: 0.1680 - branch_softmax_2_mean_ev_succ: 1.9653 - branch_softmax_2_mean_ev_fail: 0.0937\n",
      "overall loss: 6.10290002822876\n"
     ]
    }
   ],
   "source": [
    "brevis.saveName=(\"mobileNetv2_evidence_conv2d_30\")\n",
    "model = brevis.trainTransfer(18, loss=loss_fn, optimizer=tf.optimizers.SGD(), transfer=False, customOptions=\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "model  = tf.keras.models.load_model('models/mobileNetv2_evidence_conv2d.hdf5', custom_objects={\"CrossEntropyEndpoint\":CrossEntropyEndpoint,\"cross_entropy_evidence\":loss_function()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1202/10000 [==>...........................] - ETA: 6:35 - loss: 0.2180 - accuracy: 0.9259- ETA: 4:40 - loss: 0.0120 - accu - ETA: 5:34 - loss: 0.0320 - accuracy: 1.00 - ETA: 5:49 - loss: 0.0304 - accuracy:  - ETA: 6:13 - loss: 0.0356 - accuracy: 1.00 - ETA: 6:22 - loss: 0.0359 - accuracy: 1. - ETA: 6:41 - loss: 0.0419 - accuracy: 1. - ETA: 6:57 - loss: 0.0390 - accuracy: 1.00 - ETA: 7:07 - loss: 0.0377 - accuracy: 1. - ETA: 7:19 - loss: 0.0 - ETA: 6:52 - loss: 0.1342 - accuracy: 0.95 - ETA: 6:57 - loss: 0.1315 - accuracy:  - ETA: 7:10 - loss: 0.1248 - accuracy:  - ETA: 7:20 - loss: 0.1204 - accuracy - ETA: 7:25 - loss: 0.2196 - accura - ETA: 7:38 - loss: 0.3047 - accuracy: 0.93 - ETA: 7:40 - loss: 0.3003  - ETA: 7:38 - loss: 0.3212 - accuracy: 0.91 - ETA: 7:41 - loss: 0.3172 - accuracy: 0.91 - ETA: 7:41 - loss: 0.3132 - accuracy: 0.91 - ETA: 7:44 - loss: 0.3095 - accuracy - ETA: 7:47 - loss: 0.3095 - accuracy - ETA: 7:45 - loss: 0.2940 - accu - ETA: 7:35 - loss: 0.2757 - accuracy: 0.92 - ETA: 7:36 - loss: 0.2730 - accuracy: 0. - ETA: 7:35 - loss: 0.2652 - accuracy: 0.92 - ETA: 7:37 - loss: 0.2627 - accuracy: 0.92 - ETA: 7:38 - loss: 0.2603 - accuracy:  - ETA: 7:09 - loss: 0.2206 - accuracy: 0.93 - ETA: 7:11 - los - ETA: 6:53 - loss: 0.2282 - accuracy: 0. - ETA: 6:51 - loss: 0.2245 - ac - - ETA: 6:49 - loss: 0.1976 - accuracy: 0. - ETA: 6:50 - los - ETA: 6:43 - loss: 0.2167 - accuracy: 0. - ETA: 6:45 - loss: 0.2149  - ETA: 6:47 - loss: 0.2061 - accuracy:  - ETA: 6:49 - loss: 0.2043 - accuracy: 0. - ETA: 6:51 - loss: 0.2028 - accuracy: 0. - ETA: 6:52 - loss: 0.2012 - accuracy:  - ETA: 6:54 - loss: 0.2094 - accura - ETA: 6:53 - loss: 0.2386 - accuracy: 0.91 - ETA: 6:53 - loss: 0.2377 - accuracy: 0.91 - ETA: 6:54 - loss: 0.2369 - accuracy - ETA: 6:56 - loss: 0.2439 - accuracy: 0.91 - ETA: 6:57 - loss: 0.2431 - accuracy: 0. - ETA: 6:58 - loss: 0.2414 - accuracy:  - ETA: 7:00 - loss: 0.2501 - accuracy: 0. - ETA: 7:01 - loss: 0.2497 - accuracy:  - ETA: 7:01 - loss: 0.2605 - ac - ETA: 6:58 - loss: 0.2528 - accuracy - ETA: 6:58 - loss: 0.2493 - accuracy - ETA: 7:00 - loss: 0.2535 - accuracy - ETA: 7:03 - loss: 0.2575 - accuracy:  - ETA: 7:03 - loss: 0.2543 - accuracy - ETA: 7:00 - loss: 0.2610 - accuracy: 0.91 - ETA: 7:00 - loss: 0.2603 - accuracy: 0. - ETA: 7:01 - loss: 0.2579 - accuracy: 0.91 - ETA: 7:02 - loss: 0.2571 - accu - ETA: 7:04 - loss: 0.2542 - accuracy: 0.91 - ETA: 7:04 - loss: 0.2535 - accuracy:  - ETA: 7:06 - loss: 0.2513 - accuracy: 0. - ETA: 7:06 - loss: 0.2503 - accuracy - ETA: 7:08 - loss: 0.2478 - accuracy: 0. - ETA: 7:09 - loss: 0.2470 - accuracy:  - ETA: 7:07 - loss: 0.2441 - accuracy:  - ETA: 7:09 - loss: 0.2466 - accura - ETA: 7:09 - loss: 0.2429 - accu - ETA: 7:11 - loss: 0.2425 - accuracy: 0. - ETA: 7:12 - loss: 0.2427 - accuracy:  - ETA: 7:13 - loss: 0.2409 - ac - ETA: 7:11 - loss: 0.2432 -  - ETA: 7:08 - loss: 0.2408 - accuracy: 0. - ETA: 7:07 - loss: 0.2398 - accuracy - ETA: 7:07 - loss: 0.2388 - accuracy: 0. - ETA: 7:06 - loss: 0.2365 - accuracy: 0.91 - ETA: 7:06 - loss: 0.2359 - accuracy: 0. - ETA: 7:07 - loss: 0.2350 - accuracy: 0.92 - ETA: 7:07 - loss: 0.2344 - accuracy:  - ETA: 7:08 - loss: 0.2344 - accuracy: 0. - ETA: 7:08 - loss: 0.2443 - accuracy: 0.91 - ETA: 7:08 - loss: 0.2439 - accuracy: 0.91 - ETA: 7:08 - loss: 0.2434 - accuracy: 0. - ETA: 7:09 - loss: 0.255 - ETA: 7:08 - loss: 0.2515 - accuracy:  - ETA: 7:08 - loss: 0.249 - ETA: 7:09 - loss: 0.2489 - accuracy: 0.92 - ETA: 7:10 - loss: 0.2484  - ETA: 7:11 - loss: 0.2446 - accuracy:  - ETA: 7:10 - loss: 0.2443 - accuracy: 0.92 - ETA: 7:11 - loss: 0.2438 - accuracy: 0.92 - ETA: 7:11 - loss: 0.2433 - accuracy:  - ETA: 7:12 - loss: 0.241 - ETA: 7:10 - loss: 0.2360 - accuracy:  - ETA: 7:09 - loss: 0.2336 - accuracy: 0.92 - ETA: 7:09 - loss: 0.2331 - accuracy: 0. - ETA: 7:09 - loss: 0.2317 - accura - ETA: 7:09 - loss: 0.2306 -  - ETA: 7:08 - loss: 0.2258 - accuracy: 0.92 - ETA: 7:08 - loss: 0.2254 - accuracy: 0. - ETA: 7:08 - loss: 0.2337  - ETA: 7:07 - loss: 0.2349 - accuracy: 0.92 - ETA: 7:07 - loss: 0.2344 - accuracy: 0. - ETA: 7:07 - loss: 0.2336 - accuracy: 0. - ETA: 7:07 - loss: 0.2343 - accuracy: 0.92 - ETA: 7:07 - loss: 0.2339 - accu - ETA: 7:08 - loss: 0.2311 - accuracy: 0. - ETA: 7:08 - loss: 0.2302 - accura - ETA: 7:09 - loss: 0.2355 - accuracy: 0.92 - ETA: 7:10 - loss: 0.2350 - accuracy - ETA: 7:11 - loss: 0.2334 - accuracy: 0.92 - ETA: 7:11 - - ETA: 7:14 - loss: 0.2263 - ac - ETA: 7:13 - loss: 0.222 - ETA: 7:13 - loss: 0.2224 - accuracy: 0.92 - ETA: 7:14 - loss: 0.2220 - accura - ETA: 7:13 - ETA: 7:06 - loss: 0.2166 - accuracy: 0.92 - ETA: 7:06 - loss: 0.2163 - accuracy:  - ETA: 7:06 - loss: 0.217 - ETA: 7:05 - loss: 0.2208 - accuracy: 0. - ETA: 7:05 - loss: 0.2199 - accura - ETA: 7:04 - loss: 0.2224 - accuracy - ETA: 7:03 - loss: 0.2208 - accuracy:  - ETA: 7:03 - loss: 0.2226 - accuracy: 0.92 - ETA: 7:04 - loss: 0 - ETA: 7:00 - loss: 0.2196 - accuracy: 0. - ETA: 6:59 - loss: 0.2187  - ETA: 6:56 - loss: 0.2161 - ac - ETA: 6:55 - loss: 0.2129  - ETA: 6:53 - loss: 0.2197 - accuracy: 0.92 - ETA: 6:54 - loss: 0.2207 - accuracy: 0.92 - ETA: 6:54 - loss: 0.2204 - accuracy: 0.92 - ETA: 6:54 - loss: 0.2201 - accura - ETA: 6:54 - loss: 0.2226 - accuracy: 0. - ETA: 6:53 - loss: 0.2215 - accuracy: 0.92 - ETA: 6:53 - loss: 0.2212 - accuracy: 0.92 - ETA: 6:53 - loss: 0.2209 - accuracy: 0. - ETA: 6:52 - loss: 0.2202 - accuracy:  - ETA: 6:52 - loss: 0.2192 - accuracy - ETA: 6:51 - loss: 0.2201 - accuracy: 0.92 - ETA: 6:51 - loss: 0.2198 - accuracy: 0. - ETA: 6:51 - loss: 0.228 - ETA: 6:48 - loss: 0.2234 - accuracy:  - ETA: 6:47 - loss: 0.2223 - accuracy: 0.92 - ETA: 6:47 - loss: 0.2224 - accuracy: 0.92 - ETA: 6:47 - loss: 0.222 - ETA: 6:45 - loss: 0.2300 - ac - ETA: 6:44 - loss: 0.2283 - accuracy:  - ETA: 6:44 - loss: 0.2274 - accuracy: 0.92 - ETA: 6:44 - loss: 0.2273 - accuracy: 0. - ETA: 6:44 - loss: 0.2267 - accuracy: 0.92 - ETA: 6:44 - loss: 0.2264 - accuracy: 0.92 - ETA: 6:44 - loss: 0.2261 - accuracy: 0. - ETA: 6:45 - loss: 0.2302 - accuracy: 0. - ETA: 6:44 - loss: 0.2 - ETA: 6:42 - loss: 0.2299 - accuracy: 0.92 - ETA: 6:42 - loss: 0.2297 - accuracy - ETA: 6:41 - loss: 0.2282 - accuracy: 0. - ETA: 6:41 - loss: 0.2275 - accuracy: 0.92 - ETA: 6:41 - loss: 0.2272 - accuracy: 0. - ETA: 6:42 - loss: 0.2267 - accuracy: 0. - ETA: 6:42 - loss: 0.2267 - accuracy - ETA: 6:41 - loss: 0.2255 - accuracy - ETA: 6:42 - loss: 0.2264 - accura - ETA: 6:42 - loss: 0.2280 - accuracy - ETA: 6:42 - loss: 0.2303 - accuracy:  - ETA: 6:42 - loss: 0.2296 - accuracy: 0. - ETA: 6:43 - loss: 0.2291 - ac - ETA: 6:42 - loss: 0.227 - ETA: 6:39 - loss: 0.2267 - accuracy - ETA: 6:38 - loss: 0.2250 - accuracy: 0. - ETA: 6:38 - loss: 0.224 - ETA: 6:37 - loss: 0.2240 - accuracy: 0.92 - ETA: 6:38 - loss: 0.2237  - ETA: 6:37 - loss: 0.2265 - accuracy: 0. - ETA: 6:37 - loss: 0.2261 - accuracy: 0.92 - ETA: 6:37 - loss: 0.2259 - accuracy:  - ETA: 6:38 - loss: 0 - ETA: 6:37 - loss: 0.2231 - accuracy - ETA: 6:37 - loss: 0 - ETA: 6:35 - loss: 0.2251 - accuracy:  - ETA: 6:36 - loss: 0.2247 - accuracy: 0. - ETA: 6:35 - loss: 0.2241 - accuracy: 0.92 - ETA: 6:35 - loss: 0.2238 - accuracy: 0.92 - ETA: 6:36 - loss: 0.2236 - accuracy: 0. - ETA: 6:36 - loss: 0.2232 - accuracy: 0.92 - ETA: 6:36 - loss: 0.2230 - accura - ETA: 6:35 - loss: 0.2221 - accuracy: 0.92 - ETA: 6:35 - loss: 0.2219 - accu - ETA: 6:35 - loss: 0.2201 - accuracy: 0. - ETA: 6:35 - loss: 0.2195 - accura - ETA: 6:34 - loss: 0.2207 - accuracy:  - ETA: 6:34 - loss: 0.2204 -  - ETA: 6:32 - loss: 0.2180 - accuracy: 0. - ETA: 6:32 - loss: 0.2193 - accuracy: 0.92 - ETA: 6:32 - loss: 0.2191 - ac - ETA: 6:32 - loss: 0.2194 - accuracy: 0. - ETA: 6:32 - l - ETA: 6:30 - - ETA: 6:30 - l - ETA: 6:32 - loss: 0.2185 - accuracy: 0.92 - ETA: 6:33 - loss: 0.2183 - accuracy: 0. - ETA: 6:33 - loss: 0.2215 - accuracy: 0.92 - ETA: 6:33 - loss: 0.2213 - accura - ETA: 6:34 - loss: 0.2203 - accuracy:  - ETA: 6:34 - loss: 0.2202 - accuracy: 0.92 - ETA: 6:34 - - ETA: 6:32 - loss: 0.2178 - accuracy:  - ETA: 6:33 - loss: 0.2174 - accu - ETA: 6:33 - loss: 0.2193 - accuracy: 0.92 - ETA: 6:33 - loss: 0.2191 - accu - ETA: 6:34 - loss: - ETA: 6:35 - loss: 0.2178 - accuracy: 0.9260 2312/10000 [=====>........................] - ETA: 6:05 - loss: 0.2328 - accuracy: 0.9238 ETA: 6:35 - loss: 0.2174 -  - E - ETA: 6:39 - loss: 0 - ETA: 6:41 - loss: 0.2147 - accuracy:  - ETA: 6:47 - loss: 0.216 - ETA: 6:48 - loss: 0.2177 - accuracy: 0. - ETA: 6:48 - loss: 0.2173 - accuracy: 0. - ETA: 6:49 - loss: 0.2171 - accuracy: 0.92 - ETA: 6:49 - loss: 0.2169 - accu - ETA: 6:49 - loss: 0.2158 - accuracy - ETA: 6:49 - l - ETA: 6:50 - loss: 0.2179 - accuracy - ETA: 6:51 - loss: 0.2177 - accuracy - ETA: 6:52 - loss: 0.2161 - accura - ETA: 6:53 - loss: 0.216 - ETA: 6:53 - loss: 0.2144 - accuracy: 0. - ETA: 6:53 - loss: 0.2141 - accuracy:  - ETA: 6:53 - loss: 0.2139 - accuracy: 0.92 - ETA: 6:53 - loss: 0.2162 - accuracy: 0. - ETA: 6:53 - loss: 0.2169 - accuracy:  - ETA: 6:54 - loss: 0.2226 - accuracy: 0. - ETA: 6:54 - loss: 0.2223 - accuracy:  - ETA: 6:53 - loss: 0.2228 - accuracy: 0.92 - ETA: 6:53 - loss: 0.2247 - accuracy: 0. - ETA: 6:53 - loss: 0.2244 - accura - ETA: 6:54 - loss: 0.2236 - accuracy: 0. - ETA: 6:54 - loss: 0.2233 - accuracy: 0.92 - ETA: 6:54 - loss: 0.2232 - accuracy: 0.92 - ETA: 6:54 - loss: 0.2230 - accura - ETA: 6:54 - loss: 0.2223 - accuracy - ETA: 6:55 - loss: 0.2215 - accuracy: 0.92 - ETA: 6:55 - loss: 0.2214 -  - ETA: 6:55 - loss: 0.2200 - accuracy: 0.92 - ETA: 6:55 - loss: 0.2199 - accura - ETA: 6:55 - loss: 0.2210 - accuracy:  - ETA: 6:55 - loss: 0.2252 - accuracy: 0.92 - ETA: 6:55 - loss: 0.2260 - accuracy:  - ETA: 6:55 - loss: 0.2256 - accuracy: 0. - ETA: 6:55 - loss: 0.2253 - accuracy: 0.92 - ETA: 6:55 - loss: 0.2251 - accuracy:  - ETA: 6:54 - loss: 0.2250 - accuracy:  - ETA: 6:55 - loss: 0.2246 - accuracy:  - ETA: 6:54 - loss: 0.2241 - accuracy: 0.92 - ETA: 6:54 - loss: 0.2240 - accuracy:  - ETA: 6:54 - loss: 0.2253 - accuracy: 0.92 - ETA: 6:54 - loss: 0.2252 - accuracy: 0.92 - ETA: 6:54 - loss: 0.2250 - accuracy: 0. - ETA: 6:55 - loss: 0.2247 - accura - ETA: 6:55 - loss: 0.2241 - accuracy: 0. - ETA: 6:55 - loss: 0.2240 - accuracy:  - ETA: 6:55 - loss: 0.2247 - accuracy:  - ETA: 6:55 - loss: 0.2242 - accuracy - ETA: 6:55 - loss: 0.2237 - accuracy:  - ETA: 6:55 - loss: 0.2232 - accuracy:  - ETA: 6:56 - loss: 0.2250 - accuracy:  - ETA: 6:56 - loss: 0.2273 - accuracy: 0.92 - ETA: 6:56 - loss: 0.2272 - accuracy:  - ETA: 6:56 - loss: 0.2282 - accuracy: 0. - ETA: 6:56 - loss: 0.2279 - accuracy: 0.92 - ETA: 6:56 - loss: 0.2278 - accuracy - ETA: 6:56 - loss: 0.2308 - accuracy:  - ETA: 6:56 - loss: 0.2302 - accuracy: 0. - ETA: 6:56 - loss: 0.2316 - accuracy:  - ETA: 6:56 - loss: 0.2313 - accura - ETA: 6:56 - loss: 0.2317 - accuracy: 0.92 - ETA: 6:55 - loss: 0.2314 - ac - ETA: 6:55 - loss: 0.2327 - accuracy: 0.92 - ETA: 6:55 - loss: 0.2326 - accuracy: 0. - ETA: 6:55 - loss: 0.2334 - accuracy: 0.92 - ETA: 6:55 - loss: 0.2332 - accuracy: 0. - ETA: 6:54 - loss: 0.2328 - accuracy: 0.92 - ETA: 6:54 - loss: 0.2327 - accuracy:  - ETA: 6:54 - loss: 0.2323 - accu - ETA: 6:55 - loss: 0.2322 - accuracy - ETA: 6:55 - loss: 0.2318 - accuracy: 0. - ETA: 6:55 - loss: 0.2327 - accuracy: 0.92 - ETA: 6:55 - loss: 0.2326 - accuracy: 0.92 - ETA: 6:55 - loss: 0.2324 - accuracy: 0. - ETA: 6:55 - loss: 0.2321 - accura - ETA: 6:55 - loss: 0.2315 - accuracy:  - ETA: 6:55 - loss: 0.2312 - accuracy:  - ETA: 6:54 - loss: 0.2313 - accuracy: 0. - ETA: 6:54 - loss: 0.2312 - accuracy:  - ETA: 6:54 - los - ETA: 6:53 - loss: 0.2303 - accu - ETA: 6:53 - loss: 0.2296 - accuracy: 0.92 - ETA: 6:53 - loss: 0.2297 - accu - ETA: 6:54 - loss: 0.2295 -  - ETA: 6:53 - loss: 0.2278 - accuracy: 0.92 - ETA: 6:53 - loss: 0.2278 - accuracy: 0. - ETA: 6:54 - loss: 0.2277 - accuracy: 0.92 - ETA: 6:54 - loss: 0.2275 - accuracy: 0.92 - ETA: 6:54 - loss: 0.2274 - accuracy:  - ETA: 6:54 - loss: 0.2271 - accuracy: 0.92 - ETA: 6:54 - loss: 0.2269 - accuracy:  - ETA: 6:54 - loss: 0.2264 - accuracy - ETA: 6:54 - loss: 0.2286 - accura - ETA: 6:54 - loss: 0.2285 - accu - ETA: 6:54 - loss: 0.2277 - accuracy: 0. - ETA: 6:54 - loss: 0.2297 - accuracy: 0.92 - ETA: 6:54 - loss: 0.2296 - accura - ETA: 6:54 - loss: 0.2289 - accura - ETA: 6:54 - loss: 0.2281  - ETA: 6:52 - loss: 0.2316 - accuracy:  - ETA: 6:51 - loss: 0.2313 - accuracy: 0. - ETA: 6:47 - loss: 0.2275 - accuracy - ETA: 6:46 - loss: 0.2 - ETA: 6:46 - loss: 0.2339 - accuracy:  - ETA: 6:45 - loss: 0.2334 - accura - ETA: 6:45 - loss: 0.2363 - accuracy: 0. - ETA: 6:45 - loss: 0.2359 - accuracy:  - ETA: 6:44 - loss: 0.2 - ETA: 6:42 - loss: 0.238 - ETA: 6:40 - loss: 0.2379 - accuracy:  - ETA: 6:40 - loss: 0.2376 - accuracy - ETA: 6:39 - loss: 0.2368 - accuracy: 0. - ETA: 6:39 - loss: 0.2367 - accuracy: 0. - ETA: 6:39 - loss: 0.2365 -  - ETA: 6:38 - loss: 0.2376 - accuracy:  - ETA: 6:37 - loss: 0.2371 - accuracy: 0. - ETA: 6:37 - loss: 0.2373 - accuracy: 0.92 - ETA: 6:37 - loss: 0.2372 - accuracy: 0.92 - ETA: 6:37 - loss: 0.2371 - accuracy: 0. - ETA: 6:36 - loss: - ETA: 6:35 - loss: 0.2354 - accuracy:  - ETA: 6:34 - loss: 0.2349 - accura - ETA: 6:34 - loss: 0.2344 - accuracy:  - ETA: 6:34 - l - ETA: 6: - ETA: 6:29 - loss: 0.2312 - ac - ETA: 6:27 - loss: 0.2360 - accuracy: 0. - ETA: 6:27 - loss: 0.2360 - accuracy: 0. - ETA: 6:26 - loss: 0.2357 - accura - ETA: 6:26 - loss: 0.2349 - accuracy:  - ETA: 6:25 - loss: 0.2352 - accuracy:  - ETA: 6:25 - loss: 0.2348 - accura - ETA: 6:25 - loss: 0.2355 - accuracy: 0. - ETA: 6:25 - loss: 0.2353 - accuracy:  - ETA: 6:25 - loss: 0.2349 - accuracy: 0. - ETA: 6:25 - loss: 0.2346 - accuracy - ETA: 6:25 - loss: 0.2342 - accura - ETA: 6:25 - loss: 0.2342 - accura - ETA: 6:25 - loss: 0.2337 - accuracy: 0. - ETA: 6:25 - loss: 0.2340 - accuracy - ETA: 6:25 - loss: 0.2341  - ETA: 6:24 - loss: 0.2383 - accuracy: 0. - ETA: 6:24 - loss: 0.2381 -  - ETA: 6:24 - loss: 0.237 - ETA: 6:23 - loss: 0.2358 - accuracy: 0. - ETA: 6:22 - loss: 0.2355 - ac - ETA: 6:22 - loss: 0.2345 - accuracy:  - ETA: 6:22 - loss: 0.2342 - accuracy:  - ETA: 6:21 - loss: 0.2342 - accuracy:  - ETA: 6:21 - loss: 0.2345 - accuracy: 0.92 - ETA: 6:21 - loss: 0.2344 - accuracy: 0.92 - ETA: 6:21 - loss: 0.2343 - accuracy: 0.92 - ETA: 6:21 - loss: 0.2342 - accuracy:  - ETA: 6:21 - loss: 0.2337 - accuracy: 0.92 - ETA: 6:21 - loss: 0.2335 - accuracy: 0.92 - ETA: 6:20 - loss: 0.2333 - accuracy: 0.92 - ETA: 6:20 - loss: 0.2335 - accuracy: 0.92 - ETA: 6:20 - loss: 0.2333 - accuracy: 0.92 - ETA: 6:20 - loss: 0.2331 - ac - ETA: 6:19 - loss: 0.2328 - accuracy: 0. - ETA: 6:18 - loss: 0.2324 - accuracy: 0.92 - ETA: 6:18 - loss: 0.2323 - accuracy: 0.92 - ETA: 6:18 - loss: 0.2320 - accura - ETA: 6:17 - loss: 0.2312 - accuracy:  - ETA: 6:17 - loss: 0.2340 - accuracy:  - ETA: 6:17 - loss: 0.2337 - accuracy:  - ETA: 6:17 - loss: 0.2334 - accuracy - ETA: 6:17 - loss: 0.2327 - ac - ETA: 6:16 - loss: 0.2363 - accuracy: 0.92 - ETA: 6:16 - loss: 0.2362 - accuracy: 0. - ETA: 6:16 - loss: 0.2363 - accuracy: 0. - ETA: 6:16 - loss: 0.2361 - accuracy - ETA: 6:16 - loss: 0.2355 - ac - ETA: 6:15 - loss: 0.2350 - accuracy: 0.92 - ETA: 6:15 - loss: 0.2349 - accuracy: 0. - ETA: 6:15 - loss: 0.2347 - accura - ETA: 6:15 - loss: 0.2351 - accuracy: 0. - ETA: 6:15 - loss: 0.2349 -  - ETA: 6:15 - loss: 0.2345 - accuracy: 0. - ETA: 6:15 - loss: 0.2347 - accuracy: 0.92 - ETA: 6:15 - loss: 0.2346 - accuracy: 0. - ETA: 6:15 - loss: 0.2343 - accuracy: 0.92 - ETA: 6:15 - loss: 0.2344 - accuracy: 0. - ETA: 6:15 - loss: 0.2341 - accuracy: 0. - ETA: 6:15 - loss: 0.2338 - accuracy:  - ETA: 6:14 - loss: 0.2333 - accuracy: 0. - ETA: 6:14 - loss: 0.2330 - accuracy:  - ETA: 6:13 - loss: 0.2325 - accuracy: 0. - ETA: 6:13 - loss: 0.2322 - accu - ETA: 6:12 - loss: 0.231 - ETA: 6:11 - loss: 0.2311 - accuracy - ETA: 6:10 - loss: 0.2306 - accu - ETA: 6:09 - loss: 0.2296 - accuracy:  - ETA: 6:09 - loss: 0.2293 - accuracy: 0. - ETA: 6:08 - loss: 0.2296 - accuracy: 0.92 - ETA: 6:08 - loss: 0.2294 - accuracy - ETA: 6:08 - loss: 0.2289 - accuracy: 0.92 - ETA: 6:08 - loss: 0.2288 - accuracy:  - ETA: 6:08 - loss: 0.2286 - accuracy:  - ETA: 6:07 - loss: 0.2282 - accuracy: 0. - ETA: 6:07 - loss: 0.2300 - accuracy - ETA: 6:07 - loss: 0.2297 - accuracy:  - ETA: 6:06 - loss: 0.2291 - accuracy: 0. - ETA: 6:06 - loss: 0.2290 - accuracy:  - ETA: 6:06 - loss: 0 - ETA: 6:05 - loss: 0.2325 - accuracy - ETA: 6:05 - loss: 0.2326 - accuracy: 0.9239 3690/10000 [==========>...................] - ETA: 4:42 - loss: 0.2330 - accuracy: 0.9238 ETA: 6:05 - loss: 0.2325 - accuracy: 0. - ETA: 6:04 - loss: 0.2321 - accu - ETA: 6:04 - los - ETA: 6:01 - loss: 0.2311 - accuracy:  - ETA: 6:01 - loss: 0.2307 - accuracy:  - ETA: 6:01 - loss: 0.2304 - accuracy: 0.92 - ETA: 6:01 - loss: 0.2 - ETA: 5:59 - loss: 0.2301 - accura - ETA: 5:58 - loss: 0.2301 - accura - ETA: 5:58 - loss: 0.2296 - accuracy: 0.92 - ETA: 5:58 - loss: 0.2295 - accuracy: 0.92 - ETA: 5:58 - loss: 0.2294 - accura - ETA: 5:57 - loss: 0.2325 - ac - ETA: 5:56 - loss: 0.2347 - accura - ETA: 5:56 - loss: 0.2342 - accuracy: 0. - ETA: 5:56 - - ETA: 5:53 - loss: - ETA: 5:52 - - ETA: 5:50 - loss: 0.2349 - accuracy: 0. - ETA: 5:50 - loss: 0.2346 - accuracy: 0. - ETA: 5:49 - loss: 0.2343 - accuracy - ETA: 5:49 - loss: 0.2366 - accu - ETA: 5:48 - loss: 0.2356 - accuracy:  - ETA: 5:47 - loss: 0.2360 - accuracy: 0.92 - ETA: 5:47 - loss: 0.2359 - accuracy: 0. - ETA: 5:47 - loss: 0.2356 - accuracy: 0.92 - ETA: 5:47 - loss: 0.2356 -  - ETA: 5:46 - loss: 0.2391 -  - ETA: 5:45 - loss: 0.2382 - accuracy - ETA: 5:45 - loss: 0.2377 - accuracy: 0.92 - ETA: 5:44 - loss: 0.2375 - accuracy: 0.92 - ETA: 5:44 - loss: 0.2378 - accuracy: 0.92 - ETA: 5:44 - loss: 0.2377 - accura - ETA: 5:43 - loss: 0.2373 - accuracy - ETA: 5:43 - loss: 0.2386 - accuracy: 0. - ETA: 5:43 - loss: 0.2389 - ac - ETA: 5:39 - loss: 0.2405 - accuracy: 0. - ETA: 5:39 - loss: 0.2403 - accura - ETA: 5:39 - loss: 0.2398 - accuracy:  - ETA: 5:39 - loss: 0.2394 - accuracy: 0.92 - ETA: 5:39 - loss: 0.2393 - accuracy: 0.92 - ETA: 5:39 - loss: 0.2407 - accuracy:  - ETA: 5:39 - loss: 0.2404 - accuracy: 0.92 - ETA: 5:39 - loss: 0.2403 - accuracy: 0. - ETA: 5:38 - loss: 0.2400 - accuracy: 0. - ETA: 5:38 - loss: 0.2399 - accuracy:  - ETA: 5:38 - loss: 0.2395 - accuracy: 0. - ETA: 5:38 - loss: 0.2393 - accuracy:  - ETA: 5:38 - loss: 0.2390 - accuracy - ETA: 5:38 - loss: 0.2386 - accuracy - ETA: 5:38 - loss: 0.2382 - accuracy:  - ETA: 5:38 - loss: 0.2380 - accuracy: 0.92 - ETA: 5:38 - loss: 0.2379 - accuracy: 0.92 - ETA: 5:38 - loss: 0.2381 - accuracy: 0. - ETA: 5:37 - loss: 0.2379 - accuracy: 0. - ETA: 5:37 - loss: 0.2385 - accuracy: 0.92 - ETA: 5:37 - loss: 0.2384 - accuracy: 0.92 - ETA: 5:37 - loss: 0.2383  - ETA: 5:36 - loss: 0.2369 - accuracy: 0. - ETA: 5:36 - loss: 0.2367 - accuracy - ETA: 5:36 - loss: 0.2363 - accura - ETA: 5:35 - loss: 0.2356 - accura - ETA: 5:34 - l - ETA: 5:34 - loss: 0.2344 - accuracy: 0.92 - ETA: 5:34 - loss: 0.2343 - accuracy:  - ETA: 5:34 - loss: 0.2340 - accuracy: 0.92 - ETA: 5:34 - loss: 0.2339 - accuracy: 0.92 - ETA: 5:34 - loss: 0.2339 -  - ETA: 5:33 - loss: 0.2353 - accuracy:  - ETA: 5:33 - loss: 0.2350 - accuracy:  - ETA: 5:33 - loss: 0.2346 - accuracy:  - ETA: 5:32 - loss: 0.2356 - accuracy: 0.92 - ETA: 5:33 - loss: 0.2355 - accuracy: 0.92 - ETA: 5:33 - loss: 0.2354 - accuracy: 0. - ETA: 5:32 - loss: 0.2354 - accuracy: 0.92 - ETA: 5:32 - loss: 0.2354 - accuracy: 0. - ETA: 5:32 - loss: 0.2352 - accuracy - ETA: 5:32 - loss: 0.2351  - ETA: 5:31 - l - ETA: 5:30 - loss: 0.2351 - accuracy: 0. - ETA: 5:30 - loss: 0.2354 - accuracy: 0. - ETA: 5:29 - loss: 0.2352 - accuracy - ETA: 5:29 - loss: 0.2354 - accuracy:  - ETA: 5:29 - - ETA: 5:28 - loss: 0.2355 -  - ETA: 5:27 - loss: 0.2346 - accuracy:  - ETA: 5:27 - loss: 0.2345 - accuracy:  - ETA: 5:27 - loss: 0.2342 - accuracy:  - ETA: 5:27 - loss: 0.2347 - accuracy: 0.92 - ETA: 5:27 - loss: 0.2346 - accuracy - ETA: 5:26 - loss: 0.2346 - accuracy - ETA: 5:26 - loss: 0.2344 - accuracy - ETA: 5:26 - loss: 0.2339 - accuracy:  - ETA: 5:26 - loss: 0.2337 - accuracy: 0.92 - ETA: 5:26 - loss: 0.2336 -  - ETA: 5:25 - loss: 0.2330 - accuracy:  - ETA: 5:25 - loss: 0.2326  - ETA: 5:23 - loss: 0.2322 - accuracy:  - ETA: 5:23 - loss: 0.2318 - accuracy: 0.92 - ETA: 5:23 - loss: 0.2318 -  - ETA: 5:22 - loss: 0.2313 - accuracy: 0.92 - ETA: 5:22 - loss: 0.2312 - accuracy: 0. - ETA: 5:22 - loss: 0.2309 - accu - ETA: 5:21 - loss: 0.2307 - accuracy: 0.92 - ETA: 5:21 - loss: 0.2306 - accuracy: 0.92 - ETA: 5:21 - loss: 0.2305 - accuracy: 0.92 - ETA: 5:21 - loss: 0.2304 -  - ETA: 5:19 - loss: 0.2314 - accuracy:  - ETA: 5:19 - loss: 0.2311 - accuracy: 0. - ETA: 5:19 - loss: 0.2308 - accuracy - ETA: 5:19 - loss: 0.2304 - accu - ETA:  - ETA: 5:16 - loss: 0.2319 - accuracy: 0. - ETA: 5:16 - loss: 0 - ETA: 5:14 - los - ETA: 5:13 - loss: 0.2334 - accuracy: 0.92 - ETA: 5:13 - loss: 0.2333 - accura - ETA: 5:12 - loss: 0.2346 - accuracy:  - ETA: 5:12 - loss: 0.2343 - accuracy: 0.92 - ETA: 5:12 - loss: 0.2342 - accuracy: 0.92 - ETA: 5:12 - loss: 0.2342 - accuracy: 0.92 - ETA: 5:12 - loss: 0.2341 - accu - ETA: 5:11 - loss: 0.2335 - accuracy: 0.92 - ETA: 5:11 - loss: 0.2334 - accuracy: 0.92 - ETA: 5:11 - loss: 0.2333 - ac - ETA: 5:08 - - ETA: 5:06 - loss: 0.2347 - accuracy:  - ETA: 5:06 - loss: 0.2345 - accuracy - ETA: 5:05 - loss: 0.2346 - accuracy:  - ETA: 5:05 - loss: 0.2344 - accuracy: 0. - ETA: 5:05 - loss: 0.2350 - accu - ETA: 5:05 - loss: 0.2344 - accuracy:  - ETA: 5:05 - loss: 0.2342 -  - ETA: 5:04 - loss: 0.2352 - accuracy:  - ETA: 5:04 - loss: 0.2350 - accuracy: 0. - ETA: 5:04 - loss: 0.2347 - accu - ETA: 5:04 - loss: 0.2343 - accuracy: 0. - ETA: 5:04 - loss: 0.2341 - accuracy:  - ETA: 5:04 - loss: 0.2338 - accuracy:  - ETA: 5:03 - loss: 0.2335 - accuracy:  - ETA: 5:03 - loss: 0.2332 - accu - ETA: 5:03 - loss: 0.2328 - accuracy: 0.92 - ETA: 5:03 - loss: 0.232 - ETA: 5:01 - loss: 0.2329 - accuracy - ETA: 5:01 - loss: 0.2328 - accuracy:  - ETA: 5:01 - loss: 0.2328 - accuracy:  - ETA: 5:01 - loss: 0.2327 - accuracy: 0. - ETA: 5:01 - loss: 0.2326 - accuracy: 0.92 - ETA: 5:01 - loss: 0.2325 - accuracy: 0.92 - ETA: 5:01 - loss: 0.2326 - accuracy: 0.92 - ETA: 5:01 - loss: 0.2326 - accuracy - ETA: 5:01 - loss: 0.2323 - accuracy: 0. - ETA: 5:01 - loss: 0.2322  - ETA: 5:01 - loss: 0.2315 - accuracy: 0.92 - ETA: 5:00 - loss: 0.2314 - accuracy: 0.92 - ETA: 5:00 - loss: 0.2313 - accuracy: 0. - ETA: 5:00 - loss: 0.2 - ETA: 4:59 - loss: 0.2330 - accu - ETA: 4:58 - loss: 0.2342 - accuracy: 0. - ETA: 4:58 - loss: 0.2 - ETA: 4:57 - loss: 0.2339 - accuracy: 0.92 - ETA: 4:57 - loss: 0.2338  - ETA: 4:57 - loss: 0.2348 - accuracy: 0.92 - ETA: 4:57 - loss: 0.2347 - accuracy: 0. - ETA: 4:57 - loss: 0.2352 - accuracy: 0. - ETA: 4:57 - loss: 0.2351 - accuracy: 0.92 - ETA: 4:57 - loss: 0.2350 - accuracy: 0. - ETA: 4:57 - loss: 0.2349 - accuracy: 0.92 - ETA: 4:57 - loss: 0.2348 - accuracy:  - ETA: 4:56 - loss: 0.2345 - accu - ETA: 4:56 - loss: 0.2343 - ac - ETA: 4:55 - loss: 0.2336 - accura - ETA: 4:54 - loss: 0.2334 - accuracy: 0.92 - ETA: 4:54 - loss: 0.2333 - accuracy: 0.92 - ETA: 4:54 - loss: 0.2332 - accuracy: 0. - ETA: 4:54 - loss: 0.2344 - accuracy:  - ETA: 4:54 - loss: 0.2365 - accuracy: 0.92 - ETA: 4:54 - loss: 0.2365 - accuracy: 0. - ETA: 4:54 - loss: 0.2376 - accuracy: 0.92 - ETA: 4:54 - loss: 0.2375 - accuracy:  - ETA: 4:53 - loss: 0.2373 - accuracy: 0.92 - ETA: 4:53 - loss: 0.2372 - accuracy: 0. - ETA: 4:53 - ETA: 4:52 - loss: 0.2369 - accuracy: 0.92 - ETA: 4:52 - loss: 0.2368 - accuracy:  - ETA: 4:52 - loss: 0.2366 - ac - ETA: 4:52 - loss: 0.2362 - accuracy: 0.92 - ETA: 4:51 - loss: 0.2361 - accuracy: 0. - ETA: 4:51 - loss: 0.2359 - accuracy: 0.92 - ETA: 4:51 - loss: 0.2358 - ac - ETA: 4:51 - loss: 0.2362 - accuracy:  - ETA: 4:51 - loss: 0.2360 - accuracy: 0. - ETA: 4:51 - loss: 0.2 - ETA: 4:50 - loss: 0.2357 - accura - ETA: 4:50 - loss: 0.2354 - accuracy - ETA: 4:50 - loss: 0.2352 -  - ETA: 4:49 - loss: 0.2359 - accuracy: 0.92 - ETA: 4:49 - loss: 0.2358 - accuracy: 0. - ETA: 4:49 - loss: 0.2356 - accuracy: 0. - ETA: 4:49 - loss: 0.2 - ETA: 4:48 - loss: 0.2350 - accuracy:  - ETA: 4:48 - loss: 0.2347 - accuracy:  - ETA: 4:47 - loss: 0.2343 - ac - ETA: 4:47 - loss: 0.2 - ETA: 4:46 - loss: 0.2361 - accuracy:  - ETA: 4:45 - loss: 0.2360 - ac - ETA: 4:45 - loss: 0.2355 - accuracy:  - ETA: 4:45 - loss: 0.2352 - accuracy:  - - ETA: 4:43 - loss: 0.2335 - accuracy: 0.92 - ETA: 4:42 - loss: 0.2333 - accuracy: 0.92 - ETA: 4:42 - loss: 0.2333 - accu - ETA: 4:42 - loss: 0.2331 - accuracy: 0.92 - ETA: 4:42 - loss: 0.2330 - accuracy: 0. - ETA: 4:42 - loss: 0.2329 - accuracy: 0.9238 4874/10000 [=============>................] - ETA: 3:54 - loss: 0.2281 - accuracy: 0.9255 ETA: 4:42 - loss: 0.2327 - accuracy:  - ETA: 4:42 - loss: 0.2331 - accuracy: 0.92 - ETA: 4:42 - loss: 0.2331 - accuracy: 0. - ETA: 4:42 - loss: 0.2330 - accuracy:  - ETA: 4:42 - loss: 0.2328 - accuracy: 0.92 - ETA: 4:42 - loss: 0.2327 - accuracy:  - ETA: 4:42 - loss: 0.2326 - accuracy: 0.92 - ETA: 4:42 - loss: 0.2326 - accuracy: 0.92 - ETA: 4:42 - loss: 0.2333 - accuracy:  - ETA: 4:42 - loss: 0.2331 - accuracy: 0.92 - ETA: 4:42 - loss: 0.2330 - accuracy:  - ETA: 4:42 - loss: 0.2329 - accuracy: 0. - ETA: 4:42 - loss: 0.2327 - accuracy: 0.92 - ETA: 4:42 - loss: 0.2327 - accuracy:  - ETA: 4:41 - loss: 0.2325 - accuracy: 0.92 - ETA: 4:41 - loss: 0.2324 - accuracy:  - ETA: 4:41 - loss: 0.2325 - accuracy - ETA: 4:41 - loss: 0.2324 - accu - ETA: 4:41 - loss: 0.2320 - accuracy: 0. - ETA: 4:41 - loss: 0.2319 - accuracy:  - ETA: 4:41 - loss: 0.2317 - accuracy: 0. - ETA: 4:41 - loss: 0.2316 - accu - ETA: 4:40 - loss: 0.2327 - accuracy: 0.92 - ETA: 4:40 - loss: 0.2326 - accuracy: 0.92 - ETA: 4:40 - loss: 0.2325  - ETA: 4:40 - loss: 0.2319 - accuracy: 0.92 - ETA: 4:40 - loss: 0.2319 - accuracy: 0. - ETA: 4:40 - loss: 0.2320  - ETA: 4:39 - loss: - ETA: 4:39 - loss: 0.2354 - accuracy:  - ETA: 4:39 - loss: 0.2353 -  - ETA: 4:38 - loss: 0.2347 - accuracy: 0. - ETA: 4:38 - loss: 0.2348 - ac - ETA: 4:38 - loss: 0.2346 - accuracy - ETA: 4:38 - loss: 0.2349 - accuracy: 0.92 - ETA: 4:38 - loss: 0.2348 - accuracy:  - ETA: 4:38 - loss: 0.2346 - accuracy:  - ETA: 4:38 - - E - ETA: 4:37 - loss: 0.2337 - accuracy: 0.92 - ETA: 4:37 - loss: 0.2336 - accuracy: 0. - ETA: 4:37 - loss: 0.2335 - accu - ETA: 4:37 - loss: 0.2333  - ETA: 4:37 - loss: 0.232 - ETA: 4:36 - loss: 0.2326 - accura - ETA: 4:36 - loss: 0.2329 - accuracy: 0. - ETA: 4:36 - loss: 0.2328 - accuracy: 0.92 - ETA: 4:36 - loss: 0.2327 - accuracy:  - ETA: 4:36 - loss: 0.2326 - accuracy - ETA: 4:36 - loss: 0.2323 - accuracy - ETA: 4:36 - loss: 0.2323 - accuracy: 0.92 - ETA: 4:36 - loss: 0.2322 - accuracy: 0.92 - ETA:  - ETA: 4:36 - loss: 0.2312 - accuracy: 0.92 - ETA: 4:36 - loss: 0.2311 - accu - ETA: 4:36 - loss: 0.2308 -  - ETA: 4:36 - loss: 0 - ETA: 4:35 - loss: 0.2304 - accu - ETA: 4:35 - loss: 0.2309 - accuracy: 0. - ETA: 4:35 - loss: 0.2307 - accuracy:  - ETA: 4:34 - loss: 0.2305 - accuracy: 0. - ETA: 4:34 - loss: 0.2310 - accura - ETA: 4:34 - loss: 0.2310 - accuracy: 0.92 - ETA: 4:34 - loss: 0.2309 - accuracy: 0. - ETA: 4:34 - loss: 0.2314 - accuracy: 0.92 - ETA: 4:34 - loss: 0.2313 - accuracy:  - ETA: 4:34 - loss: 0.2312 - accura - ETA: 4:34 - loss: 0.2309 - accuracy: 0. - ETA: 4:34 - loss: 0.2309 - accuracy: 0.92 - ETA: 4:34 - loss: 0.2308 - accuracy:  - ETA: 4:34 - loss: 0.2306 - accuracy: 0. - ETA: 4:34 - loss: - ETA: 4:33 - loss: 0.2301 - accuracy: 0. - ETA: 4:33 - loss: 0.2301 - accuracy - ETA: 4:33 - loss: 0.2299 - accuracy - ETA: 4:32 - loss: 0.2296 - accuracy: 0. - ETA: 4:32 - loss: 0.2302 - accuracy: 0.92 - ETA: 4:32 - loss: 0.2302 - accuracy: 0.92 - ETA: 4:32 - loss: 0.2301 - accuracy: 0.92 - ETA: 4:32 - loss: 0.2301 - accu - ETA: 4:32 - loss: 0.2302 -  - ETA: 4:32 - loss: 0.2309 - accuracy - ETA: 4:32 - loss: 0.2307 - accuracy: 0. - ETA: 4:31 - loss: 0.2305 - accuracy:  - ETA: 4:31 - loss: 0.2304 - accura - ETA: 4:31 - los - ETA: 4:30 - loss: 0.2303 - accu - ETA: 4:30 - loss: 0.2307 - accuracy: 0. - ETA: 4:30 - loss: 0.2307 - accuracy: 0.92 - ETA: 4:30 - loss: 0.2307 - accu - ETA: 4:29 - loss: 0.2304  - ETA: 4:29 - loss: 0.2314 - accuracy:  - ETA: 4:29 - loss: 0.2312 - accuracy: 0.92 - ETA: 4:29 - loss: 0.2312  - ETA: 4:28 - loss: 0.2309 - accuracy: 0. - ETA: 4:28 - loss: 0.2308 - accura - ETA: 4:28 - loss: 0.2309 - accuracy:  - ETA: 4:27 - loss: 0.2319 - accura - ETA: 4:27 - loss: 0.2315 - accuracy: 0. - ETA: 4:27 - loss: 0.2314 - accuracy:  - ETA: 4:27 - loss: 0.2312 - accuracy: 0.92 - ETA: 4:27 - loss: 0.2312 - accuracy - ETA: 4:26 - loss: 0.2313 - accu - ETA: 4:26 - loss: 0.2316 - accuracy:  - ETA: 4:26 - loss: 0.2315 - accuracy: 0.92 - ETA: 4:26 - loss: 0.2314 - accura - ETA: 4:26 - loss: 0.2316 - accuracy: 0.92 - ETA: 4:26 - loss: 0.2315 - accuracy: 0. - ETA: 4: - ETA: 4:24 - loss: 0.2303 - accura - ETA: 4:24 - loss: 0.2300 - accuracy:  - ETA: 4:24 - loss: 0.2299 - accuracy: 0.92 - ETA: 4:24 - loss: 0.2 - ETA: 4:24 - loss: 0.2301 - accuracy - ETA: 4:23 - loss: 0.2298 - accuracy: 0. - ETA: 4:23 - loss: 0.2297 - accuracy: 0. - ETA: 4:23 - loss: 0.2297 - accuracy - ETA: 4:23 - loss: 0.2295 - accuracy: 0.92 - ETA: 4:23 - loss: 0.2295 - accuracy:  - ETA: 4:23 - loss: 0.2293 - accu - ETA: 4:23 - loss: 0.2290 - accuracy: 0. - ETA: 4:23 - loss: 0.2290 - accuracy:  - ETA: 4:22 - loss: 0.2289 - accu - ETA: 4:22 - loss: 0.2285 -  - ETA: 4:21 - loss: 0.2310 - accuracy - ETA: 4:21 - loss: 0.2311 - accuracy: 0.92 - ETA: 4:21 - loss: 0.2310 - accu - ETA: 4:21 - loss: 0.2307 - accuracy:  - ETA: 4:21 - loss: 0.2305 - accu - ETA: 4:21 - loss: 0.2304 - accuracy: 0.92 - ETA: 4:20 - loss: 0.2303 - accuracy:  - ETA: 4:20 - loss: 0.2301 - accuracy - ETA: 4:20 - loss: 0.2299 -  - ETA: 4:19 - loss: 0.2293 - accu - ETA: 4:19 - loss: 0.2289 - accuracy: 0.92 - ETA: 4:19 - loss: 0.2288 - accuracy: 0.92 - ETA: 4:19 - loss: 0.2288 - accuracy: 0.92 - ETA: 4:19 - loss: 0.2288 - accura - ETA: 4:19 - loss: 0.2290 - accuracy:  - ETA: 4:18 - loss: 0.2287 - accura - ETA: 4:18 - loss: 0.2284 - accuracy: 0.92 - ETA: 4:18 - loss: 0.2284 - accuracy - ETA: 4:18 - loss: 0.2282 - accuracy: 0.92 - ETA: 4:18 - loss: 0 - ETA: 4:16 - loss: 0.2281 - accuracy:  - ETA: 4:16 - loss: 0.2278 - accuracy: 0.92 - ETA: 4:16 - loss: 0.2277  - ETA: 4:15 - l - ETA: 4:13 - loss: 0.2274 - accuracy: 0. - ETA: 4:13 - loss: 0.2275 - accuracy: 0.92 - ETA: 4:13 - loss: 0.2275 - accura - ETA: 4:12 - loss: 0.2272 - accura - ETA: 4:12 - loss: 0.2270 - accu - ETA: 4:11 - loss: 0.2270 - accuracy - ETA: 4:11 - loss: 0.2266 - accuracy:  - ETA: 4:11 - loss: 0.2265 - accuracy:  - ETA: 4:11 - loss: 0.2263 - accuracy: 0.92 - ETA: 4:11 - loss: 0.2263 - accuracy: 0.92 - ETA: 4:11 - loss: 0.2262 - ac - ETA: 4:10 - loss: 0.2258 - accura - ETA: 4:10 - ETA: 4:09 - loss: 0.2249 - accuracy - ETA: 4:08 - loss: 0.2252 - accuracy: 0.92 - ETA: 4:08 - loss: 0.2251 - accuracy:  - ETA: 4:08 - loss: 0.2250 - accuracy: 0. - ETA: 4:08 - loss: 0.2259 - accuracy: 0.92 - ETA: 4:08 - loss: 0.2258 -  - ETA: 4:08 - loss: 0.2254 - accuracy: 0. - ETA: 4:08 - loss: 0.2252 - accura - ETA: 4:07 - loss: 0.2250 - accuracy:  - ETA: 4:07 - loss: 0.2248 - accuracy: 0.92 - ETA: 4:07 - loss: 0.2253 - accuracy: 0.92 - ETA: 4:07 - loss: 0.2252 - accuracy:  - ETA: 4:07 - loss: 0.2258 - accuracy: 0. - ETA: 4:06 - loss: 0.2257 - accuracy:  - ETA: 4:06 - loss: 0.2255 - accuracy: 0. - ETA: 4:06 - loss: 0.2254 - accuracy: 0. - ETA: 4:06 - loss: 0.2253 - accuracy: 0.92 - ETA: 4:06 - loss: 0.2252 - accuracy: 0. - ETA: 4:06 - loss: 0.2251 - ac - ETA: 4:05 - loss: 0.2266 - accuracy: 0.92 - ETA: 4:05 - loss: 0 - ETA: 4:04 - loss: 0.2272 - ac - ETA: 4:04 - loss: 0.2275 - accuracy: 0.92 - ETA: 4:04 - loss: 0.2274 - accuracy: 0. - ETA: 4:03 - loss: 0.2273 - accura - ETA: 4:03 - loss: 0.227 - ETA: 4:03 - loss: 0.2269 - accuracy:  - ETA: 4:03 - loss: 0.2268 - accuracy: 0. - ETA: 4:03 - loss: 0.2267 - accuracy: 0.92 - ETA: 4:03 - loss: 0.2269 - accuracy: 0. - ETA: 4:03 - loss: 0.226 - ETA: 4:02 - loss: 0.2265 - accuracy: 0. - ETA: 4:02 - loss: 0.2264 - accura - ETA: 4:02 - loss: 0.2264 - accuracy - ETA: 4:02 - loss: 0.2262 - accuracy:  - ETA: 4:02 - loss: 0.2270 - accuracy: 0.92 - ETA: 4:02 - loss: 0.2269 - accuracy: 0. - ETA: 4:02 - loss: 0.2268 - accuracy:  - ETA: 4:02 - loss: 0.2267  - ETA: 4:01 - loss: 0.2269 - accuracy:  - ETA: 4:01 - loss: 0.2267 - accuracy: 0.92 - ETA: 4:01 - loss: 0.2267 - accuracy:  - ETA: 4:01 - loss: 0.2265 - ac - ETA: 4:01 - loss: 0 - ETA: 4:00 - loss: 0.2 - ETA: 4:00 - loss: 0.2297 - accuracy: 0.92 - ETA: 4:00 - loss: 0 - ETA: 3:59 - loss: 0.2293 - accuracy:  - ETA: 3:59 - loss: - ETA: 3:57 - loss: 0.2291 - accuracy: 0. - ETA: 3:57 - loss: 0.2290 - accuracy: 0.92 - ETA: 3:57 - loss: 0.2289 - accuracy: 0. - ETA: 3:57 - loss: - ETA: 3:54 - loss: 0.2282 - accuracy: 0. - ETA: 3:54 - loss: 0.2281 - accuracy: 0.9255 6052/10000 [=================>............] - ETA: 3:04 - loss: 0.2271 - accuracy: 0.9268 ETA: 3:54 - loss: 0.2283 - accuracy: 0.92 - ETA: 3:54 - loss: 0.2282  - ETA: 3:53 - loss: 0.2278 - accuracy - ETA: 3:53 - loss: 0.2278 - accuracy: 0. - ETA: 3:53 - loss: 0.2284 - accuracy:  - ETA: 3:53 - loss: 0 - ETA:  - ETA: 3:52 - loss: 0.2298 - ac - ETA: 3:52 - loss: 0.2298 - ac - ETA: 3:52 - loss: 0.2295 - accuracy:  - ETA - ETA: 3:51 - loss: 0.2314 - accuracy:  - ETA: 3:51 - loss: 0.2322 - accura - ETA: 3:51 - loss: - ETA: 3:50 - loss: 0.2318 - accuracy: 0. - ETA: 3:50 - loss: 0.2317  - ETA: 3:50 - loss: 0.2 - ETA: 3:50 - loss: - ETA: 3:49 - loss: 0.2302 - ac - ETA: 3:49 - loss: 0.2302 - accuracy: 0.92 - ETA: 3:49 - loss: 0.2302 - accuracy: 0.92 - ETA: 3:49 - loss: 0.2301 - accuracy:  - ETA: 3:49 - loss: 0.2300 - accura - ETA: 3:48 - loss: 0.2301 - accuracy:  - ETA: 3:48 - loss: 0.2302 - accuracy:  - ETA: 3:48 - loss: 0.2303 - accu - ETA: 3:48 - loss: 0.2299 - accuracy:  - ETA: 3:48 - loss: 0.2298 - ac - ETA: 3:48 - loss: 0.2295 - accuracy: 0.92 - ETA: 3:48 - loss: 0.2295 - accuracy: 0.92 - ETA: 3:48 - loss: 0.2294 - accuracy: 0.92 - ETA: 3:48 - loss: 0.2294 - accuracy:  - ETA: 3:47 - loss: 0.2293 - accuracy: 0. - ETA: 3:47 - loss: 0.2292 - accuracy - ETA: 3:47 - loss: 0.2290 - accuracy - ETA: 3:47 - loss: 0.2289 - accuracy:  - ETA: 3:47 - loss: 0.2287 - accuracy: 0.92 - ETA: 3:47 - loss: 0.2286 - accuracy: 0. - ETA: 3:47 - loss: 0.2285 - accuracy: 0. - ETA: 3:46 - loss: 0.2284 - accuracy: 0.92 - ETA: 3:46 - loss: 0.2284 - accuracy:  - ETA: 3:46 - loss: 0.2282 - accuracy: 0.92 - ETA: 3:46 - loss: 0.2282 - accuracy:  - ETA: 3:46 - loss: 0.2281 - accu - ETA: 3:46 - loss: 0.2 - ETA: 3:45 - loss: 0.2278 - ac - ETA: 3:45 - l - ETA: 3:45 - loss: 0.2283 - accuracy: 0. - ETA: 3:45 - loss: 0.2284 - accuracy: 0. - ETA: 3:45 - loss: 0.2283 - accuracy: 0.92 - ETA: 3:45 - loss: 0.2283 - accuracy:  - ETA: 3:45 - loss: 0.2282 - accuracy: 0. - ETA: 3:45 - loss: 0.2283  - ETA: 3:44 - loss: 0 - ETA: 3:44 - los - ETA: 3:43 - loss: 0.2305 - accuracy: 0.92 - ETA: 3:43 - loss: 0.2305 -  - ETA: 3:43 - loss: 0.2302  - ETA: 3:43 - loss: 0.2299 - accuracy: 0. - ETA: 3:43 - loss: 0.2298 - accuracy: 0. - ETA: 3:43 - loss: 0.2296 - accuracy - ETA: 3:42 - loss: 0.2294 - accuracy - ETA: 3:42 - loss: 0.2292 - accuracy: 0. - ETA: 3:42 - loss: 0.2291 - accuracy: 0. - ETA: 3:42 - loss: 0.2290 - accuracy - ETA: 3:42 - loss: 0.2289 - accuracy: 0. - ETA: 3:42 - loss: 0.2288  - ETA: 3:41 - loss: 0.2289 - accuracy: 0. - ETA: 3:41 - loss: 0.229 - ETA: 3:40 - l - ETA: 3:40 - loss: 0.2285 - accuracy: 0. - ETA: 3:40 - loss: 0.2285 - accuracy:  - ETA: 3:40 - loss: 0.2293 - accuracy - ETA: 3:39 - loss: 0.2291 -  - ETA: 3:39 - loss: 0.2288 - accuracy: 0.92 - ETA: 3:39 - loss: 0.2288 - accuracy: 0. - ETA: 3:39 - loss: 0.2287 - accuracy - ETA: 3:39 - loss: 0.2285 - accuracy:  - ETA: 3:39 - loss: 0.2284 - accu - ETA: 3:39 - loss: 0.2282 - accuracy: 0. - ETA: 3:39 - loss: 0.2281 - accuracy: 0.92 - ETA: 3:39 - loss: 0.2281 - accuracy:  - ETA: 3:38 - loss: 0.2282 - accuracy: 0.92 - ETA: 3:38 - loss: 0.2282 - accuracy: 0.92 - ETA: 3:38 - loss: 0.2281 - accuracy:  - ETA: 3:38 - loss: 0.2280  - ETA: 3:38 - loss: 0.2291 - accuracy:  - ETA: 3:38 - loss: 0.2289 - accuracy: 0. - ETA: 3:38 - loss: 0.2289 - accu - ETA: 3:37 - loss: 0.230 - ETA: 3:37 - loss: 0.2298 - accu - ETA: 3:36 - loss: 0.2303 - accu - ETA: 3:36 - loss: 0.2312 - accuracy: 0.92 - ETA: 3:36 - loss: 0.2311 - ac - ETA: 3:36 - loss: 0.2309 - accura - ETA: 3:36 - loss: 0.2310 - accuracy: 0.92 - ETA: 3:36 - loss: 0.2310 - accuracy: 0. - ETA: 3:35 - loss: 0.2311 - accuracy:  - ETA: 3:35 - loss: 0.2310 - accuracy: 0.92 - ETA: 3:35 - loss: 0.2311 - accuracy: 0.92 - ETA: 3:35 - loss: 0.2311 - accura - ETA: 3:35 - loss: 0.2309  - ETA: 3:34 - loss: 0.2304 - accuracy: 0.92 - ETA: 3:34 - loss: 0.2304 - accuracy: 0. - ETA: 3:34 - loss: 0.2303 - accuracy:  - ETA: 3:34 - loss: 0.2302 - accuracy:  - ETA: 3:34 - loss: 0.2301 - accuracy: 0. - ETA: 3:34 - loss: 0.2304 - accuracy: 0.92 - ETA: 3:34 - loss: 0.2304 - accura - ETA: 3:34 - loss: 0.2302 - accuracy: 0.92 - ETA: 3:34 - loss: 0.2301 - accuracy:  - ETA: 3:33 - loss: 0.2299 - accuracy: 0. - ETA: 3:33 - loss: 0.2298 - accuracy: 0.92 - ETA: 3:33 - loss: 0.2298 - accuracy: 0.92 - ETA: 3:33 - loss: 0.2298 - accuracy: 0.92 - ETA: 3:33 - loss: 0.2298 - accuracy: 0.92 - ETA: 3:33 - loss: 0.2297 - accuracy - ETA: 3:33 - loss: 0.2296 - accuracy: 0. - ETA: 3:33 - loss: 0.2294 - accu - ETA: 3:32 - loss: 0.2296 - accuracy: 0.92 - ETA: 3:32 - loss: 0.2295 - accura - ETA: 3:32 - loss: 0.2302 - accuracy - ETA: 3:32 - loss: 0.2301 - accuracy: 0.92 - ETA: 3:31 - loss: 0.2301 - accuracy:  - ETA: 3:31 - loss: 0.2299 - accuracy:  - ETA: 3:31 - loss: 0.2298 - accuracy: 0. - ETA: 3:31 - loss: 0.2299 - accuracy: 0. - ETA: 3:31 - loss: 0.2298 - accuracy: 0. - ETA: 3:31 - loss: 0.2299 - accuracy: 0. - ETA: 3:31 - loss: 0.2298 - accuracy:  - ETA: 3:31 - loss: 0.2 - ETA: 3:30 - loss: 0.2302 -  - ETA: 3:29 - loss: 0.229 - ETA: 3:28 - loss: 0.2302 - accuracy: 0. - ETA: 3:28 - loss: 0.2301 - accuracy: 0. - ETA: 3:28 - loss: 0.2300 - accura - ETA: 3:28 - los - ETA: 3:26 - loss: 0.2290 - accuracy - ETA: 3:26 - loss: 0.2288  - ETA: 3:25 - loss: 0.2 - ETA: 3:24 - loss: 0.2290 - accuracy: 0. - ETA: 3:24 - loss: 0.2289 - accuracy:  - ETA: 3:24 - loss: 0.2287 - accuracy:  - ETA: 3:24 - loss: 0.2289 - accuracy: 0. - ETA: 3:24 - loss: 0.2289 - accuracy: 0. - ETA: 3:23 - loss: 0.2288 - accuracy: 0.92 - ETA: 3:23 - loss: 0.2287 - ac - ETA: 3:23 - loss: 0.2289 - accuracy: 0.92 - ETA: 3:23 - loss: 0.2289 - accuracy: 0.92 - ETA: 3:23 - loss: 0.2289 - accuracy: 0.92 - ETA: 3:23 - loss: 0.2288 - accuracy: 0.92 - ETA: 3:23 - loss: 0.2289 - accuracy: 0. - ETA: 3:22 - loss: 0.2288 - accuracy: 0.92 - ETA: 3:22 - loss: 0.2287 - accuracy - ETA: 3:22 - loss: 0.2289 - accuracy - ETA: 3:21 - loss: 0.2286 - accuracy:  - ETA: 3:21 - loss: 0.2286 - ac - ETA: 3:20 - loss: 0.2284 - accuracy - ETA: 3:20 - loss: 0.2282 - accuracy:  - ETA: 3:20 - loss: 0.2280 - accuracy:  - ETA: 3:19 - loss: 0.2278 - ac - ETA: 3:19 - loss: 0.2275 - accuracy: 0.92 - ETA: 3:19 - loss: 0.2274 - accuracy:  - ETA: 3:18 - loss: 0.2272 - accuracy: 0.92 - ETA: 3:18 - loss: 0.2272 - accura - ETA: 3:18 - loss: 0 - ETA: 3:16 - loss: 0.2268 - accuracy: 0. - ETA: 3:16 - loss: 0.2268 - accuracy:  - ETA: 3:16 - loss: 0.2266 - accuracy:  - ETA: 3:16 - loss: 0.2274 - accuracy:  - ETA: 3:15 - loss: 0.2274 -  - ETA: 3:15 - loss: 0.2271 -  - ETA: 3:15 - loss: 0.2 - ETA: 3:14 - loss: 0.2269 - accuracy - ETA: 3:13 - loss: 0.2268 - accuracy: 0.92 - ETA: 3:13 - loss: 0.2267 - accuracy: 0.92 - ETA: 3:13 - loss: 0.2267  - ETA: 3:12 - loss: 0.2272 - accuracy: 0.92 - ETA: 3:12 - loss: 0.2272 - accuracy: 0.92 - ETA: 3:12 - loss: 0.2271 - accuracy:  - ETA: 3:12 - loss: 0.2271 - accuracy: 0. - ETA: 3:12 - loss: 0.2273 - accu - ETA: 3:12 - loss: 0.2273  - ETA: 3:11 - loss: 0.2272 - accuracy: 0.92 - ETA: 3:11 - loss: 0.2272 - accuracy - ETA: 3:10 - loss: 0.2276 - accuracy:  - ETA: 3:10 - loss: 0.2275 - accuracy:  - ETA: 3:10 - loss: 0.2274 - accuracy: 0. - ETA: 3:10 - loss: 0.2273 - accuracy: 0.92 - ETA: 3:10 - loss: 0.2273 - accuracy - ETA: 3:10 - loss: 0.2277 - accuracy - ETA: 3:09 - loss: 0.2275 - accuracy:  - ETA: 3:09 - loss: 0.2280 - accuracy: 0.92 - ETA: 3:09 - loss: 0.2 - ETA: 3:08 - loss: 0.2275 - accuracy: 0.92 - ETA: 3:08 - loss: 0.2283 -  - ETA: 3:08 - loss: 0.2280 - accuracy:  - ETA: 3:08 - loss: 0.2279 - accuracy:  - ETA: 3:08 - loss: 0.2278 - accuracy: 0.92 - ETA: 3:08 - loss: 0.2278 - accuracy:  - ETA: 3:07 - loss: 0.2281 - accuracy: 0.92 - ETA: 3:07 - loss: 0.2281 - accuracy: 0. - ETA: 3:07 - loss: 0.2279 - accuracy: 0. - ETA: 3:07 - loss: 0.2279 - accura - ETA: 3:07 - loss: 0.2279 - accuracy - ETA: 3:07 - loss: 0.2279 - ac - ETA: 3:07 - loss: 0.2280 - accura - ETA: 3:06 - loss: 0.2278 - accuracy: 0.92 - ETA: 3:06 - loss: 0.2278 - accuracy: 0. - ETA: 3:06 - loss: 0.2279 - accuracy: 0.92 - ETA: 3:06 - loss: 0.2278 - accura - ETA: 3:06 - loss: 0.2277 -  - ETA: 3:05 - loss: 0.2273 - accura - ETA: 3:05 - loss: 0.2274 - accuracy:  - ETA: 3:05 - loss: 0.2273 - accura - ETA: 3:04 - loss: 0.2276 - accuracy: 0.9266 7577/10000 [=====================>........] - ETA: 1:50 - loss: 0.2247 - accuracy: 0.9273 ETA: 3:04 - loss: 0.2274 - accuracy:  - ETA: 3:04 - loss: 0.2273 - ac - ETA: 3:04 - loss: 0.2275 - accuracy: 0. - ETA: 3:03 - loss: 0.2276 - ac - ETA: 3:03 - loss: 0.2283 - accuracy: 0.92 - ETA: 3:03 - loss: 0.2283 - accura - ETA: 3:03 - loss: 0.2281 - accuracy: 0.92 - ETA: 3:03 - loss: 0.2281 -  - ETA: 3: - ETA: 3:01 - loss: 0.2273 - accuracy:  - ETA: 3:01 - loss: 0.2271 - accuracy: 0. - ETA: 3:01 - loss: 0.2271 - accuracy: 0. - ETA: 3:01 - loss: 0.227 - ETA: 3:00 - loss: 0.2275 - accuracy:  - ETA: 2:59 - loss: 0.2282 - accuracy: 0.92 - ETA: 2:59 - loss: 0.2281  - ETA: 2:59 - loss: 0.228 - ETA: 2:58 - loss: 0.2290 - accuracy: 0. - ETA: 2:58 - loss: 0.2289 - accu - ETA: 2:57 - loss: 0.2286 - accura - ETA: 2:57 - loss: 0.2284 -  - ETA: 2:56 - loss: 0.2291 - accuracy: 0. - ETA: 2:56 - loss: 0.2291 -  - ETA: 2:56 - loss: 0.228 - ETA: 2:55 - loss: 0.2285 - accuracy: 0.92 - ETA: 2:55 - loss: 0.2284 - accuracy: 0. - ETA: 2: - ETA:  - ETA: 2:53 - loss: 0.2295 - accuracy: 0. - ETA: 2:53 - loss: 0.2294 - ac - ETA: 2:53 - los - ETA: 2:52 - loss: 0.2288 - accuracy - ETA: 2:52 - loss: 0.228 - ETA: 2:52 - loss: 0.2282 - accuracy: 0. - ETA: 2:52 - loss: 0.2283 - accuracy: 0.92 - ETA: 2:51 - loss: 0.2283 - accu - ETA: 2:51 - loss: 0.2281 -  - ETA: 2:51 - loss: 0.2278 - accuracy: 0.92 - ETA: 2:51 - loss: 0.2277 - accuracy: 0.92 - ETA: 2:51 - loss: 0.2277 -  - ETA: 2:51 - loss: 0.2274 - accuracy: 0.92 - ETA: 2:50 - loss: 0.2274 - ac - ETA: 2:50 - loss: 0.2273 - accu - ETA: 2:50 - loss: 0.2271 - accuracy:  - ETA: 2:50 - loss: 0.2271 - accu - ETA: 2:50 - loss: 0.2269 - accuracy:  - ETA: 2:49 - loss: 0.2269 - accuracy: 0.92 - ETA: 2:49 - loss: 0.2269 - accuracy:  - ETA: 2:49 - loss: 0.2268 - accuracy:  - ETA: 2:49 - loss: 0.2269 - accu - ETA: 2:49 - loss: 0.2267 - accuracy:  - ETA: 2:49 - loss: 0.2267 - accuracy:  - ETA: 2:49 - loss: 0.2269 - accuracy: 0.92 - ETA: 2:48 - loss: 0.2269 - accuracy: 0.92 - ETA: 2:48 - loss: 0.2268 - accuracy:  - ETA: 2:48 - loss: 0.2268 - accuracy: 0.92 - ETA: 2:48 - loss: 0.2267 - accuracy: 0.92 - ETA: 2:48 - loss: 0.2267 - accuracy:  - ETA: 2:48 - loss: 0.2274 - ac - ETA: 2:48 - loss: 0.2277 - accuracy: 0.92 - ETA: 2:48 - loss: 0.2276 - accuracy - ETA: 2:48 - loss: 0.2275 - accuracy: 0. - ETA: 2:47 - loss: 0.2275 - accuracy: 0. - ETA: 2:47 - loss: 0.2274 - accuracy:  - ETA: 2:47 - loss: 0.2273 - accuracy: 0.92 - ETA: 2:47 - loss: 0.2273 - accuracy - ETA: 2:47 - loss: 0.2275 - accura - ETA: 2:47 - loss: 0.2274 - accuracy: 0. - ETA: 2:47 - loss: 0.2273 - accuracy - ETA: 2:46 - loss: 0.2273 - accura - ETA: 2:46 - loss: 0.2270 - accuracy:  - ETA: 2:46 - loss: 0.227 - ETA: 2:45 - loss: 0.2267 - ac - ETA: 2:44 - loss: 0.2263 - accura - ETA: 2:44 - loss: 0.2267 - accuracy: 0.92 - ETA: 2:44 - loss: 0.2267 - accuracy: 0.92 - ETA: 2:44 - loss: 0.2267 - accuracy: 0. - ETA: 2:43 - loss: 0.2267 - accuracy: 0.92 - ETA: 2:43 - loss: 0.2267 - accuracy: 0. - ETA: 2:43 - loss: 0.2266 - accuracy: 0. - ETA: 2:43 - loss: 0.2265 - accuracy: 0.92 - ETA: 2:43 - loss: 0.2264 - ac - ETA: 2:42 - loss: 0.2270 - accuracy: 0. - ETA: 2:42 - loss: 0.2270 - accuracy - ETA: 2:42 - loss: 0.2268 - accuracy:  - ETA: 2:42 - loss: 0.2 - ETA: 2:41 - los - ETA: 2:39 - loss: 0.225 - ETA: 2:38 - loss: 0.2262 - accuracy: 0. - ETA: 2:38 - loss: 0.2261 -  - ETA: 2:38 - loss: 0.2257 - accuracy - ETA: 2:37 - loss: 0.2256 - accuracy: 0. - ETA: 2:37 - loss: 0.2255 - accuracy:  - ETA: 2:37 - loss: 0.2263 - accuracy - ETA: 2:37 - loss: 0.2262 - accuracy: 0. - ETA: 2:37 - loss: 0.2261 - accuracy: 0.92 - ETA: 2:37 - loss: 0 - ETA: 2:36 - loss: 0.2271 - ac - ETA: 2:35 - loss: 0.2268 - accuracy: 0.92 - ETA: 2:35 - loss: 0.2268 -  - ETA: 2:35 - loss: 0.2275 - accuracy: 0. - ETA: 2:35 - loss: 0.2274 - accuracy: 0.92 - ETA: 2:35 - loss: 0.2274 - accuracy - ETA: 2:34 - loss: 0.2272 - accuracy:  - ETA: 2:34 - loss: 0.227 - ETA: 2:33 - loss: 0.2266 - accuracy - ETA: 2:33 - loss: 0.2 - ETA: 2:31 - loss: 0.2258 -  - ETA: 2:31 - loss: 0.2256 - accuracy:  - ETA: 2:30 - loss: 0.2260 - ac - ETA: 2:30 - loss: 0.2259 - accuracy:  - ETA: 2:30 - loss: 0.2258 - accu - ETA: 2:29 - loss: 0.2256 - accuracy: 0.92 - ETA: 2:29 - loss: 0.2255 - accuracy: 0. - ETA:  - ETA: 2:28 - loss: 0.2257 - accuracy: 0. - ETA: 2:28 - loss: 0.2257 - accuracy:  - ETA: 2:27 - loss: 0.2256  - ETA: 2:27 - loss: 0.2258  - ETA: 2:24 - loss: 0.2258 - accuracy: 0. - ETA: 2:24 - loss: 0.2261 - accuracy: 0.92 - ETA: 2:24 - los - ETA: 2:22 - loss: 0.2258 - accuracy - ETA: 2:22 - loss: 0.2256 - accuracy:  - ETA: 2:22 - loss: 0.2257 - accuracy:  - ETA: 2:22 - loss: 0.2255 -  - ETA: 2:21 - loss: 0.2252 - accuracy: 0. - ETA: 2:21 - loss: 0.2252 - accuracy:  - ETA: 2:21 - loss: 0.2251 - accuracy: 0. - ETA: 2:21 - loss: 0.2251 - accuracy: 0.92 - ETA: 2:21 - loss: 0.2252 - accuracy: 0.92 - ETA: 2:21 - loss: 0.2256 - accuracy: 0. - ETA: 2:20 - loss: 0.2255 - accuracy: 0.92 - ETA: 2:20 - loss: 0.2255 - accuracy:  - ETA: 2:20 - loss: 0.2257 - accuracy:  - ETA: 2:20 - loss: 0.2 - ETA: 2:19 - loss: 0.2272 - accu - ETA: 2:19 - loss: 0.2269 - accuracy: 0.92 - ETA: 2:19 - loss: 0.2269 - accuracy: 0.92 - ETA: 2:19 - loss: 0.2269 - accuracy: 0. - ETA: 2:18 - loss: 0.2270 - accuracy - ETA: 2:18 - loss: 0.2278 - accuracy: 0. - ETA: 2:18 - loss: 0.2278 - accuracy: 0.92 - ETA: 2:18 - loss: 0.2277 - accuracy: 0. - ETA: 2:18 - loss: 0.2277 - accuracy: 0.92 - ETA: 2:18 - loss: 0.2277 - accuracy:  - ETA: 2:18 - loss: 0.2276 - accuracy: 0. - ETA: 2:18 - loss: 0.2276 - accuracy: 0.92 - ETA: 2:18 - loss: 0.2275 - accuracy: 0.92 - ETA: 2:18 - loss: 0.2275 - accuracy: 0.92 - ETA: 2:18 - loss: 0.2274 - accuracy: 0. - ETA: 2:17 - loss: 0.2275 - accuracy: 0. - ETA: 2:17 - loss: 0.2274 - ac - ETA - ETA: 2:15 - loss: 0 - ETA: 2:14 - loss: 0.2266 - accuracy: 0. - ETA: 2:14 - loss: 0.2265 - accuracy:  - ETA: 2:14 - loss: 0.2268 - accuracy - E - ETA: 2:10 - loss: 0.2272  - ETA: 2:09 - loss: 0.2271 - accuracy: 0.92 - ETA: 2:09 - loss: 0.2270 - accu - ETA: 2:08 - loss: 0.2267  - ETA: 2:08 - loss: 0.2264 - accuracy - ETA: 2:07 - loss: 0.226 - ETA: 2:07 - loss: 0.2260 - accura - ETA: 2:06 - loss: 0.2260 - accu - ETA: 2:06 - loss: 0.2257 - accuracy:  - ETA: 2:06 - - ETA: 2:04 - loss: 0.2254 - ac - ETA: 2:04 - loss: 0.2251 - accura - ETA: 2:03 - loss: 0.2249 - accuracy - ETA: 2:03 - loss: - ETA: 2:02 - loss: 0.2250 - accuracy - ETA: 2:02 - loss: 0.2248 - accuracy: 0. - ETA: 2:01 - loss: 0.2247 - accuracy:  - ETA: 2:01 - loss: 0.2247 - accura - ETA: 2:01 - loss: 0.2245 - accuracy: 0.92 - ETA: 2:01 - loss: 0.2244 - accuracy: 0.92 - ETA: 2:01 - loss: 0.2244 - accuracy: 0.92 - ETA: 2:01 - loss: 0.2243 - accuracy: 0.92 - ETA: 2:01 - loss: 0.2243 - accuracy: 0.92 - ETA: 2:01 - loss: 0.2243 -  - ETA: 2:00 - loss: 0.2 - ETA: 1:59 - loss: 0.2234 - accuracy:  - ETA: 1:59 - loss: 0.2234 - accuracy: 0.92 - ETA: 1:59 - loss: 0.2233 - accura - ETA: 1:58 - loss: 0.2237 - accuracy:  - ETA: 1:58 - loss: 0.2244 - accuracy: 0. - ETA: 1:58 - loss: 0.2243 -  - ETA: 1: - ETA: 1:56 - loss: 0.2244 - accuracy: 0.92 - ETA: 1:56 - loss: 0.2244 - accura - ETA: 1:56 - loss: 0.2242 - accuracy: 0. - ETA: 1:56 - loss: 0.2242 - accura - ETA: 1:55 - loss: 0.2246 - accuracy:  - ETA: 1:55 - loss: 0.2245 - accura - ETA: 1:55 - loss: 0.2245 - accuracy:  - ETA: 1:54 - loss: 0.2243 - accuracy - ETA: 1:54 - loss: 0.2243 - accura - ETA: 1:54 - loss: 0.2242 - accuracy:  - ETA: 1:54 - loss: 0.2241 - accuracy: 0.92 - ETA: 1:53 - loss: 0.2241 - accuracy: 0. - ETA: 1:53 - loss: 0.2240 - accuracy: 0.92 - ETA: 1:53 - loss: 0.2240 - accuracy: 0.92 - ETA: 1:53 - loss: 0.2240 - accuracy: 0.92 - ETA: 1:53 - loss: 0.2245 - accura - ETA: 1:53 - loss: 0.2245 - accuracy: 0.92 - ETA: 1:53 - loss: 0.2245 - accuracy: 0.92 - ETA: 1:53 - loss: 0.2244 - accura - ETA: 1:53 - loss: 0.2 - ETA: 1:52 - loss: 0.2244 - accura - ETA: 1:51 - loss: 0.2245 -  - ETA: 1:51 - loss: 0.2250 - accuracy: 0.92 - ETA: 1:51 - loss: 0.2249 - accuracy: 0.92 - ETA: 1:51 - loss: 0.2251 - accuracy: 0.92 - ETA: 1:51 - loss: 0.2251 - accuracy: 0.92 - ETA: 1:51 - loss: 0.2250 - accu - ETA: 1:50 - loss: 0.2248 - accuracy: 0. - ETA: 1:50 - loss: 0.2248 - accuracy:  - ETA: 1:50 - loss: 0.2247 - accuracy: 0.9273 9146/10000 [==========================>...] - ETA: 37s - loss: 0.2234 - accuracy: 0.9276  ETA: 1:50 - loss: 0.2246 - accuracy: 0. - ETA: 1:50 - loss: 0.2246 - accuracy - ETA: 1:50 - loss: 0.2244 - accuracy: 0.92 - ETA: 1:50 - loss: 0.2244 - accuracy:  - ETA: 1:49 - loss: 0.2249 - accuracy:  - ETA: 1:49 - loss: 0.2248 - accuracy: 0.92 - ETA: 1:49 - loss: 0.2248 - accuracy: 0.92 - ETA: 1:49 - loss: 0.2247 - accuracy:  - ETA: 1:49 - loss: 0.2248 - accuracy:  - ETA: 1:49 - loss: 0.2247 - accuracy:  - ETA: 1:49 - loss: 0.2250 - accuracy:  - ETA: 1:48 - loss: 0.2250 - accuracy:  - ETA: 1:48 - loss: 0.2255 - accuracy: 0.92 - ETA: 1:48 - loss: 0.2255 - accuracy: 0. - ETA: 1:48 - loss: 0.2254 - accuracy: 0. - ETA: 1:48 - loss: 0.2256 - accuracy: 0.92 - ETA: 1:48 - loss: 0.2256 - accuracy: 0.92 - ETA: 1:48 - loss: 0.2255 - accura - ETA: 1:47 - loss: - ETA: 1:46 - loss: 0.2259 - accuracy:  - ETA: 1:46 - loss: 0.2258 - accura - ETA: 1:45 - loss: 0.2256 -  - ETA: 1:45 - - ETA: 1:44 - loss: 0.2258 - accuracy: 0.92 - ETA: 1:44 - loss: 0.2258 - ac - ETA: 1:43 - loss: 0.2256 - accuracy - ETA: 1:43 - loss: 0.2254 - accuracy: 0. - ETA: 1:43 - loss: 0.2254  - ETA: 1:42 - loss: 0.2257 - accuracy:  - ETA - ETA: 1:40 - loss: - ETA: 1:39 - loss: 0.2259 - accuracy: 0. - ETA: 1:39 - loss: 0.2258 - accuracy: 0. - ETA: 1:39 - loss: 0.2257 - accuracy: 0. - ETA: 1:39 - loss: 0.2256 - accu - ETA: 1:39 - loss: 0.2256 - accuracy: 0. - ETA: 1:38 - loss: 0.2255 - accuracy: 0.92 - ETA: 1:38 - loss: 0.2255 - accuracy: 0.92 - ETA: 1:38 - loss: 0.2254 - accuracy - ETA: 1:38 - loss: 0.2253 - accuracy - ETA: 1:38 - loss: 0.2253 - accura - ETA: 1:37 - loss: 0.2251 - accuracy: 0.92 - ETA: 1:37 - loss: 0.2250 - accuracy: 0. - ETA: 1:37 - loss: 0.2255 - accuracy:  - ETA: 1:37 - l - ETA: 1:36 - loss: 0.2253 - accuracy: 0. - ETA: 1:35 - loss: 0.2252 - accuracy:  - ETA: 1:35 - loss: 0.2251 - accuracy: 0.92 - ETA: 1:35 - loss: 0.2253 -  - ETA: 1:32 - loss: 0.2 - ETA: 1:30 - loss: 0.2237 - accura - ETA: 1:30 - loss: 0.2235 - accuracy: 0.92 - ETA: 1:30 - loss: 0.2235 - accuracy: 0. - ETA: 1:29 - loss: 0.2235 - accura - ETA: 1:29 - loss: 0.2240 - accuracy: 0.92 - ETA: 1:29 - loss: 0.2240 - accuracy: 0. - ETA: 1:29 - loss: 0.2244 - accuracy: 0.92 - ETA: 1:29 - loss: 0.2244 - accuracy:  - ETA: 1:29 - loss: 0.2243 - accuracy:  - ETA: 1:28 - loss: 0.2242 - accuracy: 0. - ETA: 1:28 - loss: 0.2246 - accuracy: 0.92 - ETA: 1:28 - loss: 0.2245 - accuracy: 0. - ETA: 1:28 - loss: 0.2244 - accuracy - ETA: 1:28 - loss: 0.2243 - accuracy: 0. - ETA: 1:28 - loss: 0.2247 - accuracy: 0.92 - ETA: 1:28 - loss: 0.2246 - accuracy:  - ETA: 1:28 - loss: 0.2245 - accuracy:  - ETA: 1:27 - loss: 0.2244 - accuracy: 0. - ETA: 1:27 - loss: 0.2249 - accuracy: 0.92 - ETA: 1:27 - loss: 0.2249 - accuracy: 0. - ETA: 1:27 - loss: 0.2248 - accuracy: 0.92 - ETA: 1:27 - loss: 0.2247 - accuracy: 0. - ETA: 1:27 - loss: 0.2247 - accuracy: 0. - ETA: 1:27 - loss: 0.2247 - accuracy: 0.92 - ETA: 1:27 - los - ETA: 1:26 - loss: 0.2241 - accuracy:  - ETA: 1:25 - loss: 0.2240 - accuracy - ETA: 1:25 - loss: 0.2239 - accuracy - ETA: 1:25 - loss: 0.2237 - accuracy - ETA: 1:25 - loss: 0.2238 - accuracy:  - ETA: 1:24 - loss: 0.2241 - accuracy: 0.92 - ETA: 1:24 - loss: 0.2242 - accuracy - ETA: 1:24 - loss: 0.2240 - accuracy: 0. - ETA: 1:24 - loss: - ETA: 1:23 - loss: 0.2241 - ac - ETA: 1:21 - loss: 0.2243 - accu - ETA: 1:20 - loss: - ETA: 1:19 - loss: 0.2235 - accuracy - ETA: 1:19 - loss: 0.2235 - accuracy:  - ETA: 1:18 - loss: 0.2234  - ETA: 1:18 - loss: 0.2230 - accuracy: 0.92 - ETA: 1:18 - loss: 0.2229 - accuracy: 0.92 - ETA: 1:18 - loss: 0.2229 - accuracy: 0.92 - ETA: 1:18 - loss: 0.2229 - accuracy: 0.92 - ETA: 1:17 - loss: 0.2229 - accuracy: 0. - ETA: 1:17 - loss: 0.2229 - ac - ETA: 1:17 - loss: 0.2230 - accuracy:  - ETA: 1:17 - loss: 0.2230 - accuracy: 0. - ETA: 1:16 - loss: 0.2229 - accuracy: 0. - ETA: 1:16 - loss: 0.2228 - ac - ETA: 1:16 - loss: 0.2229 - accuracy: 0.92 - ETA: 1:16 - loss: 0.2230 - accuracy:  - ETA: 1:16 - loss: 0.2228 - accuracy - ETA: 1:15 - loss: 0.2227 - accuracy: 0.92 - ETA: 1:15 - loss: 0.2227 - accura - ETA: 1:15 - loss: 0.2229 - ac - ETA: 1:15 - loss: 0.2227 - accuracy:  - ETA: 1:14 - loss: 0.2226 - accuracy:  - ETA: 1:14 - loss: 0.2228 - accuracy: 0. - ETA: 1:14 - loss: 0.2227 - accuracy: 0.92 - ETA: 1:14 - loss: 0.2227 - accuracy: 0. - ETA: 1:14 - loss: 0.2226 - accu - ETA: 1:14 - loss: 0.2224 - accuracy: 0.92 - ETA: 1:14 - loss: 0.2224 - accuracy: 0.92 - ETA: 1:13 - loss: 0.2224 - accuracy: 0.92 - ETA: 1:13 - loss: 0.2223 - accuracy:  - ETA: 1:13 - loss: 0.2223 - accuracy:  - ETA: 1:13 - loss: 0.2222 - accuracy: 0.92 - ETA: 1:13 - loss: 0.2225 - accuracy: 0. - ETA: 1:13 - loss: 0.2225 - accuracy:  - ETA: 1:13 - loss: 0.2226  - ETA: 1:12 - loss: 0.2224 - accuracy: 0.92 - ETA: 1:12 - loss: 0.2223 - accuracy: 0. - ETA: 1:12 - loss: 0.2223 - accuracy: 0.92 - ETA: 1:12 - loss: 0.2223 - accuracy:  - ETA: 1:12 - loss: 0.2221 - accu - ETA: 1:11 - loss: 0.2225 - accura - ETA: 1:11 - loss: 0.2223 - accuracy: 0.92 - ETA: 1:11 - loss: 0.2223 - accuracy:  - ETA: 1:11 - loss: 0.2221 - accuracy: 0. - ETA: 1:10 - loss: 0.2221 - accuracy: 0.92 - ETA: 1:10 - loss: 0.2220 - accuracy - ETA: 1:10 - loss: 0.2219 - accuracy: 0. - ETA: 1:10 - loss: 0.2220 - accuracy: 0. - ETA: 1:10 - loss: - ETA: 1:09 - loss: 0.221 - ETA: 1:09 - loss: - ETA: 1:08 - loss: 0.2219 - accura - ETA: 1:07 - loss: 0.2221 - accuracy: 0. - ETA: 1:06 - loss: 0.2223 - accuracy - ETA: 1:05 - loss: 0.2225 - accura - ETA: 1:05 - loss: 0.2227 - accuracy: 0. - ETA: 1:05 - loss: 0.2227 - accu - ETA: 1:05 - loss: 0.2227 - accuracy - ETA: 1:05 - loss: 0.2228 - accuracy:  - ETA: 1:04 - loss: 0.2228 - ac - ETA: 1:04 - loss: 0.2226 - accuracy:  - ETA: 1:03 - loss: 0.2228 - accuracy: 0. - ETA: 1:03 - loss: 0.2228 - accuracy: 0.92 - ETA: 1:03 - loss: 0.2232 - accuracy: 0.92 - ETA: 1:03 - l - ETA: 1:02 - loss: 0.2233 - accura - ETA: 1:02 - loss: 0.2232 - accuracy: 0. - ETA: 1:02 - loss: 0.2231 - accu - ETA: 1:01 - loss: 0.2235 - accuracy: 0. - - ETA: 1:00 - loss: 0.2233 - a - ETA: 59s - loss: 0.2229  - ETA: 58s - loss: 0.2225 - accuracy:  - ETA: 57s - loss: 0.2225 - accuracy: 0.9 - ETA: 57s - loss: 0.2223 - accuracy: 0 - ETA: 56s - loss: 0.2226 - accura - ETA: 56s - loss: 0.2227 - accuracy: 0.92 - ETA: 56s - loss: 0.2228 - accuracy: 0.927 - ETA: 56s - loss: 0.2228 - accuracy: 0. - ETA: 55s - loss: 0.2228 - accuracy - ETA: 55s - loss: 0.2230 - accuracy: 0.92 - ETA: 54s - loss: 0.2229 - accuracy - ETA: 54s - loss: 0.2233 - accuracy: 0.927 - ETA: 54s - loss: 0.2233 - accuracy - ETA: 53s - loss: 0.2232 - ac - ETA: 52s - loss: 0.2233 - accuracy: 0.927 - ETA: 52s - loss: 0.2233 - accuracy: 0 - ETA: 52s - loss: 0.2231 - accuracy: 0.92 - ETA: 52s - loss: 0.2231 - acc - ETA: 51s - loss: 0.2229 - accuracy: 0.9 - ETA: 51s - loss: 0.2228 - accurac - ETA: 50s - loss: 0.2227 - accuracy: 0.9 - ETA: 50s - loss: 0.2226 - accuracy: 0.927 - ETA: 50s - loss: 0.2225 - accuracy: 0.9 - ETA: 50s - loss: 0.2232 - accuracy: 0.927 - ETA: 50s - loss: 0.2232 - accuracy: 0.92 - ETA: 49s - loss: 0.2232 - accuracy: 0.927 - ETA: 49s - loss: 0.2231 - accuracy: 0. - ETA: 49s - loss: 0.2231 - accuracy: 0. - ETA: 49s - loss: 0.2229 - accuracy: 0.92 - ETA: 49s - loss: 0.2229 - accuracy: - ETA: 48s - loss: 0.2231 - accuracy: 0.9 - ETA: 48s - loss: 0.2230 - accuracy: 0 - ETA: 48s - loss: 0.2232 - accuracy: 0.92 - ETA: 48s - loss: 0.2232 - accuracy: 0.9 - ETA: 47s - loss: 0.2231 - accuracy: - ETA: 47s - loss: 0.2228 - accur - ETA: 46s - loss: 0.2237 - accuracy: 0. - ETA: 46s - loss: 0.2238 - accuracy: - ETA: 45s - loss: 0.2240 - accuracy: 0 - ETA: 45s - loss: 0.2245 - accuracy: 0.9 - ETA: 44s - loss: 0.2243 - accuracy:  - ETA: 44s - loss: 0.2242 - a - ETA: 43s - loss: 0.2239 - accur - ETA: 42s - loss: 0.2237 - accuracy - ETA: 41s - loss: 0.2238 - accuracy: 0.92 - ETA: 41s - loss: 0.2238 - accuracy: 0. - ETA: 41s - loss: 0.2236 - accur - ETA: 40s - loss: 0.2236 - accuracy: 0.927 - ETA: 40s - loss: 0.2236 - accuracy: 0.92 - ETA: 40s - loss: 0.2235  - ETA: 39s - loss: 0.2233 - accuracy: 0 - ETA: 38s - loss: 0.2232 - accuracy: 0.9 - ETA: 38s - loss: 0.2236 - accuracy: 0.92 - ETA: 38s - loss: 0.2235 - accuracy:  - ETA: 37s - loss: 0.2233 - accuracy: 0.927610000/10000 [==============================] - 438s 44ms/step - loss: 0.2230 - accuracy: 0.9269 37s - loss: 0.2236 - accuracy: 0.92 - ETA: 37s - loss: 0.2236 - accuracy: 0.927 - ETA: 37s - loss: 0.2235 - accuracy: 0.927 - ETA: 37s - loss: 0.2235 - accuracy: 0.927 - ETA: 37s - loss: 0.2235 - accuracy: 0.927 - ETA: 37s - loss: 0.2234 - accuracy: 0.92 - ETA: 37s - loss: 0.2235 - accuracy: 0. - ETA: 36s - loss: 0.2233 - accuracy: 0.927 - ETA: 36s - loss: 0.2233 - accuracy: 0.9 - ETA: 36s - loss: 0.2232 - accuracy: 0.92 - ETA: 36s - loss: 0.2231 - accuracy: 0.92 - ETA: 36s - loss: 0.2231 - accuracy: 0.927 - ETA: 36s - loss: 0.2230 - accuracy: 0.927 - ETA: 36s - loss: 0.2230 - accuracy: 0.927 - ETA: 36s - loss: 0.2230 - accurac - ETA: 35s - loss: 0.2227 - accuracy: 0.92 - ETA: 35s - loss: 0.2227 - accuracy: 0.927 - ETA: 35s - loss: 0.2226 - accuracy: 0.927 - ETA: 35s - loss: 0.2226 - accuracy: 0.927 - ETA: 35s - loss: 0.2226 - accuracy: 0.92 - ETA: 35s - loss: 0.2226 - accuracy: 0.92 - ETA: 35s - loss: 0.2225 - accuracy: 0.9 - ETA: 35s - loss: 0.2229 - accuracy: 0.927 - ETA: 34s - loss: 0.2229 - accur - ETA: 34s - loss: 0.2232 - accuracy: 0.92 - ETA: 34s - loss: 0.2240 - accu - ETA: 33s - loss: 0.2244 - accuracy: 0.927 - ETA: 33s - loss: 0.2243 - accuracy: 0.927 - ETA: 33s - loss: 0.2243 - accuracy: 0.92 - ETA: 33s - loss: 0.2242 - accuracy: 0.9 - ETA: 32s - loss: 0.2242 - accuracy: 0.92 - ETA: 32s - loss: 0.2242 - accuracy: 0. - ETA: 32s - loss: 0.2243 - accuracy: - ETA: 32s - loss: 0.2242 - accuracy: 0.927 - ETA: 31s - loss: 0.2242 - accuracy: 0.92 - ETA: 31s - loss: 0.2241 - accurac - ETA: 31s - loss: 0.2240 - accuracy: 0. - ETA: 30s - loss: 0.2240 - accuracy: 0.927 - ETA: 30s - loss: 0.2240 - accuracy: 0.927 - ETA: 30s - loss: 0.2240 - accuracy: 0. - ETA: 30s - loss: 0.2240 - accuracy: 0.927 - ETA: 30s - loss: 0.2240 - accuracy: - ETA: 30s - loss: 0.2237 - accuracy: 0.92 - ETA: 29s - loss: 0.2236 - accuracy: 0.927 - ETA: 29s - loss: 0.2236 - accuracy: 0 - ETA: 29s - loss: 0.2235 - accuracy: - ETA: 29s - loss: 0.2234 - accuracy: 0.92 - ETA: 29s - loss: 0.2234 - accuracy: 0 - ETA: 28s - loss: 0.2232 - accuracy: 0.927 - ETA: 28s - loss: 0.2232 - accuracy: 0.927 - ETA: 28s - loss: 0.2232 - accuracy: 0.92 - ETA: 28s - loss: 0. - ETA: 26s - loss: 0.2239 - accuracy: 0.927 - ETA: 26s - loss: 0.2238 - accuracy: 0.92 - ETA: 26s - loss: 0.2238 - accuracy: 0.92 - ETA: 26s - loss: 0.2238 - accuracy: 0.9 - ETA: 26s - loss: 0.2237 - accuracy: 0.927 - ETA: 26s - loss: 0.2237 - accuracy: 0 - ETA: 25s - loss: 0.2237 - accuracy: 0.927 - ETA: 25s - loss: 0.2236 - accuracy: 0. - ETA: 25s - loss: 0.2238 - accuracy: 0.9 - ETA: 25s - loss: 0.2237 - accuracy: 0.92 - ETA: 25s - loss: 0.2238 - accuracy: 0. - ETA: 24s - loss: 0.2238 - accur - ETA: 24s - loss: 0.2235 - accuracy:  - ETA: 23s - loss: 0.2233 - accuracy: 0.927 - ETA: 23s - loss: 0.2233 - accuracy: - ETA: 23s - loss: 0.2240 - accuracy: 0.926 - ETA: 23s - loss: 0.2240 - accuracy: 0.9 - ETA: 22s - loss: 0.2238 - accuracy: 0.926 - ETA: 22s - loss: 0.2239 - accuracy: 0.92 - ETA: 22s - loss: 0.2239 - accuracy: 0 - ETA: 22s - loss: 0.2237 - accuracy: 0. - ETA: 21s - loss: 0.2236 - accuracy: 0.9 - ETA: 21s - loss: 0.2239 - accuracy: - ETA: 21s - loss: 0.2243 - accuracy:  - ETA: 20s - loss: 0.2242 - accuracy: 0.92 - ETA: 20s - loss: 0.2241 - accuracy: 0.926 - ETA: 20s - loss: 0.2240 - accuracy: 0 - ETA: 19s - loss: 0.2238 - accuracy: - ETA: 19s - loss: 0.2237 - accuracy: 0.9 - ETA: 19s - loss: 0.2241 -  - ETA: 17s - loss: 0.2239 - accuracy: 0.92 - ETA: 17s - loss: 0.2239 - accuracy: 0.9 - ETA: 17s - loss: 0.2238 - accuracy:  - ETA: 17s - loss: 0.2236 - accuracy: - ETA: 16s - loss: 0.2237 - accu - ETA: 15s - loss: 0.2236 - accuracy: - ETA: 15s - loss: 0.2234 - accuracy: 0.9 - ETA: 15s - loss: 0.2234 - accuracy: 0.927 - ETA: 15s - loss: 0.2236 - accuracy: 0.92 - ETA: 14s - loss: 0.2235 - accuracy: 0.926 - ETA: 14s - loss: 0.2235 - accuracy: - ETA: 14s - loss: 0.2233 - accuracy: 0.9 - ETA: 14s - loss: 0.2232 - accuracy: 0. - ETA: 13s - loss: 0.2230 - accuracy: 0.927 - ETA: 13s - loss: 0.2230 - accuracy: 0.92 - ETA: 13s - loss: 0.2229 - accurac - ETA: 13s - loss: 0.2231 - accuracy: 0.9 - ETA: 12s - loss: 0.2234 - accuracy: 0. - ETA: 12s - loss: 0.2232 - accuracy: 0.926 - ETA: 12s - loss: 0.2232 - accuracy: 0.9 - ETA: 12s - loss: 0.2231 - accuracy: 0.927 - ETA: 12s - loss: 0.2231 - accuracy: 0.927 - ETA: 12s - loss: 0.2231 - accuracy: 0.927 - ETA: 12s - loss: 0.2230 - accuracy: 0.92 - ETA: 11s - loss: 0.2229 - accuracy: 0.92 - ETA: 11s - loss: 0.2233 - accuracy: - ETA: 11s - loss: 0.2233 - accuracy: 0.926 - ETA: 11s - loss: 0.2233 - accuracy:  - ETA: 10s - loss: 0.2232 - accuracy - ETA: 10s - loss: 0.2230 - accuracy: 0.92 - ETA: 10s - loss: 0.2229 - accuracy: 0.926 - ETA: 10s - loss: 0.2229 - accuracy: 0 - ETA: 9s - loss: 0.2228 - accuracy: 0.92 - ETA: 9s - loss: 0.2228 - accuracy: 0. - ETA: 9s - loss: 0.2227 - accuracy - ETA: 9s - loss: 0.2227 - accuracy: 0.92 - ETA: 9s - loss: 0.2227 - accuracy: 0.92 - ETA: 9s - loss: 0.2228 - accuracy - ETA: 9s - loss: 0.2231 -  - ETA: 8s - loss: 0.2230 - ac - ETA: 7s - loss: 0.2235 - accuracy: 0. - ETA: 7s - loss: 0.2234 - ac - ETA: 7s - loss: 0.2233 - accuracy: 0.92 - ETA: 6s - loss: 0.2236 - accuracy - ETA: 6s - loss: 0.2235 - accuracy - ETA: 6s - loss: 0.2233  - ETA: 3s - loss: 0.2226 - accuracy: 0. - ETA: 3s - loss: 0.2225 - accura - ETA: 3s - loss: 0.2225 - accuracy: 0.92 - ETA: 3s - loss: 0.2225 - accuracy - ETA: 3s - loss: 0.2225 - accuracy:  - ETA: 2s - l - ETA: 1s - loss: 0.2225 - accu - ETA: 1s - loss: 0.2228 - accuracy:  - ETA: 0s - loss: 0.2226 - accuracy - ETA: 0s - loss: 0.2229 \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.22296015918254852, 0.9269000291824341]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(test_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collectEvidence_branches(model,test_ds, evidence=True):\n",
    "    num_outputs = len(model.outputs) # the number of output layers for the purpose of providing labels\n",
    "    print(\"outputs\",num_outputs)\n",
    "#     train_ds, test_ds, validation_ds = (dataset)\n",
    "    predictions = []\n",
    "    labels = []\n",
    "    pClass = []\n",
    "    predictions=[]\n",
    "    pEvidence = []\n",
    "    pUncertainty=[]\n",
    "    pOverlap=[]\n",
    "\n",
    "    Outputs = pd.DataFrame()\n",
    "    pAcc=[]\n",
    "    for i in range(num_outputs):\n",
    "        pClass.append([])\n",
    "        predictions.append([])\n",
    "        pEvidence.append([])\n",
    "        pUncertainty.append([])\n",
    "        pAcc.append([])\n",
    "        pOverlap.append([])\n",
    "        # pOutputs.append([])\n",
    "\n",
    "    for i, (x,y) in enumerate(test_ds):\n",
    "        # if i > 10:\n",
    "            # break\n",
    "        print(\"prediction: {} of {}\".format(i,len(test_ds)),end='\\r')\n",
    "        if evidence: \n",
    "            result = model.test_on_batch(x,y)\n",
    "            if i < 1:\n",
    "            # break\n",
    "                print(result)\n",
    "            for j in range(num_outputs):\n",
    "#                 print(\"output\",j)\n",
    "                pClass[j].append(tf.argmax(y[0]).numpy())\n",
    "#                 print(\"class\",pClass[j][i])\n",
    "                pAcc[j].append(result[j+(num_outputs+1)])  \n",
    "#                 print(\"acc\",pAcc[j][i])\n",
    "                if j ==0:\n",
    "                    pEvidence[j].append(0)\n",
    "                else:\n",
    "#                     print(\"evid Number\",((num_outputs * 2)+1), \" \", ((j-1)*3))\n",
    "                    pEvidence[j].append(result[((num_outputs * 2) + 1)+((j-1)*3)])\n",
    "#                 print(\"evid\",pEvidence[j][i])\n",
    "\n",
    "                pOverlap[j].append(pAcc[0][i] - pAcc[j][i])\n",
    "#                 print(\"overlap\",pOverlap[j][i])\n",
    "        else:\n",
    "            result = model.predict(x)[0]\n",
    "#             print(result)\n",
    "\n",
    "            for j in range(num_outputs):\n",
    "                pClass[j].append(tf.argmax(y[0]).numpy())\n",
    "                # print(pClass[j])\n",
    "                # print(result)\n",
    "                prediction = np.argmax(result[j])\n",
    "                if prediction == pClass[j][i]:\n",
    "                    pAcc[j].append(1)  \n",
    "                else:\n",
    "                    pAcc[j].append(0)  \n",
    "                # print(branching.utils.calcEntropy_Tensors(result[j]).numpy())\n",
    "                pEvidence[j].append(branching.utils.calcEntropy_Tensors(result[j]).numpy()[0])\n",
    "\n",
    "                pOverlap[j].append(pAcc[0][i] - pAcc[j][i])\n",
    "        '''\n",
    "        overlap\n",
    "        if zero, both match, if else they don't match\n",
    "        TT 1-1 =0\n",
    "        TF 1-0 =1\n",
    "\n",
    "        FT 0-1 = -1\n",
    "        FF 0-0 =0\n",
    "        \n",
    "        '''\n",
    "    Outputs=[]\n",
    "    for j in range(num_outputs):\n",
    "        Predictions = pd.DataFrame({\"label\":pClass[j],\"evidence\":pEvidence[j],\"Acc\":pAcc[j], \"overlap\":pOverlap[j]})\n",
    "        Outputs.append(Predictions)\n",
    "    return Outputs\n",
    "\n",
    "def displayEvidence_cascade(branch_predictions, thresholds=None, output_names=[\"branch_1\",\"branch_2\",\"branch_3\",\"Main_Exit\"], Evidence = True):\n",
    "    plt.style.use('seaborn-whitegrid')\n",
    "    fig = plt.figure()\n",
    "    series=[]\n",
    "    fig, axs = plt.subplots(2, 2)\n",
    "    fig.tight_layout()\n",
    "    Outputs=pd.DataFrame()\n",
    "    #lets reorder the predictions so that the final layer is at the end\n",
    "    # _branch_predictions.copy()\n",
    "    _branch_predictions = branch_predictions.copy()\n",
    "    # print(_branch_predictions)\n",
    "    _branch_predictions.append(_branch_predictions.pop(0))\n",
    "    # print(_branch_predictions)\n",
    "    rollOver_indices = pd.Index([])\n",
    "    for i, Predictions in enumerate(_branch_predictions):\n",
    "        #check if rollover is active, if so, select only the predictions whose indexes match the rollover list\n",
    "        # print(rollOver_indices)\n",
    "        test_acc = Predictions[\"Acc\"].astype('bool').value_counts()\n",
    "        test_accuracy = (test_acc.loc[True] /  (test_acc.loc[True] + test_acc.loc[False]))\n",
    "        if len(rollOver_indices)>0:\n",
    "            print(\"rollover enabled, {} predictions provided\".format(len(rollOver_indices)))\n",
    "            Predictions = Predictions.iloc[rollOver_indices]\n",
    "        # print(Predictions.shape)\n",
    "        Predictions[\"Acc\"]=Predictions[\"Acc\"].astype('bool')\n",
    "        # Predictions[\"evidence\"]=Predictions[\"evidence\"].()[0]\n",
    "        acc = Predictions[\"Acc\"].value_counts()\n",
    "        # print(acc)\n",
    "        # print((acc.loc[True] , acc.loc[False]))\n",
    "        _Incorrects_missed = Predictions.loc[(Predictions['Acc'] == False)  & (Predictions[\"overlap\"] == 1)] #all the predictions that the main exit got true and the branch got wrong\n",
    "#         print(\"incorrects\",_Incorrects_missed)\n",
    "        if len(_Incorrects_missed) > 0 :\n",
    "#             print(\"incorrects\",_Incorrects_missed)\n",
    "            mean = _Incorrects_missed.groupby(\"Acc\")[\"evidence\"].mean().iloc[0]\n",
    "            std = _Incorrects_missed.groupby(\"Acc\")[\"evidence\"].std().iloc[0]\n",
    "        else:\n",
    "            print(\"pred\",Predictions.loc[(Predictions['Acc'] == False)])\n",
    "            mean = Predictions.loc[(Predictions['Acc'] == False)].groupby(\"Acc\")[\"evidence\"].mean().iloc[0]\n",
    "            std = Predictions.loc[(Predictions['Acc'] == False)].groupby(\"Acc\")[\"evidence\"].std().iloc[0]\n",
    "\n",
    "        print(\"mean\",mean , \" std\",std)\n",
    "        \n",
    "        correct_rows = Predictions.loc[Predictions['Acc'] == True]\n",
    "        incorrect_rows = Predictions.loc[Predictions['Acc'] == False]\n",
    "        \n",
    "        E_threshold = -1 #-1 is null for threshold\n",
    "        if thresholds is not None:\n",
    "            try:\n",
    "                E_threshold = thresholds[i]\n",
    "            except:\n",
    "                print(\"threshold not supplied for branch {}, using test data\".format(i))\n",
    "                \n",
    "        if Evidence:\n",
    "            if E_threshold ==-1:\n",
    "                E_threshold = mean + std\n",
    "            Accepted = Predictions.loc[(Predictions[\"evidence\"] >= E_threshold)]\n",
    "            Rejected = Predictions.loc[(Predictions[\"evidence\"] < E_threshold)]\n",
    "        else: \n",
    "            if E_threshold ==-1:\n",
    "                E_threshold = mean - std\n",
    "            Accepted = Predictions.loc[(Predictions[\"evidence\"] <= E_threshold)]\n",
    "            Rejected = Predictions.loc[(Predictions[\"evidence\"] > E_threshold)]\n",
    "        \n",
    "        rollOver_indices = Rejected.index\n",
    "        Incorrects_overlap = Accepted.loc[(Accepted['Acc'] == False) & (Accepted[\"overlap\"] == 0)].count().iloc[0]\n",
    "        print(Accepted.shape[0])\n",
    "        Outputs = Outputs.append(pd.DataFrame({\"Branch Name\":output_names[i],\n",
    "                \"Predictions\": len(Predictions.index),\n",
    "                \"test_accuracy\": test_accuracy,\n",
    "                \"Accuracy\":(acc.loc[True] /  (acc.loc[True] + acc.loc[False])),\n",
    "                \"E_Threshold\":E_threshold,\n",
    "                # \"Overlap_Threshold\":non_overlapping_incorrects_threshold,\n",
    "                \"acceptance_rate\":Accepted.shape[0]/(Predictions.shape[0]),\n",
    "                \"accepted_correct\":Accepted.loc[(Predictions['Acc'] == True)].shape[0],\n",
    "                \"accepted_incorrect\":Accepted.loc[(Predictions['Acc'] == False)].shape[0],\n",
    "                \"accepted_accuracy\":(Accepted.loc[(Accepted['Acc'] == True)].shape[0])/ Accepted.shape[0],\n",
    "                \"overlap_adjusted_accuracy\":(Accepted.loc[(Accepted['Acc'] == True)].count()[0] + Incorrects_overlap) / Accepted.count()[0],\n",
    "                \"M(T) B(F)\":Accepted.loc[(Accepted[\"overlap\"] == 1)].count().iloc[0],\n",
    "                \"M(F) B(T)\":Accepted.loc[(Accepted[\"overlap\"] ==-1)].count().iloc[0],\n",
    "                \"M(F) B(F) overlap\":Incorrects_overlap,\n",
    "                },index=[i]))\n",
    "#         print(\"TT\",Accepted.loc[(Accepted[\"Acc\"] ==True) & (Accepted[\"overlap\"] == 0)])\n",
    "#         print(\"TF\",Accepted.loc[(Accepted[\"overlap\"] == 1)])\n",
    "#         print(\"FT\",Accepted.loc[(Accepted[\"overlap\"] == -1)])\n",
    "#         print(\"FF\",Accepted.loc[(Accepted[\"Acc\"] ==False) & (Accepted[\"overlap\"] == 0)])\n",
    "        axs[round(int(i/2)), round(i%2)]\n",
    "    # fig, axs = plt.subplots(1, 2)\n",
    "        # axs[round(int(i/2)), round(i%2)].suptitle('Horizontally stacked subplots')\n",
    "        axs[round(int(i/2)), round(i%2)].scatter(correct_rows['label'],correct_rows['evidence'],c ='r',marker='+')\n",
    "        axs[round(int(i/2)), round(i%2)].scatter(incorrect_rows['label']+.3,incorrect_rows['evidence'],c ='k',marker='x')\n",
    "        axs[round(int(i/2)), round(i%2)].plot(np.repeat(E_threshold,11),'b--')\n",
    "        axs[round(int(i/2)), round(i%2)].title.set_text(output_names[i])\n",
    "    \n",
    "    for ax in fig.axes:\n",
    "            # ax.yaxis.set_major_formatter(FormatStrFormatter('%.2f'))\n",
    "            axLine, axLabel = ax.get_legend_handles_labels()\n",
    "            lines=(axLine)\n",
    "            labels=(axLabel)\n",
    "    fig.text(0.5, 0.01, 'Items Exit at Branch', ha='center', va='center')\n",
    "    fig.text(0.01, 0.5, 'Accuracy %', ha='center', va='center', rotation='vertical')\n",
    "    # fig.legend(lines, labels,bbox_to_anchor=(1., 1), loc=2,borderaxespad=0.,frameon=True)\n",
    "    # fig.set_size_inches(10, 10)\n",
    "    plt.show()\n",
    "    return Outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Experiencing connection interruptions. Will try to reestablish communication with Neptune.\n",
      "Communication with Neptune restored!\n"
     ]
    }
   ],
   "source": [
    "(train_images, train_labels), (test_images, test_labels) = tf.keras.datasets.cifar10.load_data()\n",
    "K= 10 # number of classes\n",
    "test_labels = tf.keras.utils.to_categorical(test_labels,10)\n",
    "train_labels = tf.keras.utils.to_categorical(train_labels,10)\n",
    "shuffle_size = 22500\n",
    "batch_size=1\n",
    "test_ds = tf.data.Dataset.from_tensor_slices((test_images, test_labels))\n",
    "def augment_images(image, label,input_size=(224,224), channel_first = False):\n",
    "            image = tf.image.resize(image,input_size)\n",
    "            if channel_first:\n",
    "                image = tf.transpose(image, [2, 0, 1])\n",
    "            return image, label\n",
    "test_ds_size = len(list(test_ds))\n",
    "test_ds = (test_ds.map(augment_images))\n",
    "t_target = tf.data.Dataset.from_tensor_slices((test_labels))\n",
    "test_ds = tf.data.Dataset.zip((test_ds,t_target))\n",
    "test_ds = (test_ds.batch(batch_size=1, drop_remainder=True))\n",
    "\n",
    "validation_size = 5000\n",
    "validation_images, validation_labels = train_images[:validation_size], train_labels[:validation_size] #get the first 5k training samples as validation set\n",
    "shuffle_size = 22500\n",
    "batch_size=1\n",
    "validation_ds = tf.data.Dataset.from_tensor_slices((validation_images, validation_labels))\n",
    "validation_ds_size = len(list(validation_ds))\n",
    "validation_ds = (validation_ds.map(augment_images))\n",
    "v_target = tf.data.Dataset.from_tensor_slices((validation_labels))\n",
    "validation_ds = tf.data.Dataset.zip((validation_ds,v_target))\n",
    "validation_ds = (validation_ds.batch(batch_size=1, drop_remainder=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "outputs 4\n",
      "[3.1898131370544434, 2.161137342453003, 0.1279248297214508, 0.8997544050216675, 0.000996571616269648, 1.0, 1.0, 0.0, 1.0, 0.07632288336753845, 0.7629795670509338, 0.00024929846404120326, 0.028320902958512306, 0.2819879651069641, 0.001221083221025765, 0.19269037246704102, 1.9269037246704102, 0.0]\n",
      "prediction: 4999 of 5000\r"
     ]
    }
   ],
   "source": [
    "\n",
    "# Predictions = collectEvidence_branches(model,test_ds)\n",
    "validation_Outputs = collectEvidence_branches(model,validation_ds, True)\n",
    "\n",
    "# print(Outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "outputs 4\n",
      "[2.316770553588867, 2.1611328125, 0.15376195311546326, 0.0002990508219227195, 0.0015768486773595214, 1.0, 1.0, 1.0, 1.0, 2.637917305037263e-06, 2.637917350511998e-05, 0.0, 0.010988399386405945, 0.10988399386405945, 0.0, 0.00037163501838222146, 0.0037163503002375364, 0.0]\n",
      "prediction: 9999 of 10000\r"
     ]
    }
   ],
   "source": [
    "test_Outputs = collectEvidence_branches(model,test_ds, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(test_Outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean 0.009561432281937439  std 0.0627220393444717\n",
      "719\n",
      "rollover enabled, 4281 predictions provided\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\sanity\\appdata\\local\\conda\\conda\\envs\\py36\\lib\\site-packages\\ipykernel_launcher.py:104: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean 0.0043944476036677045  std 0.02227074220913497\n",
      "698\n",
      "rollover enabled, 3583 predictions provided\n",
      "mean 0.003585334175435121  std 0.01784717069797608\n",
      "908\n",
      "rollover enabled, 2675 predictions provided\n",
      "pred       label  evidence    Acc  overlap\n",
      "17        3         0  False      0.0\n",
      "35        0         0  False      0.0\n",
      "37        7         0  False      0.0\n",
      "39        3         0  False      0.0\n",
      "296       4         0  False      0.0\n",
      "301       1         0  False      0.0\n",
      "333       3         0  False      0.0\n",
      "338       7         0  False      0.0\n",
      "370       3         0  False      0.0\n",
      "401       0         0  False      0.0\n",
      "435       4         0  False      0.0\n",
      "441       8         0  False      0.0\n",
      "450       5         0  False      0.0\n",
      "466       1         0  False      0.0\n",
      "478       7         0  False      0.0\n",
      "526       4         0  False      0.0\n",
      "548       3         0  False      0.0\n",
      "555       0         0  False      0.0\n",
      "573       5         0  False      0.0\n",
      "584       8         0  False      0.0\n",
      "589       7         0  False      0.0\n",
      "621       4         0  False      0.0\n",
      "629       3         0  False      0.0\n",
      "633       6         0  False      0.0\n",
      "638       3         0  False      0.0\n",
      "640       6         0  False      0.0\n",
      "647       7         0  False      0.0\n",
      "684       3         0  False      0.0\n",
      "686       5         0  False      0.0\n",
      "721       6         0  False      0.0\n",
      "...     ...       ...    ...      ...\n",
      "4402      3         0  False      0.0\n",
      "4446      8         0  False      0.0\n",
      "4456      5         0  False      0.0\n",
      "4481      3         0  False      0.0\n",
      "4486      5         0  False      0.0\n",
      "4491      2         0  False      0.0\n",
      "4515      3         0  False      0.0\n",
      "4521      2         0  False      0.0\n",
      "4522      7         0  False      0.0\n",
      "4558      3         0  False      0.0\n",
      "4590      6         0  False      0.0\n",
      "4598      2         0  False      0.0\n",
      "4627      5         0  False      0.0\n",
      "4628      3         0  False      0.0\n",
      "4634      2         0  False      0.0\n",
      "4640      4         0  False      0.0\n",
      "4665      9         0  False      0.0\n",
      "4671      7         0  False      0.0\n",
      "4697      3         0  False      0.0\n",
      "4766      6         0  False      0.0\n",
      "4776      2         0  False      0.0\n",
      "4843      6         0  False      0.0\n",
      "4924      4         0  False      0.0\n",
      "4935      0         0  False      0.0\n",
      "4952      0         0  False      0.0\n",
      "4960      5         0  False      0.0\n",
      "4972      8         0  False      0.0\n",
      "4973      4         0  False      0.0\n",
      "4987      4         0  False      0.0\n",
      "4997      5         0  False      0.0\n",
      "\n",
      "[214 rows x 4 columns]\n",
      "mean 0  std 0.0\n",
      "2675\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 4 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Branch Name</th>\n",
       "      <th>Predictions</th>\n",
       "      <th>test_accuracy</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>E_Threshold</th>\n",
       "      <th>acceptance_rate</th>\n",
       "      <th>accepted_correct</th>\n",
       "      <th>accepted_incorrect</th>\n",
       "      <th>accepted_accuracy</th>\n",
       "      <th>overlap_adjusted_accuracy</th>\n",
       "      <th>M(T) B(F)</th>\n",
       "      <th>M(F) B(T)</th>\n",
       "      <th>M(F) B(F) overlap</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>branch_1</td>\n",
       "      <td>5000</td>\n",
       "      <td>0.6942</td>\n",
       "      <td>0.694200</td>\n",
       "      <td>0.072283</td>\n",
       "      <td>0.143800</td>\n",
       "      <td>676</td>\n",
       "      <td>43</td>\n",
       "      <td>0.940195</td>\n",
       "      <td>0.942976</td>\n",
       "      <td>41</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>branch_2</td>\n",
       "      <td>4281</td>\n",
       "      <td>0.7414</td>\n",
       "      <td>0.706611</td>\n",
       "      <td>0.026665</td>\n",
       "      <td>0.163046</td>\n",
       "      <td>651</td>\n",
       "      <td>47</td>\n",
       "      <td>0.932665</td>\n",
       "      <td>0.938395</td>\n",
       "      <td>43</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>branch_3</td>\n",
       "      <td>3583</td>\n",
       "      <td>0.8480</td>\n",
       "      <td>0.805470</td>\n",
       "      <td>0.021433</td>\n",
       "      <td>0.253419</td>\n",
       "      <td>881</td>\n",
       "      <td>27</td>\n",
       "      <td>0.970264</td>\n",
       "      <td>0.977974</td>\n",
       "      <td>20</td>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Main_Exit</td>\n",
       "      <td>2675</td>\n",
       "      <td>0.9530</td>\n",
       "      <td>0.920000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2461</td>\n",
       "      <td>214</td>\n",
       "      <td>0.920000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>214</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Branch Name  Predictions  test_accuracy  Accuracy  E_Threshold  \\\n",
       "0    branch_1         5000         0.6942  0.694200     0.072283   \n",
       "1    branch_2         4281         0.7414  0.706611     0.026665   \n",
       "2    branch_3         3583         0.8480  0.805470     0.021433   \n",
       "3   Main_Exit         2675         0.9530  0.920000     0.000000   \n",
       "\n",
       "   acceptance_rate  accepted_correct  accepted_incorrect  accepted_accuracy  \\\n",
       "0         0.143800               676                  43           0.940195   \n",
       "1         0.163046               651                  47           0.932665   \n",
       "2         0.253419               881                  27           0.970264   \n",
       "3         1.000000              2461                 214           0.920000   \n",
       "\n",
       "   overlap_adjusted_accuracy  M(T) B(F)  M(F) B(T)  M(F) B(F) overlap  \n",
       "0                   0.942976         41          2                  2  \n",
       "1                   0.938395         43          4                  4  \n",
       "2                   0.977974         20          2                  7  \n",
       "3                   1.000000          0          0                214  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#### version 2\n",
    "display(displayEvidence_cascade(validation_Outputs, Evidence = True))\n",
    "# display(displayEvidence_cascade(test_Outputs, Evidence = False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean 0.00724401125968572  std 0.044100429902564985\n",
      "1478\n",
      "rollover enabled, 8522 predictions provided\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\sanity\\appdata\\local\\conda\\conda\\envs\\py36\\lib\\site-packages\\ipykernel_launcher.py:104: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean 0.004285258216642008  std 0.025484250740356294\n",
      "1343\n",
      "rollover enabled, 7179 predictions provided\n",
      "mean 0.005170650642565793  std 0.029986948274836285\n",
      "1768\n",
      "rollover enabled, 5411 predictions provided\n",
      "pred       label  evidence    Acc  overlap\n",
      "25        2         0  False      0.0\n",
      "37        1         0  False      0.0\n",
      "47        9         0  False      0.0\n",
      "59        6         0  False      0.0\n",
      "61        3         0  False      0.0\n",
      "68        3         0  False      0.0\n",
      "110       4         0  False      0.0\n",
      "125       0         0  False      0.0\n",
      "127       3         0  False      0.0\n",
      "164       8         0  False      0.0\n",
      "184       3         0  False      0.0\n",
      "200       5         0  False      0.0\n",
      "224       3         0  False      0.0\n",
      "226       6         0  False      0.0\n",
      "255       0         0  False      0.0\n",
      "275       5         0  False      0.0\n",
      "281       0         0  False      0.0\n",
      "309       6         0  False      0.0\n",
      "313       0         0  False      0.0\n",
      "324       4         0  False      0.0\n",
      "355       7         0  False      0.0\n",
      "356       3         0  False      0.0\n",
      "370       4         0  False      0.0\n",
      "378       0         0  False      0.0\n",
      "384       2         0  False      0.0\n",
      "432       3         0  False      0.0\n",
      "441       4         0  False      0.0\n",
      "456       3         0  False      0.0\n",
      "470       3         0  False      0.0\n",
      "473       0         0  False      0.0\n",
      "...     ...       ...    ...      ...\n",
      "9363      5         0  False      0.0\n",
      "9376      5         0  False      0.0\n",
      "9380      5         0  False      0.0\n",
      "9385      7         0  False      0.0\n",
      "9414      5         0  False      0.0\n",
      "9431      3         0  False      0.0\n",
      "9505      3         0  False      0.0\n",
      "9507      0         0  False      0.0\n",
      "9514      5         0  False      0.0\n",
      "9518      9         0  False      0.0\n",
      "9522      3         0  False      0.0\n",
      "9528      0         0  False      0.0\n",
      "9641      6         0  False      0.0\n",
      "9694      1         0  False      0.0\n",
      "9704      2         0  False      0.0\n",
      "9734      4         0  False      0.0\n",
      "9746      2         0  False      0.0\n",
      "9776      2         0  False      0.0\n",
      "9787      8         0  False      0.0\n",
      "9793      2         0  False      0.0\n",
      "9812      3         0  False      0.0\n",
      "9817      9         0  False      0.0\n",
      "9825      2         0  False      0.0\n",
      "9840      4         0  False      0.0\n",
      "9862      3         0  False      0.0\n",
      "9949      3         0  False      0.0\n",
      "9953      7         0  False      0.0\n",
      "9959      2         0  False      0.0\n",
      "9960      2         0  False      0.0\n",
      "9985      5         0  False      0.0\n",
      "\n",
      "[460 rows x 4 columns]\n",
      "mean 0  std 0.0\n",
      "threshold not supplied for branch 3, using test data\n",
      "5411\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 4 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Branch Name</th>\n",
       "      <th>Predictions</th>\n",
       "      <th>test_accuracy</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>E_Threshold</th>\n",
       "      <th>acceptance_rate</th>\n",
       "      <th>accepted_correct</th>\n",
       "      <th>accepted_incorrect</th>\n",
       "      <th>accepted_accuracy</th>\n",
       "      <th>overlap_adjusted_accuracy</th>\n",
       "      <th>M(T) B(F)</th>\n",
       "      <th>M(F) B(T)</th>\n",
       "      <th>M(F) B(F) overlap</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>branch_1</td>\n",
       "      <td>10000</td>\n",
       "      <td>0.6829</td>\n",
       "      <td>0.682900</td>\n",
       "      <td>0.072283</td>\n",
       "      <td>0.147800</td>\n",
       "      <td>1405</td>\n",
       "      <td>73</td>\n",
       "      <td>0.950609</td>\n",
       "      <td>0.957375</td>\n",
       "      <td>63</td>\n",
       "      <td>8</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>branch_2</td>\n",
       "      <td>8522</td>\n",
       "      <td>0.7320</td>\n",
       "      <td>0.696433</td>\n",
       "      <td>0.026665</td>\n",
       "      <td>0.157592</td>\n",
       "      <td>1258</td>\n",
       "      <td>85</td>\n",
       "      <td>0.936709</td>\n",
       "      <td>0.945644</td>\n",
       "      <td>73</td>\n",
       "      <td>9</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>branch_3</td>\n",
       "      <td>7179</td>\n",
       "      <td>0.8323</td>\n",
       "      <td>0.783396</td>\n",
       "      <td>0.021433</td>\n",
       "      <td>0.246274</td>\n",
       "      <td>1691</td>\n",
       "      <td>77</td>\n",
       "      <td>0.956448</td>\n",
       "      <td>0.967195</td>\n",
       "      <td>58</td>\n",
       "      <td>11</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Main_Exit</td>\n",
       "      <td>5411</td>\n",
       "      <td>0.9471</td>\n",
       "      <td>0.914988</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>4951</td>\n",
       "      <td>460</td>\n",
       "      <td>0.914988</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>460</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Branch Name  Predictions  test_accuracy  Accuracy  E_Threshold  \\\n",
       "0    branch_1        10000         0.6829  0.682900     0.072283   \n",
       "1    branch_2         8522         0.7320  0.696433     0.026665   \n",
       "2    branch_3         7179         0.8323  0.783396     0.021433   \n",
       "3   Main_Exit         5411         0.9471  0.914988     0.000000   \n",
       "\n",
       "   acceptance_rate  accepted_correct  accepted_incorrect  accepted_accuracy  \\\n",
       "0         0.147800              1405                  73           0.950609   \n",
       "1         0.157592              1258                  85           0.936709   \n",
       "2         0.246274              1691                  77           0.956448   \n",
       "3         1.000000              4951                 460           0.914988   \n",
       "\n",
       "   overlap_adjusted_accuracy  M(T) B(F)  M(F) B(T)  M(F) B(F) overlap  \n",
       "0                   0.957375         63          8                 10  \n",
       "1                   0.945644         73          9                 12  \n",
       "2                   0.967195         58         11                 19  \n",
       "3                   1.000000          0          0                460  "
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "displayEvidence_cascade(test_Outputs, thresholds=[0.072283,0.026665,0.021433], Evidence = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean 0.004004710083855178  std 0.008881118102279774\n",
      "1591\n",
      "rollover enabled, 8409 predictions provided\n",
      "mean 0.0020127950316792915  std 0.005747526619716405\n",
      "847\n",
      "rollover enabled, 7562 predictions provided\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\sanity\\appdata\\local\\conda\\conda\\envs\\py36\\lib\\site-packages\\ipykernel_launcher.py:104: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean 0.0025505108034397685  std 0.007414918399168136\n",
      "2572\n",
      "rollover enabled, 4990 predictions provided\n",
      "pred       label  evidence    Acc  overlap\n",
      "37        1         0  False      0.0\n",
      "47        9         0  False      0.0\n",
      "57        7         0  False      0.0\n",
      "125       0         0  False      0.0\n",
      "164       8         0  False      0.0\n",
      "218       8         0  False      0.0\n",
      "226       6         0  False      0.0\n",
      "228       7         0  False      0.0\n",
      "255       0         0  False      0.0\n",
      "264       0         0  False      0.0\n",
      "273       3         0  False      0.0\n",
      "309       6         0  False      0.0\n",
      "313       0         0  False      0.0\n",
      "352       0         0  False      0.0\n",
      "355       7         0  False      0.0\n",
      "356       3         0  False      0.0\n",
      "394       9         0  False      0.0\n",
      "428       0         0  False      0.0\n",
      "456       3         0  False      0.0\n",
      "464       3         0  False      0.0\n",
      "518       5         0  False      0.0\n",
      "531       0         0  False      0.0\n",
      "537       2         0  False      0.0\n",
      "551       5         0  False      0.0\n",
      "563       2         0  False      0.0\n",
      "607       3         0  False      0.0\n",
      "639       5         0  False      0.0\n",
      "665       8         0  False      0.0\n",
      "672       6         0  False      0.0\n",
      "683       8         0  False      0.0\n",
      "...     ...       ...    ...      ...\n",
      "9431      3         0  False      0.0\n",
      "9433      3         0  False      0.0\n",
      "9484      0         0  False      0.0\n",
      "9503      2         0  False      0.0\n",
      "9505      3         0  False      0.0\n",
      "9518      9         0  False      0.0\n",
      "9528      0         0  False      0.0\n",
      "9555      4         0  False      0.0\n",
      "9556      6         0  False      0.0\n",
      "9665      3         0  False      0.0\n",
      "9689      9         0  False      0.0\n",
      "9690      4         0  False      0.0\n",
      "9704      2         0  False      0.0\n",
      "9735      6         0  False      0.0\n",
      "9753      3         0  False      0.0\n",
      "9801      6         0  False      0.0\n",
      "9812      3         0  False      0.0\n",
      "9825      2         0  False      0.0\n",
      "9832      2         0  False      0.0\n",
      "9840      4         0  False      0.0\n",
      "9853      5         0  False      0.0\n",
      "9855      7         0  False      0.0\n",
      "9857      0         0  False      0.0\n",
      "9901      3         0  False      0.0\n",
      "9923      7         0  False      0.0\n",
      "9928      5         0  False      0.0\n",
      "9949      3         0  False      0.0\n",
      "9959      2         0  False      0.0\n",
      "9968      3         0  False      0.0\n",
      "9989      2         0  False      0.0\n",
      "\n",
      "[524 rows x 4 columns]\n",
      "mean 0  std 0.0\n",
      "threshold not supplied for branch 3, using test data\n",
      "4990\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 4 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Branch Name</th>\n",
       "      <th>Predictions</th>\n",
       "      <th>test_accuracy</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>E_Threshold</th>\n",
       "      <th>acceptance_rate</th>\n",
       "      <th>accepted_correct</th>\n",
       "      <th>accepted_incorrect</th>\n",
       "      <th>accepted_accuracy</th>\n",
       "      <th>overlap_adjusted_accuracy</th>\n",
       "      <th>M(T) B(F)</th>\n",
       "      <th>M(F) B(T)</th>\n",
       "      <th>M(F) B(F) overlap</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>branch_1</td>\n",
       "      <td>10000</td>\n",
       "      <td>0.4364</td>\n",
       "      <td>0.436400</td>\n",
       "      <td>0.009285</td>\n",
       "      <td>0.159100</td>\n",
       "      <td>971</td>\n",
       "      <td>620</td>\n",
       "      <td>0.610308</td>\n",
       "      <td>0.637335</td>\n",
       "      <td>577</td>\n",
       "      <td>13</td>\n",
       "      <td>43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>branch_2</td>\n",
       "      <td>8409</td>\n",
       "      <td>0.4857</td>\n",
       "      <td>0.432988</td>\n",
       "      <td>0.008253</td>\n",
       "      <td>0.100725</td>\n",
       "      <td>595</td>\n",
       "      <td>252</td>\n",
       "      <td>0.702479</td>\n",
       "      <td>0.746163</td>\n",
       "      <td>215</td>\n",
       "      <td>20</td>\n",
       "      <td>37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>branch_3</td>\n",
       "      <td>7562</td>\n",
       "      <td>0.7764</td>\n",
       "      <td>0.750992</td>\n",
       "      <td>0.007197</td>\n",
       "      <td>0.340122</td>\n",
       "      <td>2431</td>\n",
       "      <td>141</td>\n",
       "      <td>0.945179</td>\n",
       "      <td>0.960731</td>\n",
       "      <td>101</td>\n",
       "      <td>24</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Main_Exit</td>\n",
       "      <td>4990</td>\n",
       "      <td>0.9299</td>\n",
       "      <td>0.894990</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>4466</td>\n",
       "      <td>524</td>\n",
       "      <td>0.894990</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>524</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Branch Name  Predictions  test_accuracy  Accuracy  E_Threshold  \\\n",
       "0    branch_1        10000         0.4364  0.436400     0.009285   \n",
       "1    branch_2         8409         0.4857  0.432988     0.008253   \n",
       "2    branch_3         7562         0.7764  0.750992     0.007197   \n",
       "3   Main_Exit         4990         0.9299  0.894990     0.000000   \n",
       "\n",
       "   acceptance_rate  accepted_correct  accepted_incorrect  accepted_accuracy  \\\n",
       "0         0.159100               971                 620           0.610308   \n",
       "1         0.100725               595                 252           0.702479   \n",
       "2         0.340122              2431                 141           0.945179   \n",
       "3         1.000000              4466                 524           0.894990   \n",
       "\n",
       "   overlap_adjusted_accuracy  M(T) B(F)  M(F) B(T)  M(F) B(F) overlap  \n",
       "0                   0.637335        577         13                 43  \n",
       "1                   0.746163        215         20                 37  \n",
       "2                   0.960731        101         24                 40  \n",
       "3                   1.000000          0          0                524  "
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "displayEvidence_cascade(test_Outputs, thresholds=[0.009285,0.008253,0.007197] ,Evidence = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean 2.3775139060157775  std 0.44754977834901194\n",
      "rollover enabled, 7356 predictions provided\n",
      "mean 2.2910808496759185  std 0.4174707782345464\n",
      "rollover enabled, 5637 predictions provided\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\sanity\\appdata\\local\\conda\\conda\\envs\\py36\\lib\\site-packages\\ipykernel_launcher.py:257: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean 1.9400157221468561  std 0.44062205012064976\n",
      "rollover enabled, 3731 predictions provided\n",
      "mean 1.0032464265583023  std 0.582510151908757\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAa4AAAEvCAYAAAAdGSXnAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMi4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvhp/UCwAAIABJREFUeJzsnXt8FNX5/z+zmxsJuUEgWDB446ZAVoKAVJMg/SoqN+ulSsVrG2rBJFApYJVEoZq2FkhCq9TefOnXH6BVpK2lgF8ugkoFTBASQCwgILck5E4uO3N+f+zOMjM7M2dmz96Z9+u1r2R39uw5c+ac8zznOc95DkcIIbCwsLCwsIgQbKEugIWFhYWFhRkswWVhYWFhEVFYgsvCwsLCIqKwBJeFhYWFRURhCS4LCwsLi4jCElwWFhYWFhGFJbgsLCwsLCIKS3CFKbt27cLkyZODmud7772HWbNm+Zy+vLwcL774oh9LZGHBRiT1o48//hjf//73MW3aNNxzzz3YsWNHAEoXHcSEugAWkc+ZM2fw0ksvYfv27fj+978f6uJYWEQcLS0teOaZZ/DWW29h0KBBOHjwIB5++GFs3boVPXv2DHXxwg5LcIUx7e3tKCwsxPHjx5GSkoIXX3wRq1atQmNjI06cOIH8/Hzcd999ePHFF9HW1obz589j6NChWLFiBeLj4zFixAgUFBRg586dOHfuHH70ox9hxowZAIBVq1bh/fffR0xMDAYOHIiysjIAwPnz51FQUIDTp0/Dbrfjt7/9La699lrdcr777rsYM2YMrr32WjQ1NQW8XiwszBAJ/ai7uxslJSUYNGgQAOC6664DIQQXLlywBJcaxCIs+eyzz8jQoUPJnj17CCGErF69mtx3331kwYIF5NFHH/V8r6ysjKxbt44QQkhXVxeZPHky2bBhAyGEkMGDB5M333yTEELIl19+SYYPH046OjrI5s2bye23304aGxsJIYS89NJL5Pe//z3529/+RkaPHk2OHTtGCCFkyZIlZNGiRYbLXFFRQV544QXme7ew8BeR2I8IIeS3v/0t+f73v89079GMtcYVxgwZMgSjRo0CANxzzz3Yv38/WlpakJOT4/nO/Pnz0atXL7z++usoLS3FuXPn0N7e7rk+ceJEAMANN9yArq4utLe349NPP8WkSZOQmpoKAFi0aBGeeuopAMDIkSMxcOBAAMCwYcPQ0NAQlHu1sAgUkdSPnE4nli5dig0bNqCyspL95qMUy1QYxthscr2C4zjExMQgMTHR89m8efPA8zzuvPNO5Ofn4/Tp0yCSuMnx8fGetABACIHdbve8B4Dm5mY0NzcDAGJiLjUJjuNkv2VhEYlESj9qampCYWEhCCFYs2YN0tPTfbjbywNrxhXGHDp0CLW1tQCANWvWICcnBz169JB9Z8eOHZg9ezbuuusuAEB1dTV4ntf93fHjx2PTpk1obW0FAFRWVuKvf/2r/2/AwiIMiIR+xPM8CgoKMGDAAPz5z3+2hBYFa8YVxlxzzTVYuXIlTpw4gd69e6OsrMzLfDB37lzMnj0biYmJ6NmzJ2666SZ88803ur+bl5eHI0eO4KGHHgLgWghesmQJNm7cGLB7sbAIFZHQj/71r3+hqqoK7e3tuPfeez2f//rXv8aQIUNM/160wxHLFmRhYWFhEUFYMy4LKi+99BJ27dqlem3RokUYN25ckEtkYRF5WP3If1gzLgsLCwuLiMJyzrCwsLCwiCgiwlS4Z8+eUBfBwsIQ0r1B4YjVlywiBb2+FBGCC9C/idraWgwbNiyIpTFOuJYtXMsFhG/ZaOWKFKEQiX0pXMsFhG/ZwrVcAHtfskyFFhYWFhYRhSW4LCwsLCwiCktwWXiFo7EcTS0sLMKZqBNcLIOwIAi678MVlnsuLS1FcXGxJw0hBMXFxSgtLfVnEQOCMiQPLUSPRWRguj3n57tevqa3YKqzUNR3VAmu0tJSzJ07VzYIz507V3sQljT4/Px85OTkeISVIAjIyclBvqRD0DD1ABWdzVehafqeFeXbsGEDKioqPMKruLgYFRUV2LBhQ1AaoK+N/qqrrkLfvn09wornefTt2xdXXXWV4by7u7t131v4jq/PVas9r1y5UjtRVZXrpZPeqCIWSqHHojizKq++1lnIFF//npISGHbv3q17vaamhgiCQIqKiggAUlRUpPrei9RUQlJTCc/zpF+/fgQAcTgchOd54nA4CADSr18/wvM8tYwlJSWksLDQk48gCKSwsJD89Kc/VU/gzpsQQvLy8kh2drYnH57nSXZ2NsnLy9PN06d7VtRZcnIyAeD1Sk5O1k0vLYPeey/y8lwv4kOduXE6nSQmJoYAIL169SJOp5P06tWLACAxMTHE6XRSyx0fH08AkK6uLkKI6wwmACQ+Pl4zTU1Nje5v0tppOGCkLxHiw3OVUFJSImt/YrssKSnRTafXnmfOnOldBrEtAYQARMjNJUX9+/vUH1jKTWsXRsjLy/OMPYQQzxhEGwMI8b0fid/1dQwRBIGMHTuWAPDkX1hYSACQsWPHUscfPWjtNPIFV14eab3pJkIIIVlZWZ4BSXzFx8eTrKwsrzTKBq82eIsvzQfg/h1BEEh/d4dRPsC+ffvK0yvy5m+9lWTGxhIAHuGVnZ1NAJDMzExtoen+ncWLF5ORI0fKyjty5EiyePFi3TqtqakhTqeT2O121Xu22+3aAsCdt1aH0e3okjoThaayzhITE3UbPc/zJDMzU7XcunXmVhZEISW+1N5r1Zke0SK4fHqublgGQkIIWbx4saf9i6/s7Gz1QVhU/tx9iaSmkix3n6eOAX4st9gulP3FiAJFCGFSnPWEx8iRIw0pHLm5uaR3796yOuvduzfJzc3VTSfNS/mSth81LMGVmkqcyckyLVz58tLCFQ2+s0cPXcHV2dmpnrd7ENYbSHv37i1veIq8hZQUkqyRr+6sJzWVCCkpJC4uTjVtXFwcteHwPE9sNptqepvNpis0hdxcc9qWQmA7b7mFxGjct67QJK4O45Oi4c67o6NDN31HR4dmnekRDYLrwIEDPmvRIllZWV7tMi4uzpDwEBVA5ctLASTEVJvSnIlLLAC+DuA1NTVk4MCBJCMjw5OH0+kkGRkZZODAgdT6EgSBjBkzRrXcY8aMoc56tNKOGDGCqnTzPE+SkpJU0yclJVGtTSyKsx60dhoxG5C9ENeHmppgByBMmACn06n6VafTKTvwTQnNlux1Xcx72zYAADdhAs6ePauatr6+npp3i8a1lpYWCIIAu93unXdTE3gAXRppu7q6wPO87EA7JTzPa967IAjgeV5+CJ/ivk/ExQEAKioqUFFR4fnaiRMnNPMU4TgOWrUuCIJundHWorq7uxHnLhsAIC1NnjflrCO9vC8HxOdn6rm665ivr1c9DqSrqwvffPMNeJ6Xt2cJhBDNZ+t0OkEIkT+brVtdf91tnNu2DdffeCP27dvnlf7666+n9sM9e/agra1N9nl9fT327NkDQRC8DqQU4XkeJ06cgCAI6NevH86cOYN+/fqhrq4ODQ0NuvcsMm7cOPznP/9R/TzQ9OzZ0+u+xc/1IO41cmV979u3DwkJCSgtLQ1YX4pcweVejBXp/vxz3a93d3d7TjGF++A3EdLZqZuWUBY6u7q0xMel61p5tyreK2ltbfUcDQ5Adt+0pVuaQKY1Kr3rBACvUS88z3sPMornJezdq1l+QojuQGFa0Whqkr+/eFE3/eUMIQRNyvpy09TU5P1cL10EAE3lUcTpdKoP4vn5gCBoHnGvVSYpHMchISFB9VpCQoK83AolDPn5IO3tqml1+39aGgZ3dyM9PR319fWoq6uTKYu6B0K6y8Bt3YqqqirYbDZZ27XZbKiqqtLuh/n5ACE4efKk6mVVZVrlvrWUbq3PRQgh+OKLL1SvffHFF9ptxQ9EruBSDPj2jg7dr8s6C6vbtKjpuR+K/eOPgdhYY3kriNO84r4ep/0Nmkuo1sAvYloAiPednw+OEAzs6MBZFS1x4MCB3g1W8bycKhqe7LrTqTlbNF1uBfpqhkvR0Kv3cIXneTz33HM4evQo7HY7Xn75ZWRlZRn/gbQ0DHY6kZSUpKqBJyUlqaaRYu/dWzcLvb5AUxC9rouDsNif8/NVZy0AND+X/vZFjfwvXryoWza73Y7ExETU19d7XUtMTKTOtpxOJ3bu3KnqVbhz507tvrBtGwQA5zT6SX19vbfFRiVvWtm0+oIgCJoz5O7ubl3lk5XIFVwKaKJIz2xmeuaidJE34TIPcfrt1iBtCQmAjtD1evAOh+uvqDEx4LMAqKoCEQTs11AA9u/f761tKe67i+NcKxMadHV1eWvP7kFS0NAwNcstzljFWYFuanpnDle2bNkCAFi9ejV27dqFl19+Ga+++qrxH2hqgg2AlkqhJsyUs1mOopB4KTSSGQAH7X7M87x3WsUsvuOzz3Tz7ujoQI8ePVxvJEoYAOCjj0A0xgdVoSUKbPf9n2hRN/ifOHHC21SomPXYbrtNcw+il7leml4sn0Z7pSkCgI9WF3f+9i1bMHr0aOzevdvrK6NHj6YKbBZCLrh81hIVD5pl9qE/V3M1eJm2uWOH7HrXzp266bu6ujSFZitlptja2nrJzKiARVgDxtaKVE0vra0gPA91wwrQ3t7u3WkUA1wcpVOpannuWRvrjIvVxBqufO973/PsO/z222+RkZFhLKFi1nRRQ/jQZh4AoG90Bzo7O5GYmKh6zQmXCVoLPe0f8LFduIWfzyZOGFOE9AZxI+WWjV8SpZVzv9TgOI4qmGgzItXrEoVBTWjpfe4vQi64mLVEN7Sto93d3YjVMOeZNtcphCZNr5F1dsUAbqSja2FEcKmSn4+s9nbYKLM2L6EnMc0YqW+9zmqq7OLA6v7MtDnKwPpItBATE4MFCxZg06ZNMscKKbW1tbL3Q931Iw5xek4ztbW1svod6v4rpqWZYfft2ydfs3X39aEjRkCgmPBrampkSlzW4MEAgCT3+nbLddcBBw9qpj9w4ACSk5Nlnw12C42DOunE67Jx4NNPXeW+/nrddKppxXt2pz20ahUwfLhm+kOHDskEiLTOCfSfV01Njbw/uPMePHYsAODA734HZGdr5n3gwAHPuJn16KMAgCR3e7kgWn80qK6u1lS6Ozo6vNqhGUIuuHzWEv2IkRmXssH7C3XdU3JdQzsFGARXVRUSBAHtBrRMGZKZJm3JlabpmTLXKdbHBEq5aRpstJoKRX71q1/hmWeewQMPPIB//vOfXm3I6zgJiSmVNtccMmSIpgIIANpXXIwYMUJ9rYznqW1q2LBh8oFQcV9pUoGowsiRIy9ZEEQlzG3iGz5njm7a4cOHywWAZJZKs/Zcf/318jpTmPoG/fjHuukHDRpEXetWe242mw3Dhg1TVyDdgvC6667Tzfu666671H4OH5Zdi6+p0U07aNAg9WcN9mNNQi64AB+1RPdfoz4rBw8e9Ni3lWlpltivvvoKdXV1mnkbSS92uKHuRsS5hQqt/IcOHUJKSorn/eC9ez150h7eV199dcmmD7nGZAfQetNNuulrampkeQ+VlJk2tH/55ZeyRqusMy0zo8jevXvRr18/V7lHjXKV261Znx0wANBZ5/r8889xxRVXaOZNG5xrampUF9pZtcRAs27dOpw9exazZs1Cjx49wHGcsXUGybqpKeuBCrQ26TWLV5gpTaHiqaqHTKFRpO2iOG90dXXJ+hITiqUGm3v2poWXuU58pu7+qPVEVJ+VZDuN7L0RFOvr8WPHAjrLJFqzLb+gu8sryJw7d47k5+eTtrY22eeqm9Hcmw7FV5vOhlIA8t9UpD1DSXvmzBl53na76+VO30hJ39jYqJn3YUraw4cPy/OWbLo0dc+EeG1+btbYfCy+mpubNe+7k5K316ZtxX1foKS/cOGC5jNvaGjQTdvQ0KCb9zlK3ufOnfPOm4T/BuS2tjZSWFhIZsyYQR544AGyadMmr++ollHSLtopddPe3i5Pq6jbZkp6rzYlSdtCSdvS0qKbdx0lfV1d3aW0is3LHePG6ab12pQuqbNWSr6tra265e6gpPfKW5JWoKSlbdruGj9eN71qFBn3OHDx4kXdtBcvXvRO6ybiNyD7rCUqYFlwN60lKmBxae9LSdu3L+0bvtNFcxtX7k+TmB6NmNv07tuIS7oX4myVYf8ZYGw9NBJJTExEeXm5+YQSTZrW82h9k1DSE50ZG+v2DtruvIvS/XuKWQ8oHoleSMzXrHVmGolp10hb1uuHemvo4nWPmVOx/aD7u9+l5q21r46VkAuu22+/HYsWLcIPf/hDOJ1OPPvssz5NMVk6jOmBTLF21ExJ39zcrGlmML1OJTFxsK7VmL5viYmCpb4Bk8Je0WG4CRN009IEV7R6FfoD1rVL0+1C0qZY2zNtFVq2Tq3oV6bvW5KeZvZub2/XHcCN3LdsTJQ4G7EKexZYFUgWQi64fNYSFbDMmmiWa5ptm2XGZbqjSzQ92kK43iK6T+nFgYPjmAcZUwOFYj2iQfFeSUNDg9xzzUIfiXepEUVKr13R9GuvAVwiAFhnLj47K4FtU7p+YCR66CSWvmR6/BD3r4lri1u3Aj46ntEsUbTrLETNeVwsbuVGNCY9WDT4C5S0Fy5of8P0Pbe2ygSf6UbPcZ5oIazmVZbOyuKJCbArKtEMSz8C2J4rqwnXJ/OzG9NKnN3umS2y9GGAvS+ZQjwHsKnJ9TLjnCE59www5tASKKJGcLHMPlg0NYBNU2RZ4zItMHlepuHqR0nUj6PIaqIwYl7VgtXUpx/bQSNCRDQjGYRZhTrLs2GdcbE8V9NjgKQv0RY2vJY+RNcIN4JOWwf064y1LxhZ4wpHokZwsWhbphueAtr2Vr0AoUb2kMmQdBhWUyGL8GHtMPrx2fWDk7IKTdZ6i2ZY+hHAttZsyrlChVApr6ZniorTz03Neljz3rr1krkQQIzSSUWBbLbncFxy5AFgz8nRTRvVIZ/8BYv5iFXLZFnQZnGQYBUeLB3dyACntfkQYKsz1vtmnWFHHZL7ZVmvBdjqVru1uK8r25NiPxOLqZFltmda8VUIiw7K/jO9AAim+7Bi31xsnz7G0yvWlnnKJuFA9qOomXGxmAlY7fosgk/9EAfJdeUxD5IZF6tnH4upkBVTayEKTY8bMkQ3Lc2bKejuyxEE64yLxXphesalMH2zCC6WvnSGkvbMGcU3FOVmWeMyrcSJa1tunJR1w3CNIhM1gotFU2T1kGMRfCwmM9NCz4+wDv6mFrR37JCHmzp0SDctNbAoJe9AuhCHJZI1LtNegQpYZlysipRps7sElj5Mi/2RRokOEkoLAKvSHiqipoeyeAaymkdYOgzLTNH0TE8yQAFsAwWrsDdlFlJoqKydjXWmGnVI6pfVcYVlxsa6h8zUzEXRF1jXuVlgyZvVE9OUoqLwShYo/SSQ+yGjRnCl0K6naH+DdbGepbOyNDzTwkMhAFgWw1lNSqbKrhhkWJ41wD5ARjOs670sSgGL9QFgU4ZYYJ0xBVXYp6ZeirwBwEZxtZdZH3r2vHSuXoiJGsEV1E18CvSXN4E+OgugLB6JrPs/WMxCrOY2U8/LzzMuyx1eG9ZBmMV6wepVWKd7FbJA2UpYLDYsfRhgG3+YlW6KZUQmNBXrY0Hdf6YgagRXKO3ELBsQTYWpUcDaYVhMhaz1zTLrMe19poClzqOdUIbDYlUoQuV0w7ouyCI01c9dllxXnsysMPex9ONQjrlRI7hCtTALsGlMLJ2VVdtiEVyss1SWOjfdWRVEa5Bdf8A6GLFYPlgFD4sAYUnLKuxZTN+mlTiFuS+eYhnRW18LpZITNYKLRYtm1eBZBmEWgctqWmEx94VyE3AoY0tGO6zPlWU/ZdBjb0pg6YesAjeo4eqUDhYU4RKuAaejRnCxSH9WDzn9g+SB3jpHzbNoeqwDMIvAZh1kQiXsAfYZWzTDum7BYkFgzfs8Jf3589rfCGrkDAUs1gvTHol+3PvGaiJlIWoEFwusWibLIMzSWVkFLktnZTWvsrgAswbZtda4tGFVCliUKVYzZaicBVhNZizrvaG0urDmzULUCC4WzYFVA2fp7CzlDmWoKtZBwtR95+W5Xm6E7GzdtLSBwlrj0oZ1Fs9iNmM1ubEoQ6G0fLCUm3UMYBFcodwPGTWxClkG0lBq4CwmN5YI6wDbvpmguk0rYqTF7N+vm5YmNFlnbNFMKBfcTa/XKGIVspgpjcwetCJghHIfF6sSFspA2yxEzYyLBVbNgWWtiCUt6wDMYu4LaqQBxf6RDspAQDNnsZ6/Fs2wzqRZrA+mD2RUeMixDOIs1gfWpQYWwceaN8vzCqWTkyW4ELnxulgFLosbLquWySL4Qul+HO2E8rmanjEpFBqWk4hZBABrJBYWiw+rkxSLiZQ1eg4LUSO4QmmrDVWUdVZtK2LWuMIo72gnlDH7WLelsAhdFgEQypmHaQctRcgnMIQ3C2Ww6qhZ42LRwkMZuoTF3Mc682BpeKyDfyhnXBbaGHmuev0hlJ6qLI4KLB66rN51LDMX00q3IpoOR1HK/XqWoB+JmhkXi/Bh9WZi6TAsmh6rdswy02StM5bOGsqjNyz0YRnMWGdcrK78vsI642IZu1hNhSx1Zu3j8gMsgxGrxsTyAFkEQCg7DKuZgOUMI1bnCis6vDass1mWNmVamVGcGpAKfVJTtb8RqaZCVgUylOcYshA1goul4bFqLaHaiMcalJRF4IbS+yyUGz6jHdbnyqLQmM5bEQWCZSBlUXxDWWes5joWwWeZCv1AKKMcswiuULqjhnIAZyk7q+CKVC/SYMDqqMQyEJpWxPw44wrl2hxL3qE02YcqGj8QRYIrlGfasLjhsuTNqumFUnCxdHbWtZBQ2ubDnVB62JruR7fc4nq56crK0k2vNwizrBez9kOWMYDVZM/yvK19XH6AZRBmPdeKxTOQZRBmbTih2rAJsAkPVtt6KPefhDusz5XFUYk1b5plJFAzF9Z+yCI8WOuMRWiG0skp5O7w3d3dePbZZ3Hq1Cl0dXXhqaeewsSJE03/DkvDYz0yPFRrXKwNh2WmGMrZHqttPVrXuPzRl1jrhsWMazrtjh2yt8mnTumm19vIG0pHAxaMKGGB2loSSgUw5IJr/fr1SEtLw29+8xtcuHAB99xzj0+Ci+UBsAqAUNnHQ3nwHiuhtI9Hqzu8P/qSkQFcbyBkEXxGvEV79ep16QNRsXJbRFiED8taM6uwZ5lxsfZhlrKzBvhlIeSCa9KkSbjjjjs8730dsEIZqZjlAbLYqFkXhVmEfSjrmzVvlplmOOOPvsQ6k2YRHqbN5g6H6++2bQAA2+DBwOHDmun1+lIoI4awKHFG6lvP3BfKAL8shFxwiY2xtbUVhYWFKC4uVv1ebW2t7P1Q919RH6A9wP3793tMBcq0tAXl3bt3o3///pp50wRIdXU1GhsbVdPSNKZjx46hrq5ONW+affrIkSOyCPG+3HefPn1U09OER01NjWxtT5k3TWjW1NR4BIgvacX6VktP4+DBg6prix0dHV7tMJzwR1+iDQhfffWVbCBV1i1NETt8+LDMu89sm5LOuLLc+/XEJ9VEOQ3hyy+/1OyHNOHx1VdfyUyNZsst3ZeozJvWLg8dOiQTnNL0tHIfPnxYtsamzJs2hnz99dee8UeZltZWjhw5Ihu7pLD2pZALLgA4ffo0Zs+ejRkzZmDKlCmq3xk2bJhm+nxsURnM1gJ4Fa750Id45pnREo1rCx7DX/EY3kAdeuMhvKvyq6+6f2MAXnhhgkLr2YKf4beYgn/gEAbjcaxSSb8UwEcAsvHii7ddajx5rmb+0rbxGI9PsWtTK/A/n6ukLwZQjdOnb8CyZVLnji0AgFWYhb44DGAygJ+ppJ+JkSNHYuPGdLz6qjwtALyL+5CEegCPAnhMJf1dyMnJwRtvJGHtWu/0/8IE938/c5dBykVcf/316NGjB5YsAT76SJ62N+rxF9znfvcSgJsV6U9iyJAhSElJQXExUCVJCwBZOAxglvvdKgCDZdfXrh2LVatcs9yHHwZOKtJn41MAz7rfvQvlGdbvvjsMZWWu4fDOOwFxifLVV2t12+GePXs0rwUL1r5EG4SHDh3KFAJt6NChupvL9Rg2bJjcVPif/7j+5ucDAPr87W9ARoZm+pycHM3TyGnCY9iwYZprZEbqTG+dnDYvufbaazXzpplXBw4cqFvfNEXj+uuv1ww6TROaN9xwg2ba2lq2vhRywVVXV4cnnngCixcvxs03KwewyIBuJ5a8UZwtxT/1UwCPa6b162K2AhaPIlZzXSjjQ9Kel80Wmc4Z/uhLRkxHfo1+IYG1PdPanN51FhNnKCPYsJo4WfIOqZMTCTFLliwh48ePJw8//LDndfHiRdl3du/e7Z0QkL3aXWOp5qu9vd0/aVXS11HS19XVXUprt7te7rQtlLQtLS2aeZ+mpD19+rRuubso6bu6ujTTs9ZZJyV9Z2enZtozlLRnzpzRzbuDkr6jo8O7vRFCampqVD8XUW2nQcQffclJqRun06mZlgCkkZK+sbFRM30zJW1zc7Pu/Z89e1Y3/dmzZzXL3UTJu6mpSbPcrZS0ra2tunXGUuem27Iib1PjjyKtqXFPAWtfCvmM67nnnsNzzz3H/DssGhOrpmdKy1R4QsFul4WtMQOrxmPEjV9r1sVaZ6F0DAnlcQyBxB99aYLCrOriktl9wgRAXj1ys/t0itn97rsTIVfiL5ndD2MwoGN2r67moHZ7L70EjB8PfP55DKBafpfZfdu2WPzud5fyFVmFWehHMbsTQrBmDVTN7n/FfYCO2b2tjSApCfj97+E2u8vLuJlidhfbs5rZPR31gI7Z/fHHY/D2267/1czuV1PM7gsWxHnq7GG8iZMY4LnmGrW0ze7Tp6fh9tuB5593vZeb3cFEZPZQFVjcQlkHslC5lbPmy2IqZHVnZ3FJZ/UK1F/Ch8yh5XKDrgyxDhnaOdCUT9o2hdhYfZNcXFzgIlDoQVOkQmlepaP9vEwtkfgb3flYmGDEVGjK7KVI20ZJ29bWppt3PSV9fX39pbQKU+EFStoLFy5o5s2SlsCkuU6RniWtaROHIi3M2E7DAAAgAElEQVSLWcenenMT7qZCI9D6Eg+QPhr10qdPH8LzvGZaAh/MR5K0pk1uCi5cuGD8uSrKzWL6ZjXXsfQl1rHLVJ0r0rKYdln7UtTMuEIZ3dmU1tOz5yVzIQA7RZPTm7mwaluh3ATMUuesG4hZQnRFO3ou1na7nWp+Zpm5hDJ6BUubYp2txUK7P8TExOhaPlitLixlt4Ls+gGWB8AauoTFq4g2EOhdZ13jYrlvVuHBInRZO0ykhvcJBl0AzmhcO3PmDLUvsJifWRUxmvee3vVQRlMh0L53QRB0TY3aV9zXKWbKUAofFizBBXathaXDsQguVk0vlIFFWYRuUGfIlxmsdcsSzSWUWyRYBABrrFIB+oIrkOHqWJTXUDo5RY3g4qA9GHIcpzsQBjVWocNxKVSN+N5HWIUHi+CyA8iCt4YdGxuLrKwsqqbGIrhYBU+0xir0F3pmKxqRGh2eRWiGMoRYKJUw6wRkP8CiMbFOt02ZR7Zudb3ch+Bx7jhrWuh1VtaGYwPwHY1r3/nOd3Q1JgJgOrx3/Xd3d2P69OmGXNJHa1y74YYbdPNm7azWGpc2dgAL4a0UxcXFYeHChVSFJJQDKYvgYt2Mr6c0GzHXaamYcXFxunWeAO2yx8TEUK1FLGNfKIN0R43gskP7IRBCAurkYIr8fNfLfey4nRK9W6/crAKXQPsssqamJqqwX+P+Pzs7GzzPIzs7GwCwZs0aQ3mf0Lh25swZ3fRB3Xd3mUEA/BneddTV1YU///nPAV0zYZ0J6x1bQrvO2ib0xh4aBJfiACoZOnQotR/20biWnp4e0GDXoQyyGzWCi2X2EcqwKZGKDa7Olg1g7969sNls2Lt3L7KzszF06FBD9u0Wjc+9jmhXcLkuSAcDAcAFjWsXLlwIqFLAfDAqZaDUu266TeTluV7wj9Cr0bhWU1NDDVX1rca18+fPU60uLGNXOHslRwwslWhEg5eldzdY8TgFMno0sHu3ZnpZw9u61fXXHRjUtmkToNMh9QSAPxwkhgPYpXJt+PDh1DWFrXDVnVhGUXgZXZRNhnqQUL1Tn4HQnn8U7XAAUqDucJCSkuJdt2Kkd3ckmB49elwKj6CC3hpXqD37dK8r24TExB9KRyXm+I6U9IFcYmEhamZcPPTtzHqN3nTD2bFDdvpqjI7QAiiBKkPoVcgBmASgsLBQ9nlhYSEmTZpkKEim8htGA2vqHaFB2y/EaqK4HGfIRrHBtXahjLGekZGBhIQE7zbV2up6iekpHnR6bZJlnQlgW+NiFT4scNBeL0pISNDtC/FwKRpqJCUlBfQcsVD2o6gRXLEAFmhcW7BgQUAjnfukKbqdNKhrQcrrbqcOwAfzqCSt+L7EZvPKgxCCkpISyq8DpQDmSspICMHcuXNRWlpKTcsBeBLwrIuJZGdn47777tPtrKyDjOUOrw0BMBWA8hSluro6TJ061bs9utdqRUIZM5RlH5dpc5+kL5kud2rqpZkqAKFnT80TmDs6OqhxOx/VuGbESYqlzkPZj6JGcPEAyjSulZWVUV1h9aIFeGkOiugXsRQvND2hSZsVeV2X5G1a41GUmyQl4WaOQ2VlJQoLCyEIAgoLC1FZWYmbb76ZaiZoBFAOYO7cuR6hVV5ejsbGRsOOIdXV1SgqKoIgCCgqKkJ1dTVaWlqodn09aHZ9a41LGwLgY41rH3/8MfW5spi9mJ8r5bn5dV+iQmCbQrElxjZqFEZpuMyPGjWK6t27VuPahg0bdBVfILQnP7MQNTYRI7MerYZrJK1MCLTKzw7m2vWPc9ONfsFxiI2NVTVvxcbG+te+rSg3Wltx0v0d6awJAE6ePKn72xyA5e7/y8vLUV5eDgAoKirC8uXL6SZQAGmK7y9f7vrF7u5u3fTMJiVKeq/nfRlhA5AKwAFAenKcw+FAamqq9yAq9in3IM6iFLAOokZMhYHYFOvTUoPyvY+zEwLgnPv/Xr164dy5c+jbty8aGhrQ0NDgLbgUp1MICQlAh9Z8L7CnNLAQNTMuFuzQrgibzebd2RQzF56yX0GvQxFCNDVJp9Op+/D9MXO41/23srISNpsNlZWVrs/vvVc7kRup8BIxIrRESgEsW7bM832O47Bs2TLMmTNHN512N3Nf1+mIFnS2ALhV8dmtt96KLVu2eH9Z0Rc4ymbbQIYwM229kGDa3C/xKrQDuEJD0bniiiuo4wd69lSeFWMYGy4pcg899BBsNhseeughV7nsdu97bmx0vdxwTVobYtzXA7jGzkLUCC5Wr0K9fRiqMxfJ7CVWJ4wNQJ8BaDUO1c8lZgYb9B1SqM4ZHIcVNpuqc8aKFSsMeefNVXwmmg29UFlfK+U4zJs3TzbbmzdvHlauXKmbr77PoYpXomJNwUYx7UbqeVz+QHymlYDMhFtZWan+bBVmL2HkSN3fD+S6B4ujE4tHowDgjIbyeebMGe97VggPcuECOoYPV03f0dFBnbnc6P77u9/9DjabDb9zH6A1bNgw7y+L+0jdcBMmeH9HAi30Wn+Na/3797ecM4xgg3ZEhMTERPlgpBhEbTabpn3d6XQaGsh8DZPDcRwyMpQ+XC4yMjK8G44YeSM1FVxyMuwav6/qmae0y/M8SgUB2xTRO7Zt20Z1sBAHuHLIB7jy8nL1AU6RN+F5NBIi+764RkZb4zK9kK4YKOwtLZrKRGxs7GW9xuUx4QIyE25RURHS0tK821RVleslfe8jzPuhzDo6SWAxU5pWfNPSXC83XHo6ju7bp5r+6NGjVIE8VuNzpeOTGizCvhvAKY1rp06dCugG5Kg5j+si5WwY2RHmqamulzttV2Ii4TTScRznfYS9ogxOp1M3b6+jtyUIgkDGjBmjmm7MmDFEEAT1hHl5pGv8eN18vcqtOAuM5ziSrZE2OzubevZSCUCKAE8ZBUEgRUVFpKSkxLu8iryJ3U4Em40UFRXJ8i0qKiIHDhzQzdf0uU15ea6X+LxuucWn53U5nMclvgTF0KDXDqV1y996q27derUpSRlMH2GvoLu7m8TExKimjYmJId3d3Ze+rBgDupOSdPOWpZWSmkouJiQw9UPBZiNjNNKqjgGKZ8UDXv04Ozub7N+/X7uy3Pff1dVlvOyKfE2fIybBOo/LjSlTYVOTZ3ESAGLa29FPI12/fv3o0cZpx9RTrms5Qug6SGzdaj7OoXI9IjkZCRqzSdr+EcC1RrVcko+onavO1lRme5wgeBwyRIyskbHu42LZ73O5YHh/nsQCgNRU8K6z5TXxqtuYGNfLD9hsNk2X97i4OLnlRGHiJJTZCVHO1kSTW1MT4jo6kKhRP4mJid4zeGVfEASM08h33DitK+5ywWX5qFZ8Xl1djbKyMnoUGQZPTNY1SRaiRnDpPx6VhqdI21fjWt++fbUbrRsWO7EgCGiUmLGkNDY26p9LZbdrRp7u2bOnd6NTdFbuxhsx6corQ7IBGXB3urnyVTLNNTIJpl2XFeasmP37ddNfrh6FTLjblt1u1wxSbGQQ1xomjZjrbTYbbrzxRtVrN954o/w3/GjitAGY17+/12Bqs9kwb948dU9MST1wdjuqob6nsbq6mtqn1CLfAMA+NfOjROCiqQkkL0/XMU2vL1oHSfoBUy7SEo8gAOBycxGjEYomJiaGatfnqqs1j3OIj4/X98yx2ZAmsXdLSUtLo3bYlBT1ffNanyspGThQ/XMDG5BZEDXF8vJyrzUyI5qiKRQzbLS0aDp40MJNWWggzrygHcxW9XOJ44wAbScJ2rlUgGs9+vPPP1e99vnnn8u9dxVtgmVLCw/g1W+/9dqeIggCXn31Ve9ZptPperkh3d24OGYMqqvl86bq6mpcvHjRcF+Q7sU0g95ZYHqwRCtiJWpUSyPmI48mrjCx8du3e021Raqrq7339YizFvfvcDfeCG7HDpflV4HNZqPu4xo0aBAyMjJkGtLIkSORnp6um5YQgiYNd1Yxujst/dyvv0bF9u2e/VSig4Ro9gvUdJ+Da5+Qw+HwuMQvW7YM27ZtQ21trX/zVcTTs6WmIqelBTsVnctutyMnJ+ey9ipkRe/sO9VrUpM9XAqoWl+OiYmhzoT1ZgiEEG8HLUA22+OgbrlRvR/FXqx6jUG+vr7e+0OJtQYASF4eug4cUE3f1dWl24/FsG1jAY8n8IoVKwC4BLmqcxdwyTlk61ZwsbGq9UbdfuB+qYknm80W2H6kuwIWJhhZUDa1sKuyEB2rkS42NlZ7Ydq9wMnqnFFYWKiarrCwUDtvQgjP88Rms6mmtdls2gvheXmk9aabCCGElJSUkKKiImMOFop687yMoFLnRbjkkCHmC4DMnDlTft+KtC2UZ93S0qJbBqfTSTIyMlTTZmRkXH7OGSqOM8Ru9+n3nU4n4ThOtW45jvOuW4WTQV+NZ5qenq7dnt0IgkD69++vmr5///7yNqXiVNJDI+8ePXp45y1x7jDtVKJwDCGpqSQFIHa7XZbObreTlJQU7xtV6YNqzjSq7VVx30JuLknWGEOSk5N1+yEPkESNe05MTNR9Xqx9KWpmXIR2XapRKDRwpKRgVFsbdvG8Zw9TcXExKioqMGrUKO0fdc+8jBynEAh7LyEE8fHxqkeDx8fHa5sYqqqQ4NYQS0tLZRpdwGZaioj6XF4elhMC3HijV+SNgoIC3fxN29YVGq594kQM6+zE5wkJss3KCQkJGDZs2OXnDq806fjJxNO7d2+cPXsWmZmZ6jMPQNYuCIAGjd9qbm6mh5viODz55JNYt26dl/Vi+vTp8jalsLpwH3+MNKhHxVfdBiCuS3McOLj2LZ065e0c3r9/f+rMg+d5dMPbcYXneXR3d+tG/fGUX/neYP8lhIDXqFee5+lWG53fDSRRYxOJAXClxrUrr7zS29SndFIYMEC28XbFihV0JwW3XZ9mwqBFh09PT1d1kKCZCu12O5555hmvjmGz2fDMM89oN3aHAx1DLx1dp8wjkN5Aynx88SpkXRQmhGBUSopXhI2Ojg6MGjXqsj7WhBW73Y4rr7wSdrsd9fX1iImJQX19vexzLWwA4jWevZdXoAqEEPz73//Gvn37ZOs9+/btw7///W/d58oDOK1x7fTp097rNRIHLQ7AvRq/bSgCDcdhkMa1QYMGGeqPytyNtmFCCASN7wqCQHVq01LZu7u7rZBPRuChri0BwMWLF+UNT7KQLL4vPXZMFi1CFF5GIp2zusNrOULQHCQEQcD69eu9fl/rc09n27YNSZ9/7uUdGVAUbtPYuhVkyxafvApNCy4xb7dTDrdtG7649lqvjd8ZGRn44osvgia4oxFBENCrVy/V2UOvXr2826TE0YnAdUabGklJSYYGQsNbSxTRVFhDVVW1tqK3ot317t0bVVVV3mlVFOceGg4tPXr0MLQtpRiXhBUhBMXFxdQINIBLyU3XUKzT09N1lQVxjQsAZs+eDUEQMHv2bM/vXhYhn6qrqzFz5kyf09vhipyRoIgbmJCQ4O2Gqxyw3e99nXmoxgST/IaelkmIK2JERUWFzLuuoqKCOohzHKe5o5/mzRhqxPv2xatQP8AW0EkJwSUIApqbm1FXVweHwwGe5+FwOFBXV4fm5ubL+lgTVmw2G6ZMmeLV5u12O6ZMmeLdTxQhzAquvFJVANx///2GBkJxhlNRUQGbzYaKigrZ5x4UYdu4tjZdT1O9CDbdSUlouvpq1CuEdX19PZqamqjtieM43J6Wpjp23X777VRT3QYAFQCKi4s9QquiogI7duwwFE2kWaN8Rsyz4uYD8XmLf7W2JfgN3RWwIPGHP/yBTJ48mdx///2q1404ZwgAGQu5U4Po9DB27FjdRUbPy0dM7T5XwZSDhASn00l69OihuaCs6RSSmkqcycnmb5Sl3hSLwiQvj5QMHKh63z/96U918+2iLIbT6psQQhYvXkwcDocsncPhIIsXL9ZME7XOGX7sD06nkyQmJmou2Hu1SUm7EADSPy5ONW3fvn11HZVE1JydVJ2cVBwkno+NJenp6bK06enp5Pnnn/fOSFHuscnJquX2GnsUaQlckVwyNCJ+qDoLKca972j0A9U6U3FKidFIHxMTI3ewUGkjgrt+qfWtICqcM7KyslBZWYmf//znTL8jGgOIZMoM0I/oYIUlKjXgu4OE3W5Hnz59cPbsWdksIz4+Hn369NF2UmhqcpnbxPdSs2kQKb3qKhDJfYr3ffDgQfkXFa7L/tix/8ILL6CkpERWR3v27LFc4Rmx2WxITU1Fu8q+KNVjUSQQAE4GRwFTSJwrAIBcuICm4mJccM/QRC5cuEDdWsIBmNSrF8YmJ6Pi2289n2uuU4v9zd3/bFu2oDM1FWhpwZw5c1BRUYHCwkKsXLkSnZ2d1DrrC+BblWu9evXyLrcib2HzZvBxcVDbysPzPARBuJS/cguB3Q4OLjf8Ckm9GQnQzUpYCK477riDKlxqa2tl70XXArF6CIABcAV9rKys9BzPAbhMDdK9QYPd9mR7SwsAgHe/P6zIwyi04J/79+8PyNHfhBDk5eXhzTffxMyZM7Fw4UKUlZXhzTffRF5eHmpqamQNKMs9mIgmkTb3+28M3rdanQPAQQPpNfNWCim4nCSkz1uZr91uxwCeh1qLyczMxFdffUUtDyEEZWXyo0cfe+wxLFy4ULPTKcsVrlRXV+OVV17Bm2++aSyB0stWsvZjFkIIMjMzcfq0t6tDZmam9kAaEwOOEDzwk5/I+q6IkUguRGImkyK+lw2oyrXd/Hzs+uIL1d/dtUslNoVEALS1t6Nk1y4UFxcDiryNbOTn3Kck1NfXo6KiAhzHecrcu3dv6tEiuwH0g/zU6oyMDLzzzjtURcxut6Nfv36qz6tfv35y5VfF+5RAPfpNIPeAAmCwj/mZEydOMJsKCzWmvJpTVwaTiBSWfVwiyvIZMYsQQkheXh5xOByeKT3P88ThcJC8vDy1L3uZ64ja97Tws6lQK28vM4IiP5bgnoQQ2Z4x5R4yqemSWi4F4WAq9MnsrmI2I6mpPpchNzfXa59cRkYGyc3NpeadZ7eTbMU+sOzsbDJ69GhqvoIgkLFjx8r6vOZygcp+JtHcR00rJS+PtIwe7VN7Uiu/3nsPinFPK0DviBEjqHmbCvKtMuZq7cWk3XdUmAr9gXio4TZ3fC+R7OxsbenPoFlKsdls+M53voNvv/WesPfp08eQqbCxsdFTTuJ2XEhLS9P1aiSEwOFwoLy8HPPmzcPy5csxb948VFVVoaioyL+mlTCC1miNHCWTlpamevqy6p6dCMKI2V05axzsXpwXdWve/d4XCwQhBFlZWdi+fbvs87q6OmRlZXlZAaR5EwAjOA4rFWar6upqPPjgg15p1cjJycGgQYMwa9YsHDx4ELNmzUJDQwNSUlLkJuhXX3XlP9Z1KMjh115DzsqVuK6pSZa2vr4eqamp3uZrye90dHSg+49/xMyZM1FQUICDBw+ioKAADQ0N6O7u1k7rI1ILBAFwTON7J0+eRE1Nje74IwgCjhw5onrtyJEjsvRKywfgOi3bl/tmtl7oirUgEpIZl58QBIHExsaq5m2323Xz9lX7V0sPhfajiSRyhilCNeNSWVBO1IjOQNuxL8XsLDcSZlyE+NCXWGfiEqTRK55++mkiCAJ5+umnCaASvUJKairhk5O9HGbE19ChQwPzXCVRQkpKSmRjhTjrojlJie3CV6uJaSR9jwdIpsa417t3b2qd8TxPMjMzVdNnZmZSnTMI4NN9R82xJgMGDMDatWuZfuNvWp//TeuKfxAXMdUghOgGm5Qe1FdeXg6bzeZxETdiJxZj/EkRY/9FDYp9d9y2bRh+002qXx0+fLjhew/VxuuwQy1SOkO0dBGp040RbDYbLly4oLq/rqmpybDjjKHnKm6JcUenJ3l5aPzjH2XbUMRtKo2NjWG7Kd0G10xIeShLdnY2rr76akOnoGtZKFQDjGv8ht77QBA2gosVAnhOMc7OzgbP855jApxOZ0AbnhicVY0bbriBHq7FxwgSgGvxV5l3Tk6O/qLw1q345o03qL8dNqicGPtpba3qMRCffvrp5SuAfEUZPV/53iSG91JJaWwEuXABmZmZqKurk12qq6tDRkaGf/uwygkPy1tafFYgS0tLPfuoAHgcRYwEMGBlC4BcxWe5ubl4w0AfF8NkqfWlJ598Mnz7EnVOFwYY3Xuy2L2QC8XCrt7eHH9RUlLiMYmIr6efftp7T5IKPpn7yCVHDAAeBw3ley1oU3VVWEyFIgbMUF5lUyzgCykpZKxGYFDdhXRGotZU6I/nKsHwXiopeXmEv/VWv5gKDaFhHhUEQZavkbZ04MAB404h/kDSHwSAFLn3vlGDVatgaqnCj+3Ecs6Q8AKAkr17ZTOcvXv3BmVvzvPPP4/MzEzZZ2+//Ta2UvZIESKPICE9WgTQn3nZbDZMnToVAFBVVeW5b4fDgalTp0bPniTFMTIkOxvHPvsMUDHPHjt2LGqdUoxi2uyuCIAsPasumCjbs4jD4cD48eP9255VjvcQ+6IUo67dY8eOxa5du1BRUSFzxx/rdv7wK9KIHwDSurtRJNn7KVpvuru7DS01GHZUCpN2AiC6ZlyCW0uAyZkLK9JjMsSd7uL79PR0qju8r5EzRHiel92zEc00ZDMuA3iVTXHshmCzaboAe7nwBrJcCsJlxqWHbhnhfTyGWUwf06Pilj7niitk6ebMmUMOHDjAVC5NJDMtX52kampqfJtl+orK9gVBcfyJ5rEmGphysPBDv48a5wxWCLRP1DUSuJUFu92O7u5uJCQk4PTp07Db7Th9+jQSEhIMHWlSWloq0+pErceIfZwQgnnz5sk+mzdvXtguJvtEz56ulxsuORl3xsfj6aefln3t6aefxp133nlZz7Z8QhHpnDX4srhhV3kir+pGXgkEwLi9e7FSsRl25cqVePDBBwPaprVmHkVFReG3RUIRpBcOBzhFbECz5Y04RyUmsRkkjM64ShTakdmZi6/oaWtG7MyByJd1A6AqoZpxiUhcl4Oq4dLK5SZiZ1x+PEiSEJNu5ZIZFw+Qfu5tJco124yMDP+ucSnzd+OLa/eBAwd8PgzWJwz2Q5/6uB5+3KjO2peiSnARlcXUQJsJpfmomSkDZuJw46uZMSIFl7ujsO5983u53FiC6xKG+6HCVLg4K4s4kpJk/cjhcBhycjKFH/euBd05w+Dz8rvg8mP/t5wzFIT6UETRqQKAesBYPxO0E4zDAXdgVA6uhePCwkKZWYcQEn5mnUjglltcf8VFd/E9A4b7oSLo6wtbt6JEELyCHx86dIi5TIGC4zgkJCTA4XDI2uP27duRkJDg//YYgOdlCD/GtGQlata4Qg1x79uQIt3XEUgizj5tEV6IG7ztdhC73fug1SBCgrVmqzhclOWeCXGFXquqqvKUVQy95nA4/F/2AG0Yp9LYeCmqvtr7YOLTPC/IBGPvCQt6wT1HjhwZNHOlGXwyIwTApORL2SxToe/oltHXUGB+IiRrxQwmQhHRqzBoHs2hMhWKhIFXYdSZCi0CiMqxBqFAutekvLzcY541GuXAQoOqKiSE8PRnvT1FRvYk+YSfZpZaSwUBKbPoYSua7CQet75CFHsfle8BqB4FAyA0s3MmsRkkwn3GRYi2l1ugnTN8JSKdMxT4EuUgkOWK+BmXrydj+xk1x46AzR78QNBnXCIUi4fROjPs4OVHi4u1jytM4DgOK1askH0WjJNAg4q4HqD1PogQjSgHJJr2rwULMRZkU5PrcFVFbMhgE2lrtmJbDPoeUsX+Rl8ghKCxsVFWVvFewjm4cOSaClWOkQ4lWgNpQUFBiEoUAEQvJq33QUI5UJgJk2Vh4W9Cdr6bHxwjTJndA2Ci9JXIFVyhcglVQW8gbWhowBtvvBEdA2mYKAvRfBBkSBAHwLQ08IIAe6g8xSKYSN6WEtT1OT9hWHAdP34cFy9exNChQ+lfDgZhov0DIVpUDgVO98Ex4v2I70NAJA8UFtFJpJk4RbSsReHcnwwJrj/96U/49ttvwXEczp8/L5PMISNMtH8RrYE00BuQQ0KI61okUgeKsKWxEYdrazEs1OWwCBqmzO6KUxpk8RKDjKbg+uMf/4hHHnkEcXFxOH78OObNmwebzYYnn3wymOXTRqL9EwBcCLV/kctmIA2DurawsGDHlNld5SiYUKEpuG688UbMnz8ft99+Ox555BEsWbIEHR0dKCoqCmb56ISJ9m9hYWERiZg2u4dwpiWiKbhycnKQk5ODv//971i5ciVmzpypeTx9SHE6cdAyb1hYWFj4jClrUQhnWiKa+7gOHz6MX/7ylzhy5Ajmz5+PPXv24Nlnn8WJEyeCWT4LCwsLCwsZmoJr8eLFuPfee5Gbm4sVK1agoKAAP/vZz/DGG28Es3wWFhYWFhYyOKKxNfrRRx/FxIkT0d7ejrq6Ojz33HPBLpuHPXv2hCxvCwszhKU5XYLVlywiBb2+pCm42tvbsXPnTiQmJmL8+PHR6yFnYWFhYRFRaAouCwsLCwuLcMQKsmthYWFhEVFQI2d0d3cjNjY2GGWhIggCSktLcejQIcTFxWHp0qUYOHCg5/ratWuxevVqxMTE4KmnnsKECROCUq7u7m48++yzOHXqFLq6uvDUU09h4sSJnut/+ctf8O6776JXr14AgBdeeAHXXHNNUMoGANOnT0dycjIAYMCAAXj55Zc910JVZ++99x7ef/99AEBnZydqa2uxc+dOpKSkAACWLl2KvXv3IikpCQDw+9//3nMPgaS6uhqvvPIK3nzzTRw/fhwLFy4Ex3EYNGgQSkpKYLNd0vU6Ojowf/581NfXIykpCb/61a88zzicCdd+BIR3XwrHfgSEZ18KeD+inZsyefJksnTpUnLo0CHaVwPOv//9b7JgwQJCCCFffPEF+clPfuK5du7cOTJ58mTS2dlJmpubPf8Hg3fffZcsXbqUEEJIQ0MDyVOcqPqzn+K3xnQAACAASURBVP2MfPnll0Epi5KOjg4ybdo01WuhrDMppaWlZPXq1bLPHnzwQVJfXx/UcvzhD38gkydPJvfffz8hhJBZs2aRzz77jBBCyPPPP082btwo+/6f//xnUlFRQQgh5B//+AdZsmRJUMvrK+HajwgJ374UCf2IkPDoS8HoR1RT4QcffIBbbrnFswn5nXfeQVtbG7NE9oU9e/bg1ltvBQA4HA7s37/fc23fvn248cYbERcXh+TkZGRlZQUtTuCkSZNkEUXsimgeBw4cwB/+8Ac89NBDWLVqVVDKJHLw4EFcvHgRTzzxBB555BFUVVV5roWyzkS+/PJLHDlyBD/4wQ88nwmCgOPHj2Px4sV48MEH8e677walLFlZWaisrPS8P3DgAMaMGQMAyM3NxSeffCL7vrQ95ubm4tNPPw1KOVkJ134EhG9fCvd+BIRPXwpGP6KaCm02G3JzcwEA7777Lt5880387W9/wz333COroGDQ2tqKnpIzYOx2O5xOJ2JiYtDa2iqb/iYlJaG1tTUo5RKn4K2trSgsLERxcbHs+t13340ZM2agZ8+emDNnDrZs2RI0U0JCQgKefPJJ3H///Th27Bh+/OMfY8OGDSGvM5FVq1Zh9uzZss/a29vx8MMP4/HHHwfP83jkkUcwfPjwgJ9McMcdd+DkyZOe90QSBicpKQktLS2y70vrT+16uBKu/UjMTyxjOPWlcO9HQPj0pWD0I+qM69e//jUmTZqEzZs348c//jHWr1+Pt99+G//v//0/UzfjD3r27Cmb7QmCgJiYGNVrbW1tQVkTETl9+jQeeeQRTJs2DVOmTPF8TgjBo48+il69eiEuLg55eXmoqakJWrmuvvpqTJ06FRzH4eqrr0ZaWhrOnz8PIPR11tzcjP/+978YN26c7PMePXrgkUceQY8ePdCzZ0+MGzcuJBqs1A7f1tbmWTMQkdaf2vVwJZz7ERCefSmc+xEQ3n0pEP2IKriuuuoqvP/++1iyZAmGDRvmKcjKlStNFd4fjBo1Ctu3bwcAVFVVYfDgwZ5rI0eOxJ49e9DZ2YmWlhZ8/fXXsuuBpK6uDk888QTmz5+P++67T3attbUVkydPRltbGwgh2LVrF4YPHx6UcgGuWXJZWRkA4OzZs2htbUWfPn0AhLbOAODzzz/H+PHjvT4/duwYZsyYAZ7n0d3djb179+KGG24IWrlErr/+euzatQsAsH37dowePVp2fdSoUdjmPuJh+/btYb/5WCRc+xEQvn0pnPsREN59KRD9iLqPa82aNfj666/x7LPP4oknnsDUqVMxffp0X++BCdEb6vDhwyCE4KWXXsL27duRlZWFiRMnYu3atVizZg0IIZg1axbuuOOOoJRr6dKl+Ne//iXzbrr//vtx8eJF/OAHP8C6devw5ptvIi4uDjfffDMKCwuDUi4A6OrqwqJFizznqT3zzDOorq4OeZ0BrqNzYmJi8NhjjwFweYyJ5Xr99dexYcMGxMbGYtq0aXjooYeCUqaTJ09i3rx5WLt2LY4ePYrnn38e3d3duOaaa7B06VLY7XY88cQTeO2118DzPBYsWIDz588jNjYWv/3tbz2DWTgTrv0ICN++FM79CAi/vhTofkQVXPfccw9Wr16N+Ph4dHd34+GHH8aaNWv8epMWFhYWFhZGoZoKbTYb4uPjAQCxsbFW6CcLCwsLi5BC9SqcOHEiZsyYgZEjR+LAgQO47bbbglEuCwsLCwsLVQzFKqytrcXRo0dxzTXXBNwl2cLCwsLCQg+qqfD48ePYvn07/vvf/2Lz5s1YvHhxMMp12bNr1y5Mnjw5qHm+9957mDVrlul0//rXvzB16lRMmTIFjzzyCI4dO+b/wllYmOTkyZMYMmQIHn74Ya9rCxcuxJAhQ9DQ0KCZvry8HOvWrfM5/8rKSowbNw7Tpk2TvV555RXddL/4xS88m3Sfe+452QZxCxdUU+GCBQswYcIE7N27F3379kV7e3swymURIZw/fx4lJSVYv349+vXrh7feegtLlizBn/70p1AXzcIC8fHxOHr0KE6dOoX+/fsDcG3K3bt3LzWtNIKHr9x1112mlf1f/vKXnv8/+eSToAd6iASoM66EhATMmjULmZmZKCsrQ11dXTDKZQFXByssLMS0adMwc+ZMHD16FAsXLsRPfvIT3H333fjNb36Do0eP4vHHH8cDDzyACRMm4KmnnkJnZycAYMSIEaisrMSDDz6I2267DW+//bbnt1etWoVJkyZh8uTJmD17tme3+vnz51FQUIApU6Zg+vTp+Prrr3XL2KdPH+zcuRP9+vWD0+nEqVOnkJaWFrhKsbAwgd1ux5133om///3vns82btzoCdxLCMHSpUtx//3346677sKdd97pOWxz4cKFHgVMry/5QkdHB+6++2787//+LwDgnXfewZQpU3Dx4kXMnDkTGzZswPLly3Hu3DmP673FJaiCixCC8+fPo729He3t7WhqagpGuSzgiiDw2GOP4YMPPsDkyZPx85//HICr0f/zn//E/PnzsXbtWkyfPh1r167Fxo0bcfLkSWzduhWAa+9Jeno6Vq9ejYqKCrz88svo7OzERx99hPfeew9r1qzBP/7xDwwYMABvvfUWAODEiRP4xS9+gb///e8YPXq0oZlTbGwsvvzyS+Tl5WHt2rWqphkLi1Axffp0fPDBB57369atwz333AMAOHr0KM6dO4c1a9bgww8/xD333IPXX3/d6ze0+hKNDz/80MtU+PHHHyMhIQHLli1DRUUFtm3bhhUrVqC8vBw9evTwpJ07dy769u2LV155BdnZ2X6oieiBaiqcM2cONm/ejKlTp2LixIkh23x8OTJkyBCMGjUKgGs/XWlpKfr27SvbWT5//nzs3LkTr7/+Oo4dO4Zz587JzLmiZnnDDTegq6sL7e3t+PTTTzFp0iSkpqYCABYtWgTAtcY1cuRIzxEXw4YNw6ZNmwyVdcSIEdi5cye2b9+OWbNmYfPmzRETAskiuhk+fDjsdjv279+P3r17o62tzRPZ4pprrkFxcTFWr16NEydOYNeuXZ54iUrU+pK4VUgLPVPhkCFDMGfOHMyaNQtlZWVBPeoo0qEKrn379uHJJ58EANm5OBaBRxrjCwA4jkNMTAwSExM9n82bNw88z+POO+9Efn4+Tp8+DamjqNixxP13hBDY7XbZfrzm5mY0NzcDgCdmnZiG5nR69uxZHD58WBbduWfPnvjmm2+CGtrKwkKPqVOnYv369ejVqxemTZvm+Xzbtm34/e9/j8cffxwTJ07ENddcg/Xr16v+hlpfYuWrr75CRkYGqqurrUmBCaimwm3btoHn+WCUxULBoUOHUFtbC8AVeisnJ0dmSgCAHTt2YPbs2bjrrrsAuA5woz2v8ePHY9OmTZ4I1pWVlfjrX//qUxm7urowb948HD9+HADw2Wefwel04tprr/Xp9ywsAsG0adOwYcMGfPjhhzJv3S+//BITJkzAjBkzMHz4cGzevDlo493GjRuxa9curF+/Hjt37sTmzZu9viNG7reQQ51xXbhwAbfeeisGDBgAjuPAcRxWr14djLJd9lxzzTVYuXIlTpw4gd69e6OsrEx2zg3gsoPPnj0biYmJ6NmzJ2666SZ88803ur+bl5eHI0eOeGKWXXfddViyZAk2btxouoxXXnklli5diqeffhocxyElJQWvvfaal4C1sAglmZmZuPbaa5GcnCxzHrrrrruwdOlSTJkyBU6nE9/97nexceNGCILgl3w//PBDj7OHyBVXXIGSkhKUlJTgtddeQ69evVBWVobZs2d7WSn+53/+B/Pnz0dpaSluueUWv5QpGqBuQD516pTXZ6JbqYWFhYWFRbChzrjef/99r8/mzJkTkMJYhCcvvfSS51gCJYsWLfI6A8jC4nLgs88+w8svv6x6bezYsXj22WeDXKLLB+qMSzQLEkJQU1MDQRBkG+QsLCwsLCyCCXXG9eCDD8re/+hHPwpYYSwsLCwsLGhQBdfRo0c9/58/fx6nT58OaIHUUC5uWliEK+F+CrLVlywiBb2+RBVcixcv9uznSUhI8ERvCDZ6N1FbW4thw4YFsTTGCdeyhWu5gPAtG61ckSIUIrEvhWu5gPAtW7iWC2DvS1TB9cc//hFff/01rr/+emzevBnjx483X0oLCwsLCws/Qd2APH/+fE+ARzHIq4WFhYWFRaigCq6zZ896Nqr++Mc/xrlz5wJeKAsLIygdYv0RgsfCP1jPxiKQUAUXcMlB45tvvvHbjvJwxOpskUNpaSnmzp3reUaEEMydOxelpaWhLZiF9WwsAg5VcD377LMoLi7GLbfcguLi4qg1FVqdLXIghKCxsRHl5eWeZzZ37lyUl5ejsbHRUjhCiPVsLIIB1Tlj2LBhePnllz3OGUOHDg1GuYKKtLMBwPLlyz2draioCIQQWTR1i9DCcRxSU1PhcDhQXl7ueW4OhwOpqanWswohHMdh+fLlACB7NkVFRVi+fLn1bCz8AnXGJT19MxKcM0yZ+/Lzgfx8T2crKipCeXk5bDabR2hZnS38IISgqakJVVVVss+rqqrQ1NRkafUhRiq8RKx+ZOFPQu6cwfM8Fi1ahAcffBA//OEPqZHN9SgtLUVxcbHM3FdcXGzI3MdxHJYtWyb7bNmyZUHrbNb6mnHEZ+VwOGSfOxwOU8/MqvPAIJoHpUjN8BYWrJhyzjh+/LjfnTO2bNkCwBUTsbCwUDNoJQ1CCDZs2ICKigqP8CouLkZFRQU2bNgg7zTumRa2bXO98vNRMnCg18bMnJwclJSUaGcq/g4j1vqaOQghGD9+vOqMa/z48YYGSKvOA4N0TauoqAiCIHgsGZbwCl8iTYkz5Zzx5JNP+n0D8ve+9z0sWbIEAPDtt98iIyPD598aO3YsAKCiogI2mw0VFRWyz7UQBAHr6+tRVVUFh8MBnufhcDhQVVWF9evXB9ST8nJfzPa1w5w4ccLU58o8Luc6DyQcxyEtLU1mZhfN8GlpaZa5MAyJRCWO6pyRnZ2NJUuW4K233sLOnTtRX1/v/0LExGDBggXYtGmTR9goEU8CVqOjowMHDx7ErFmz0NDQgLfeestz7eGHH8asWbNw8ODBSwlefRUAMNgt0A6vWoWbKytx8f/+D1VVVbDb7QCAIUOG4Oabb8ahQ4dk+WU9+igAIOnzzwEAbWPGAAC+eeMN1bLplR0ACgoK0NDQIFvMnjlzJgoKCuTl9iNGyhVoVq5ciZaWFixcuNATVqysrAw9evRAcXGx1/fFej/2l78gJSUF3377rdd3UlJSUFNTA5tNXyfzpc7Doc4igdLSUgiC4BFSommX9kwsgk/EOqYRDTo7O8l7771H7r33XvLQQw+RKVOmkIsXL2p93S+cO3eO5Ofnk7a2Ntnnu3fv1k1XU1NDCCFEEARSWFhIAHhehYWFRBAEeYK8PNcLcL3y8kheaioZOXKkLO3IkSNJXl6ed4Yq6Yna9yRlo8HzvCxvnucNpfMVo+Wioaxbr7pW4q4rQRBIUVERAUCKiopk72fOnKn+O5J6Xrx4MXE4HLI6czgcZPHixabKLk1PKzutzmjtNBww2pdYKCkp8TxTQojn2ZaUlPj8m/5qr4EgXMtmtFyGx80glo3WTjVVoNtuuw2HDh3CK6+8grfffht9+/ZFQkKC3wXnunXrsGrVKgBAjx49wHGcZ8ZjBiJZ05IiXfPSQhAE1La1Yd++fbLP9+3bh9raWm9T4datrldenuslvveRkpIS8+trocS9tsfqDKPlySnOwJT5Sdck8de/gud52W8q32uVWyyr5UCgID/fM6v1FWKZYSOOF154wdTnYYGWRFu1ahWZMmUKmTNnDtm6dSt54oknTElUo7S1tZHCwkIyY8YM8sADD5BNmzZ5fceIligIAhk7dqxMWxC1iLFjx6prD3Y7IXY74XneS3uHRIvXnP2kprpelLLppZXmLealfB8IpOVyOp2ya8r3XqSmEiElxVx9a8xS1WaaXnWmSMvfeitJ4jjV55WUlKRdZwZme9KZgl6dqRHxM668PNJ6003MeUjrU3zp1asRwnVWQ0j4ls1IudRmW+IrkLMu1r6kucZVUFCAgoIC/Oc//8E777yD/fv34ze/+Q2mTZuGwYMH+01wJiYmeuyrLHAch0mTJmHs2LFYsWIFOI7DihUrAADp6em6dlqbzYapU6eCEOLZswa41vemTp2qbZtXuGP7gs1mQ0pKCjIyMmTraxkZGUhJSQn4usBVV12FtrY2nDlzBna7HTzPo1+/fkhKSsKxY8fkXxY9KJuaAADkwAEArlmtdKZLDGrVJUePYr3KTHP8+PH43e9+d+lDcTablub6/S1b0Baj3nTb2tq88xfLvW0bAICbMAFVVVXIzs72uM8vW7YMW7duRVVVVXja9AOJpH6SpO99tCKIs2lpv7b2cVn4E6pzxpgxYzBmzBg0Nzfjgw8+wM9//nOsW7cuGGUzRn4+strbgf/8R3VRePny5d6Dv3sAhGhaSkvDluZm1CgGw5qaGqSkpHhPmRUDofie/+gjmZlT1XQl5i0O/qmp6GxrQ53iu3V1dejs7DS0OKr8DjWNu874Tz/F+fPn0d7ejn79+uHMmTPo168f6urq0N7eDp7n5WZbhfv5uK4u/Efl58eNG+f9oTgIuutK+L//wx/698cZtyfnnj17kJOTg6qqKpw8eRKVlZW6QjsmJgZOp1P1cxqEEBy5eBGnqqtRXFyM8vJyFBcXo7q6Gv379w/fBekIgWiYYY0KL7X2bBEYOI5Deno6CgsLZcpnYWEhVeEPKcxzviCgO21MTSXO5GRCiIlFYdHE5zY9dSclqU6VxVd3d7c8vYrZa2B8PMnIyPCY2ZxOJ8nIyCBXXHGFbt5CSgq5QiPfK664gjpVLykpkU3pxam/7kK42yQkCAKZM2eOat5z5syhmvv4W28l2YmJsnTZ2dn65k2JuW7MmDGqeY8YMUKetyJfITeX3KTxzG5y35cqEvNskkZ6PVOjZSqk46sZVkSrD//0pz9lKlcgiWRTISGR6ZwRuYJLZTAr6t/fXIdxp+3s7NQVXJ2dneoFc5dBFFIAPMJLfJ+enq6+ZuQeRKXfVb6kglAN0+t6KgJXyM0ls2fPluU7e/Zs/UbrXuPSEjxjxowx1OgXL16s6snpNUipCEy7xrOyu9csaekzY2JU02dmZlqCixFfvQp98jQNA8JBcKl59xpd42JRNHzl8hVcipkLSU0luTablxDIyMggubm58rSKgaz7u981N+NSloEQ8txzz5H4+HhZuvj4ePKTn/yEmjY3N5fY7XavAdir3Ar0Zi2qwkNFcD03YABJSEiQpU1ISCDPPfecdsZ5ea7BPzPT9OBvpOxeMy4RtzNNV1eX7vPq6urSvW8hN5f01HDu6Nmz5+XrnEHkW0ukmB3AfE2v5dhx4MABU/kHk1ALLtZZaiC2L9AImDt8pCEIAprhWhuSUldXh+bmZrlLe1WVfL1GsXZjFkIINm3ahM7OTtnnnZ2d+OSTT9Rt9A4H4HBAEAQcPnxY1bX78OHD1KgdWjZoI7Zpp9OJspMn0dHRIfu8o6MDZWVlqmtI0t/X2rZgt9sNrcsdcDt3KDly5Ih6nfE8wPO65QLgfV3crpCaCqSmgv/oI7RqrJu0trbS3eqjHH9EUlA+f6NrJVaAXnMQne0HLS0thsOfSetYfAYRHTkjbBE9+twOErZRo7Cb55FZWyuL7tG7d2/s3r1bvtDf2ir/rbY2c3krvOuQn48Te/eqfvX06dPUn2tubjb1uZT9+/eb+lyKzWaDlghwOp3azhFVVQAhuHLYMNXoFVdeeSU1b0KIpoAQBEHe4USHFjdcerrub3sNckpvyLw8atkuV6QDIRD8SAriwCtl7ty5KCgoCFiekYzeMTIFBQXaz0rhOeqrohEqIldwiV5qHAcCAFu2IDEhAV1dXbKv1dfXIzExER0dHZceRs+err/ugcwZHw8oZktSnE6n3FtNMUNz7t6Nb7u7VdOeP38eTqcTsbGxrg8UHokkLw9d7e2qabu6unQHUZ7nvWZLIh0dHeB5Xl5uiWdfW3s7emzditiEBHSrlN1TXikSAcAB+CQuDqMSE1EtKX92djY++eQTaoexb92K2NhY1fLHxMTIZ3MKRSNW51lplt3CEKE860w6WxBjHYrvGxoa8MYbb4T9gBoKtLYfBCpcXDgQNaZCnue9hJZIV1eXXLt3m+lESHa27m97CQ9F+pjRo3XT67lo22w29NUYaPv27as960lLg61XL81ZC8/zhvaAaQ3yqp9LTKwEQNaOHTKhBQDV1dXIysqizlqcTqfm8+ru7pab+9wmQk9a3V/WMRXa7YDdDtv27brpL+eYeiSEZ53pBehNTk7WV4b8cEoDK8q6+f/tvXmYFNW5P/6p7tlgmL1BcGRQMToIzjSLLMosxORCEkHNBQXDiKhRMcwCRiEGmdEQxVwVGAwueDUGf141RPOFmyfqjc+wiDLCQI8CA6hRDMg2++Js3XV+f3RXT9Wpc2rpmqVnqM/zzANd1W+fU6eWt97t8/aW5c6zUpnjs1hocnNVIQnNEAW13qZkuwn91+KSuY8EAF7JiuJAYTV99JFin+NTVjWSbL/Og0wvJqKwfGSWovS5MT4eYFg9mq5CA7EYPeVFCNFU9kyFDQA7d4IAqBcEf7oDBYnaR0XbFJAFAOcNN4BwLCdRFDVpv7TtLX9sMTo6Wj12YL3E7GxNeb3xBzIEQQgWxMvjxVJBfE9bPCUlJYprR1Je4W49lJSUoL6+PqhwJWWSmJjYo7Gi7rBScz0eNEyciIqKCjgcDoiiiIkTJyIhIQE7dIrQc3Nz0dDQEJKsFfRfxUW5j6I6OhAB9tt4REQEoqKiujZQD3y9RVBZTJTiw549Or+g+DHFR5/DgWbOV6VEAcX4suJpvfca7pvPzp0YDEAEX+kaSVBo5/w+naTCgtfrBVtl+vcp3KsJCf5/A65dh9OpOodyqJQ1ZT2InHhkcH8vvDGGK0RRxPbt25lJTtu3b0dJSUmPW6SG4y0cIgBTjB+UDP3CZSSm15dxQZ6VCvg9F6pxGUQADYHC/4kB5SURAbgDyWPB802tt5idjYYDB+BpadGX7Wb0X8VFQRAERBPCVFzR0dGaF47eI1rFIEE9NE0pEKsZazKFbcRlxrQ84LdSSU4O151B/KUSyo0yhU00xvd6vWpZ6eEijf/xx5pzV5wvRiIOAi1lWLiQXX1W4XA4MHv2bJw8eVJlcc2ePbvn1tYizZQcppSP7KUmVKtJK0GiRzMiA2tWsmMHs40M3Y5JgcBxOxwOhcKRnnMSm43W+XY4HKiYMAETm5pMy1pF/1VclALoEEXwcgNbWlrQ0dHR9RCn3uB5Fo+E5uZmJCcnd22QlFhgDhEcl5kEIzREoSBab3809Q1Kefh0LEWVtSdbc9PKnkLI1iIYMSy9/XQWKSehxYb/Id/Y2MgtK+nprEJToDgssWOHX/nI6KW4yoeRaVr/1VfYcOoUAPNWU1/yM7IU7vLly9HZ2ank/JRDFqOXlJf8fmUqHspaw44dcACooFzrPa20gP6suLoRsTExmg+z2NhY5QY6KzEmBmht5cp7vV6lq1IGnrssuL+jg6v42HmMsv2dnZpK07TyoBS2KVhR9pSrT5ARIbOgelhQ89ULmffXdPjOzk488sgjOHXqFDo6OrBkyRLccMMNpn5DEAQcPHiQGeM6ePBg9z+Irbj7rCgfxjW1jhAg0GbHrNVEAi195CgqKgoSfncrZGtGANR/+SXzmPPy8nRjzVJixsSmJsUQE2UxLy1IMa1QZK2g//pUAsWkEghtXVDQehjpXVh6+1np5Eb366UAaFktRqwe6se6FAiMKU0FvF7/H/yuRi0wlYdsPm06yoGX5g9or4mR/abn3k+wbds2JCYm4o033sDmzZvxu9/9zvRviKIYtLjcbjd8Ph/cbje7kD/MIAgC1o0ezezxplI+VGYw3G4I48fj2WefVfym1D1AC4QQTJs2DaWlpSgoKIAoikHS2mnTpvVsJiaABKcT7thYxTG73W7tTMwARFFE6t69wbiUdL49Hg9SU1PZ5zuQpSspLZbsxIkTe/Ra6b8WFxX3EMaPB/bu5X5dcQKlwmFpn4a1pJJljA23G9i/X3/ODJhWXLKHv2nLgVJkph/gskzOXrXWKFdfu86LQnt7OwYPHswd23RssJ9g1qxZmDlzZvBzKJmRUosfAKq4hWaLn1BjVAz3k2lZmatQALCOEH2XHc2W4/GguLWV2WZnzpw5PdNUUZp3fb05OdmaEULQMH48PFRrKI/Hg2uuuUZtcVHrLZSVwTlyJHDqFLKysiAIArKysoLnnqn4ArKOHTtQV1cHl8sVJHnYv38/hg8fjrq6OjvGxQR14ZGDBzW/rvXWo8eb0dLSgkQ5ewMVK4rQUVpa7jor7j6281G2n+OelGAlzmRk3oqHP6WwTClsqwktlGvXERUFcMoAgP6b3CG5tJubm1FQUKByXUmoqqpibk9btAiXiCLmb9mCW2+9FePGjQvue/311+FwOPiygXq+bzn79aAn39bWxh37ysB1eryqCoQQrF27VrF/0aJFqq7akox0lXV6vdjm9cLj8SA9PR1bt27F3Llz4fF40NbWhltvvZV7XbS3t+OVV17Bk08+qehNt3DhQvzmN7/hpvLL5x0Krgxkx3aMHYukpCTU1dUF9yUlJSEmJoY7dnC9jx7FBx98gCeeeAIbN27Exo0bAQC/+MUv8MgjjzDlJdkTR44gOzsbW7ZsweLFi7Fy5UqsXbsW1dXV+MlPfoIjR45wLT6t82kImkyGYQIm4aJEFCsR5WqQroImyg2QtUqy3+vIfv/995pjN+jINzQ0cGVrdGRramq4Y5salzF2s458c3MzV75eR7a+vl5z7A4deQVRLiXboiPb0tKiLV0g0wAAIABJREFUHJsiZP4+Ksrc+Q6gP5Dsfvfdd+SWW24hf/nLX5j7jbDDm2pzwelsbRoyOcNM51Y6RDDmvTotTdUJ3e12k9WrV2tOXZobq5s3EwyCcJKQYJ6YOCGB+OLiuN3b09PTDXdQNzR3TneJULpd2yS7ARgpSg2CircYsR60YCVOpV02DQzRKKw2XX+Wk+P/C6AjI0NTnlecDIRwzFRM0oiVy4Np5ozmZoW7kWgcF4B+m5xRXV2Nu+66Cw899BDmzp1rXFDGpjB43z5MS0jok3gNwCf4fe6553RlBUGAp7kZbrdb0dlairvoxXseu+wyVFRUKLZVVFQYchMWFxczkxSKi4t1ZQGgpK3NOLGxdL4aGuBoasLsmhq4qHvd5XIhNzfXkPfAytylNZbDSFzQKgaM4rKiPGJ0ZGNiqG9QSQ6mkyRkMP0QlsF0jEqieQlA/OwzTXktV6HehaO6YRoaFLFF06n8MlhZMwBgM0PK9nO4I8MdL7zwAhobG7Fp0ybk5eUhLy9PM8mlWyDRaUkvRYHPtILjKjyZ0iQ7d6L+5ZeNM51TY5OyMrjvvBMejwfLly8PpoVLyQN6SlcaTw4udZIMoihi27ZtzCSFbdu2qe+j+nr/X+A5QurqUH///czjllhoFKCo1xrPnkU1dc1XV1ejpaWle+fOONfFOTmWFHbI0LTHwgRGXIWm3H2UbJuObFtbm3JsytXYpCPf1NTEHbtOR7auro573FZkCUAadeQbGxu5x23VVWhqbErW6tjtOvK8xqH9wVWoByPdxEPqiCtz9Znq78Rw9xVcfLFqbM1+XJSb0ZDrijHulLg4xbFqNmWV4ciRI2T16tXm3YzS/WRm3oy5r05LI26qo7fb7SZLlizhjy1Ddna2sT6GEmSdxKVjdrvdzM9aa6aFgesqpKwe024zGUxbTJSr0Yrby4qlqPcurfe2bdpFKjtuq1aPFYvLtLVHgejIk37qKuwuCIKA9evXK7bp1iPJLC1efyim9UC9xT82YwbAcHNyC2kZczfUz4siXsaOHTgZHw+g6/xL/548eZI/YG4u0hYtCmbjySFl6bFkkJvbdT/l5kKYMcN4HzLZ3InDgYZbboGnpQWFhYUQRRGFhYXweDxobm7WtXQJIWhvb2cWnLe3t7PlA6UEUgaqZKE5nc6g5aaZgdoN6L+KyypjuHyfBVkAiNXcyyhglsGKwrUyLmAtq9Dqw1+7AAFolZco0DV7BupqtGD1fA84yGImzqYmkJwcLKN6qhlxmQFdisNQLZUcHg/IwYOoq6sLZuVJKC0tDbJ26EFSlIbnLnuO/Od//icAYOPGjXA4HMEMO2m71ph1dXXB70vYuHEj6urqun/eMsUniCIS//Y3FKammmPUl2Hy5MnGtjOY5R/buRP7qazq/fv390z5gAz9V3FRsKIArKaVmy7klcFUUkk3w4risrpmpixNq21oKOvcajLOQAYBsCzAPiF/g5dbUHoIqYsxXRBsBNSDVFK4kqLUnHtiorLDRFISEl94AW5qDm63G4mJiezmpIGxY/fvB7ZuNT5vqiM3KSvDsvHjjc2bgZJLL8W60aNVjPpLly5Vf5mhfMpffZX5u+Xl5bqHUvz115hEtXWaNGlSj8e4+m8dFwUjb9G83lNWrQerrqtQYdVyMJ2UIoPVNTOVWELV7EXodHdWvaRQrl4rxz0gIStK/f7775H405+iUMZ9Jykh5gOcAZ71wFReMgoiAf5+bpmM5qRHjx41xHCTGBHBZEpXzZ0qaidNTagXRWYfsuzsbE2uQgFAUmQkCi6+GKWyjuAFBQVISkoyNu/ERBQUFCjmTQhhrzmjaJsewUxWH/e4DDDLb5MxZ8jJegF/tqLNDq+DvqTxsWI1WZm3VWvNSHZdfMDvT8Oq1WLquCmmE6KjkMP1RaO/gNcTy4zSYvWHArQtLwIgMzZW8fAH/Mps4cKFuiwQ2LEDJYE56M59+nT/v1KW7fTpIF98AZw+zTwmPRSPGoXCL79UyTEtD4pjEbm5wDffADfdpDsOD3QLEa63hME2MrmwEOWUmxPguxAlOByOYO82OcuK1LttQMe4Ojs78dBDD+H222/H3Llz8eGHH4b0O1bcXlaVnpUECysPUauWgxWryerDv7+6SG1oQxCE4Bu4oVoqeXJGdjbKqdiahEotYmVZerhhSOPKPn+alsb86qc6jWYJgGkHD2Ljd98pat82btxoqPaNEIJ6rxelpaWKhJbS0lJ2Qgt1DLm5uZgwYULwuhVFERMmTMAdd9zBH1RW28g7Pu5xyxJxxo8fz0zsGD9+fI8mOfW54uoOYlDAmvKxonj6Elbqx4C+VfZW5K0et21xaaOkpARFRUWK7LqioiJDnXwJIUElZaqWSkf5GI2PmZ57IP4pCAKio6PhcrkUu10uF7ufn0zhtkyaBFx9NX9+NKhMSmHnTqz7979RUFCgSGiRuw6ZSEyEGB+Po0ePorKyMqi8JkyYgMrKSnz99dfqe5jKaBRmzEB0VZXx45b9hvylRA75S0uPQTNZvhfQ3NwcrHOqra0lP/zhD1XfMVLHZYoGiJK1SvlUqyNfW1vLlbVSi2Wa+oga+5yO/Llz57jypmrXGGNbqeOyer5MU10FMGDruGR1QSIQcj2TBFM1SRJktUGZmZkK2czMTHLo0CHNeROYrMWiZH1ZWcFaKFM1SaHQZNFzIP7at/z8fIV8fn4+u/ZNtmZifLxqXOlv4cKF6vEpuilfXBwZLgjM4x4+fLgm9RPrPBs931bvpT6PcYVKDJoe+FfS6URnnKqqqiBjOC2rh6NHjyrcbrS8ngVw+PBhpKSkMGWNzFtO8CuX16vjOnjwoKIBJj22Xpzq888/x4gRI5jyepmUlZWVmmPrrf2xY8eC1wYtq2cpHjlyRMEOH8rYgwYNUm23TAzaTzAlPh7lTU0KwlgAmDJliiF5U00VZfEeAmB5WhoqA72lJFRWVuKpp57Ca6+9pvsWb3julHXn+OwzzOnsBGQ1SYABVvwAeOnfjz32mK6lSgjBP/7xD5VrbuPGjZg8eTKKi4uVxy09DxoaIABY/6c/AZGRKJXFlQsKCnDffffpdrYQxo9H2oEDONPcrDhuAEhLS9Ps5SXMmIGEr79GZmamwpWbmZmJhISEgW1xERIiMaiVt2hK1jRzBiV/Vkf+7NmzXFkrRLmmmS+osc/oyJ85c4Yrb5Xg14rFZfW4W3XkW1tb1dcbuTAsLoIAe0WArFb6M2M9mLK4KAtgdVQUcTscClm3200eeOAB/tjSuQ2MbcjyYRDdFkdHm7d6cnJI06RJXKtHc90YDBT0H9Pao+b+aGQkSaHkUlJSyP3338+cL4sol3XcPc02ooWwZ84ImRiUQl/GuKzUkPVqI0kKfZmkYGXuVmNUdoyLAh1f8niAc+dC+g1CZRXq1iTJ4lMEQMPQofAEZOQsEEyuQilew/tsAoQQbG5vZxYRb968mT+2VMf1178aH0xW8I2GBjh++EPE/+tfzDgTMztPtmY+AE97vaihhqipqcGrr76q+wyQQD8fNRk7JEIAGdsIDU22kW5An7sK5cSgmzZtAgBs3rzZdC2NFeVhpIBYS55dHSbbz6kfA6yllaudWdR+hrtLDitFxFYf/n3J2mHkfOsVUA9UEABFra0KtxOAoNtNRf1EpXYLM2Yg8ZtvjNVSAV2ZfREREAB4rrgCmS6XIiNxx44dqKqqUst+9JFy7rt3o0gUUbprl/7cKZeZmJGBuj17/LYEhbq6OoiiyH2JFAAkRUSgIDUVpTI3J7eOi9FL0N3ail2NjYrtUhdqolND5hAE5rwdDoehsac1N6OcsWbl5eX45JNPun6Dkcb/n4SgNPB9uWtWj23EKvr81XLVqlXYs2cPtmzZEvwLpQDUysPMitKzCiuWohXGDsC6ArACK0rX6rz7suYvLEG1fSkPKC15ajfAYVJgWGslsuJloCvmpRnrGTIEJDYWX375JSorK4OZgUVFRaisrMS3335r6HqUZqg7d2rejs8+wzWc837NNdfovogVjxqlmh8hnDoumiXE7QYZOpT5u3rH7ADw4CWXIIO6XzIyMrB48WJ93k5CcCjwkpiZmQmfz4fMADPNoUOHNMcXBAHrr7giuMYSCgoK9LktLaLPLa7uQl+mV1t5kPZlKn5fUh9ZOV9WCHoB21WoBQHALEHAFEKCDx+JcJdpPVCWi/TZkOsJUCVnDPX5cApQdOMFgOTkZLXlQRURC1lZmPXNN5g8Z45i7oQQJCcna15TgiBgptOJr5KSUFPT5XhLSUnBzJkzNWUJ/HVc5U1NwYd2UVERSktL8emnnyqtFkCd9u/x4J2mJuZvv/POO4okF6b8mTMQqHuVO9/6ev+/gQQPoa4OY6dOxaefforKykrF82bs2LHsgm9Z8TIIATgJdT2JC/cOlcHqG7gVy8eKu84qX6AVd51VhWuFrspqbM5WXBSmT+9SAgBKsrOxPjtbYTGtX7+ebTExWNYVhb0m4AAwOznZUlNEab5anwGorB4xIwPboqIUSgvwx4qYPbUohnZTdVyUhYvmZqRyXm5TU1M15y4CeJEQVFLylZWVeOutt/j3QiC+JggC9u7di/z8fMXu/Px87N27V1thByxiFimyvJauJzBg7tC+tLisKBArSs/qvK24SK3yJJpSPhRJrlXFZXXdBhxkyoYEPguyhqOAAfcp1a0hFBAADT4fsymiZosOSZ6EyEARwBkOW8uZM2fUG2WFvA5RxCfR0X6uwtJSOByOYAdplbUFqF4UMH06prIUFICpU6eqN8pfFgQBTZwYulY7JbrjgiFlTyWVyAl6DbmVuxEDxlVo5WFmlTqpr1xuVi0HK3Emq3FBK+4+qxZyX8Y0wxKyTDwh8JkQolBe3AQBOotPxhtoCFSCxVaKp1DC+++/r95Iua6EnTuxjhAgkNUoudiYLVUonkOhrAwRI0cCVA0Z4L8e9JIchMpKrBNFyG0Po/yOgiAgKSIC+fn5Cvdofn4+2z0rrbHPBwHAOIcDLHKmH/zgB2pZWQ0YAJCEBBS1taGUekHmJuNQ856VnIwpixcbcyt3IwaMxdVXmX2AtYewlXlbaWAJWDtuq8rDVFyQepvvSxfphYCSb77Bsq++UtAmLVu2jO0q/OgjpfKhP+thyBD/H/yWMC8Jv7a2lm8Jy1xvpluqBOYrCAKcTqfqJTUmJgbOACWUApSrsTghAROp68ZMC3tCCHZRmX27du0ylGj0yfjxyKR672VmZuJ//ud/DCmP8oCFq2s1MdLhS775RqHcNN3K3QnNKq8wgZEC5E6ACJyCUkEQSGdnJ1fWdEEqJV+jI19TU9MjY5tuQU+NfVJH/uTJkz02dqeOvNb5Mk3ZRMmbnnsAA7YAWVbQKgKkMCqKAF1Fw1IxMbOImFHISxISzE8sIJ+VlUWcTqfifDidTjJx4kS1DKeY1hTdVEDW6/UG29e7XC7mZyYSEkhHbKy5FvYs2iWAKc+lXQrMXQTMUT7RcyB+uil5obRUSKwqvGast1R0bhb9nvIpZEgxj8BbmHS2WCCEaL659GUbev6sAvs15m2EYV3L+rCSYGHEWtMa28ia81x2dnJFN0OWKCAASOjogBtQuNvcbjebxkfKUpO2S5+NQkZnRgBMKC/Hbsqy8vl8uPrqq9XuSkZN0rK2NmzYtUu/pYpsXABwpqRgTGMj9sXEoLq6OnjtxcTEYMyYMdodHgIt7P1TMk8XJQgC0hwOnAn0A9OkXfILKD5upZIjJHzwwQf8QWWtglhtbMyks9PnRXWeegD2HQ5gsN7+wdrfMNWGnoIVV2FftjWxMm/AZJyJSs6w6iq0QUHmriMAGqKiQPO0ezweNDQ0GGavoL/HvZakQD/8SjO+owMu6isulwuxsbG67jph/Hh4Bg1CZmamooA5MzNT3VJFNi4AkIYGTCAEbW1KBtC2tjZMmDBB32UnCMjKylJsy8rKMpTRKIwfj73Tp4eW2Ycu17fb7YbP5wuytYuiaDizL1TmjJLc3JA7CVhCSHZeL8OIq1CEtqtQYTJTsl4d15HKTUDJW+Hds+Jyq9eRra+v15y3FXZ4q+5VK65Cq+zwps93AAPWVchgSs8MMKVLf5mZmZpM4cH1zckhxaNGKdxzkruRyfnndPr/AOIDiBtsl1l6erp6fAZ3XmrAzSlx7UkcfKmpqZrPAOm6kNyD0h/XTUgx6hdcfDHzWjLKk7g6KkrFV+h2u8nq1avVY1PyaQCJocaNiYkhw4cP15WVPtNzZLoYw4ircMAoLi9AIjgPooiICOXFR8lajdeYepBSslbiNVbbmlhRfFaVvSliYyuyDPkOHfmOjg719UYuDMUlAsGHP/2nevgzfoMVE9OMkVEPw5yEBOKOjQ0qKUl5TZo0ST0mdV7lio/+U8WaGLLDObLMOBMVF0zlyDLXjBHjoomFufOmjt8XmB9L1uVyqWWp4yYAKQZCinGxyJg1FbYMF26MS4ZclAGg63PeBvA8gEHw+f6BG26Qe0XLcCf+hDvxGqqRgp9jK+NXnw/8xiX40Y+cULqpy/AgnsFs/C+O4UrcixcZ8msAfAggE7NmRcvc0v65PoFHcB0+QQWmAXiCIV8EoBK7d0dj7Vrl2ADwIu7DpTgO4EYADzLk8+B0OvHWW8DzzytlAWAr5kJADYBFAO5kyP8UXq8XmzYBb7+tln8fMwL/ezAwBzla4fV64XQ68bvfAf6m1jsC+whSUIOtjlsBUQwc+zRK/mTQ3VdUBHhk4wLAaBwHcF/g04sArlTs//WvnZCyihcuBE5S8pPxCYBHgisBpCj2//73Dkiejp/8BJA8vV3rOHBBAHg5WZVer5cfvwjEmwRBQEJCAtxut7EYmZSWHti+o74eXq83GBdyOBzYt28fvvjiC925OwDUwB+blWcgOp1O1NTUKGNNUg2T5KaMj0czxRUoobm5Wdddx4vZMteM0Vok6uBBgMGeERUVxU9pRyC7l1VnBhiKNREA7wEol6W/S0XFU6ZMUbZUYZQArCcEKChQFCHblE8mEQF20kBEhHa8pedTu1XxVMPoSVZ7vX5etL/f2tjKVerUSTnXSu7o+eQMvTM6cCEAuG3YMJQy6qluu+02/vUoMbwTgoaGBnioh5zH40FOTo76YUrVgeUmJqK+sxMHmprgcDggiiImTZqEyMhI7Nu3T3PuPvjjzXTavM/nQ2trK3w+H/eeIITgCocDHsZ1ecUVV2gqAQeAU0OG4KLmZgVLe0pKCk6dOqW+Hhn1Zz8pKcHkmho899xzwa8tXboUKSkp6nFlyTQigGrmrID6+npNcmAJ0tVOE+USQt0HcrYP6TP9nd6Cpj0WJjDiKiQAieeY6/Hx8ZqyXoA4ObJOp1PX7WUq3kPJWnF7WXVxntaRP336dI+NbSXGZTW+RnTkeRiwrkLKdVUcHU0KIiMVa8J0HRHCjHH5srKYXYz1YmQ+gFwUGFf6vvQ7KSkpum4vESC/4pzTX/3qV7oxrmywY1zZ2dmaayYvIaD/NFPxZSnpq1evZq4ZM8ZFxQWHcY45OTlZNxVfjI8nBZy5q9x9snEJQESHgxRwxu5pV2H/VVzUCeiMjdVUPloPQlMPUYa8qXgPJWtFAViN9VhJcrA6til56oYxHaOixrZjXBQYMZfMQDt3M4qHQBmwp/+YAXtKAdAKU/pj1iRR14VPEEgs55zGyuJmLFnR4SCTObKTJ0/WjVOlQf3y63Q6SVpaGv9kSPP2+YJxKkN1XLJrWSuuZyShRXrRYCWGqGQZazYFSkV1wTSSDBkUUaXj+++pSEUXUlJSNN1HVpkUrNSBWXH3WXVxWnUV8n5dYiHQgqmUdoo5wwm+jzsiIsKwa4S7v6/cH30FirR1oiiikhBFenVlZSUmTpyoSTYbJNk1QzgrgwAgURCQSV23mZmZiIuL072eBUHgcn92dHSomeVlfIHk+utxglM3eOLECfU1IVszL4Dv4HdVxsTEoLOzEzExMfD5fPjuu+/U9z9VQiDMmAFHgNxXquOSXK3MnloynkEBwFecY/7222/ZPcxkzCZk924s272b6dpVNf6k1kzIykJMIJ4p773mdrsRExNjx7iYoPzYAiG4FGzKmEsvvVRzEfuyo64RwleuX15HVu8BbIXyyaExPiFEd82MFE/z6tAI9IPhWrD7cVGQJUgIABImTYK7oQEVFRVwOByoqKjAxIkTkZCQoD6vMt486fOUf/8bLIrVKVOmqDfKa6kAvNfRgUrqK5WVlcGaJK22Jt6pU9H58cfMQ+zs7ITX6+2qL6RIhLF7N5rVYgD8yRkqyB7+TnTdx21tbYoaRub9S4+9axfYaSFAIythRFb0LQLwDhrUlUEkgyiKhmJce3nb91J7qEQaUlYGd6DAe/ny5Vi3bh2WL18Oj8eDwsJCQ8khoaL/WlwUBPjf4ukby+FwsDNzZLCqAKwyb4QKq8kZVshmCfgF0DExMbprZmXuVi3kvuyB1huorKxEXl6ecQGZBSDAn/9ZERenyOyrqKjADhZxLtWQUaisRFJNDbO5oB7xqgjgM86+Y8eOqc8roxmkFrRepgQACZx9zGxImRdAhPa9oHc9iuCz70RHR6vlExO7EjsACBoEB6p5y4rNpc+nAt+huQpPMQiH6d+WZ5A6HA5s2LCBn0Hajei/FhcFL4B9UD+0RFHEvn37NCmEIgDEgk1aGxsbq8sWboXJoS9dhVYtRZ4jsa2tTdNSlMaWv6nK4XQ6dR8yWtA7biNWbn+ljdq8eTO2bdumyeyvAqOxIX303PWQWUzS52L4izloMAlnc3L8/+7cCQH8c+Pz+TSz6wDA2dqKTgAxjHT4trY25fVIU8YJAlyEgMVN73K51NaDTN4B4B4AGxmy99xzj3rtqFR8R3w8Rjc1oYbxsjd69GjNa9EBYKggoIUQRXahy+VCZGSkWpbR+POSgwdxipGKf8kllyg3UDRZJCEBDW1t8FDM8twM0m7EgFFcTmi7rvTe4LVk9cztvnL3WbUUrchbpXxyABgKgFWBkpiY2KOKYyDHuNLS0rBx40Y8/PDD3O9UVVUpPqdLzOqBzyTw+Sj1PRbSA9e1IFkfDgeWEYLS0lLk5eVh5cqVWLt2LUpLS1FXV4eVK1cqHmZp338PwP/iKADIGDwYFYFtinHS03H06FGFbHpgTGmL6PMhCex0+ISEBOzbty8oT8uCevDLcfr0ac2xCfwVnyy88cYbuO+++xTX85WBl2vpieD1+XCYc819/vnnOHLkCFeeAJgTEYGNlFu/uroaCxYswJEjR5TKI1CMmB6IQx594QW8QgiefPJJRTr8woUL8Zvf/AZHjx7tOmbqOkFLC54RRbwfODfB76Wn45e//KViG422tjbVdWgKmqkbYQKjlE9RnOyaqKgo3VTYkYEsILmc0+kkI0eO1B3bSlZhX6bDn9eRP3/+PFfeC22KLb0Sgg4deUVmH+NcX8yRvfjii9XZTJS86SzSAPpLVuG///1vMm/ePOY+I1mFITG8S7KEkJycHEVWmpQll8NiEqfO67WcczJu3Djd82oqW5SS9QEkkiMXGRmpm9mnxdpjhB3elDy1ZrxsyGuuuUaXqUTOeCKXZWYEMlLpQyoDIBdyViEFEUA8Z198fLymn1kEUAsEM6jk/9bW1vZoR10rcSarsRorrPZa8npygLXkDimLiwVmFpcNbdTXBwP+hPpsGIGsQhLIRvR4PFi+fDkIIcGAvdvt1vUg/Juz78yZM92bdJOT0+WmBECmT+fGTvXIao14bLTg8/k0XaSq5wdFOq2V3avCzp3K5JCdO0F27sSyZcsUX1NlFAKqTG6hpQUHOzrgcilpkV0uFw4ePGg3kjQCB4BLOfsuvfRSw64n6SIz00zQalaiqQtPhr5sYe8E8DCAZGp7cnIyHn74YUMJDoaPm7pRnTrrYqfDmwSVnIHcXBDZQx0wsCaBVGkpJbqgoEARsC8oKGA3dKRSu9M498qIESO6N2bLSCrRSrDQY7Dh9Y8YPHiw+nqkFEBkezuGgI0hQ4ao3e5er/8P/jX7j0cfZSqP66+/3hDrzzL4W9gUFhZCFEUUFhZiw4YNbOUlgyiKaITfLSl/4a+urkZjY2OPNmQdMDEugG+B6D3IHPAz7j2XnIzPPvss+P3k5GQsXbrUUDp8BNjZhXqJBpJtzdxHtPuIRQK4DgArAfi6667TjTNZTXIoBvD/4LdWJYwcOdJQ11cngCwA5dHRaG/vSo6Pjo7GuHHjlOeMSnv2TZsGcNKeAf3kCrufFwUqOaNk717UE4J1geA6If4OyImJiYbaVTz22GPc7Xryob7ESUgBFLRLwe0pVJUnlVQiMJITNMenkkrGx8fjcEsLamQveykpKRg7diw7s082B3HwYLRyxm9tbYUoisprUlYDJgLY/uyzqKa6nVdXV6OsrEwtSx8XgEQg2L9M3kE6MTFROXdq3o74eMQ3NSE5MVHRRyw5ORnx8fE9ex9pOhLDBEZjXBM5vt6JEydqxrhMszsz5Hkxl2HDhmmObYUFwguQQRy5QYMG6caZGnTGbmho4MqbZtNmrFkhRz4vL0/3fBluYcOQH+gxLi3o3Uvy8xIKu7uYnW2uxQfFxpANEBcl53K5DLHDd0Kbuk2LPccLkMEc2cGDB6vvJYp2KZMjy2QcYbD+ODjyDodDl7lnNWfNlixZortmwfMWYluTOA6rfVxc3IUR4zJde0LBB6gKF+W/reX2IrBW0ErAL+bt7OzUlI8AP0MvMjJSNxVfiylAD1Y7N0tZWMnJyfB6vUhO9jsOq6urDcUjDgJMF4cqE4qCCICX7D1o0CBDLopQrfMBCZkrVgCwzuFAAWDM1cdAOcd6KC9nlSV3QQT8ridA5Xpqbm5Wn1fKhewQBM1YkZ4FoBXjYvxgMJVegHYdl66L0+HAUM6+oUOH6npsGqAm2pXWTO8+lGCokSTlXhUPHIAmlGCHAAAgAElEQVSPc6/5fL4edRWGheLavHkzVq1apXAZmUUEgIuhfvA4nU5cfPHFusW0WpXrRhRXLWefnjwBkMTZl5SUpCurdaMavWhDgRN+N2USgNraWkRERKC2thZJSUm47rrrdBVA8CHF8I+3tLQoL3o6GB1geGBBEARDsRCtxJILzlVI4THOdcNzAXYXHPAnWLmgpD5yuVwYMmSI+rxQVGCizvWudU05HA5VvFZCcnKyrrstBoCb2s6lPqI6IDsmTEB0VJSKKSYmJoZ9PVIvGgcBZtfoqqoqzZgi87MJCIKAKzj7rrjiioGfnCHVnpgCnRWUnY0RcXHMGo4RI0ZoPsQd8LdDYKG1tdVQcgXvQRgZGal5AgUAvPyt+vr6HqWqsprcIQBIo7alpaUZumAdAOYAwQw06SHldrvxwx/+UPtBIQjgseFdffXV+px28Pv1WVD59S8wEAB1hKCU2i7VYanuI4r7TtizBzObmlQxpZSUFMycOVOTyYEAGB8VxbQexowZY8iK17KkFWNTSg9mrQPZA58AyIyKAlXGDY/Hg8zMTN01E3ftQlJHh4obtK2tDUlJSboZ0aasVEppqj5roaFBERt0NDUhEWqChaioqB6vxQyL5IyZM2fi5MmTmt+hi9XkhYsA0PL99/g3hxT266+/Vrx9pAe2S5exF/43JpZ0VFQUDh06pLDYaHnAb3moyyaBuLg4RfEiLetDlwJJTEzE7t27kZWVhfr6evh8Phw+fFhhvdDyGWDT5Fx55ZWoqqpSXDy0rN5ldfz4ccVFKZcX4Ve4LF659PR0HD58WHNsAqAEwK2vv45x48YFv/f666+jo6NDcb7TJkwAAMQG+jG1TJiAqooKv6edwuHDh1Vvmqw1r+Mcc01NjWrNJVgumgxXSOUDguBPFCooAEpp1cUBI9FgW0sLamqUKRI1NTXYtm0biouLlQ80GZODACD+oovgOn0a1bKSBpfLhdjYWEOWtANsL4TD4VCOSzFniGDznALAuXPn1CQEFGvHXo5rXsX3JxtTAiEEhzljHz58WK34ZPIsKxXoWjOV8mCwpIQK6RlAhyU6OjqCvcB6THlpRsB6EVaLJn1xcdy+NMOGDdNt221YliPPa6cwaNAgzbG1CgiZ7RS6SVYKSLNkpT+t5A4RIPkcufz8fN2xpd+Q2iBIfwUFBeTw4cNKWSqAbzqQzjhfUjA8IyOD+Hw+kpGRQQB/MJzXKn3AJmdQQffiUaNULdnN9OPKjo8nMTExCvmYmBh2XysqycFUiw7G9Ww4WYmaty8ri8RyEg1ULVGosbsjuSuOI89McqBkDSc5UetNgK7PRmD1PpRhwCRnmAajGO4yzlcvu+wyXXedlqtQ108cF+d/U2WAGHBv7IW/26kcS5cuxd69e3XnPRNs/zbTLcOQ16o/0aqnEgAkCwKWUnJLly5FcnKyofqRafC7oOTknqWlpViwYIFy3Si3jgPahKhGXLsjAv/Pzc2FIAjIDaQYG6kXGnCggu7FdXUQqXbwoiiyyxwYAfvG5mam24tZ2yNzFTrgT2WnbV2n04n6+npD51WLhEBxXil3nePjj5EiiqqEpOjoaHZbJCpGlsoZNzWVsYeO2TqdWAZ2otKyZct0nwHPgh1fW7FihSGSXcVnLVBtTRzZ2XByzoleGZBV9F/FRUEQBHwsCHBT/lq3242PP/5Ylx1e6wGuUj6U0kRzM+I48rGxsZw9yvE/kt1ECHzWU3oigO1gZxRt375dN0YlvR4x9xFGDRmlQAgheJOSe/PNN7s/KYS60eFwcC9cIzeLAOBu+Ps8lZaWwuFwoLS0FJmZmbj77rsvPMUli10QACMbG/Gcz4f8/HyIooj8/Hw899xzGDlypPrcMhINZqemMh/Cs2fPVp8fGUuHD0Cry6Vy9fl8PrS3t2tmBgP+86rVp00zU1UU0Q6oEsTa29vR3t6uqXAJgHbOb7e3t+veR6LPh23w37dyVFdXY9u2bZr3MQGwHGDG15566qnuvRcZRdvjgWA2sYTk5GSMHz9+4CdnAH4m4rff5lFVMkC9KZDYWCyLjDTWEI0CgcmsQtoCIAT3gv3GNG/ePN0mlhMD86TnzWzcJ4MA4Hzg//KHDACcP3/ekMWllcavJe8D8DT8StPlcsHr9cLlcqG6uhpPP/20oYfMTLCVh5GK/1t422/h7ekCgT+FuLJSGaGrrKxEQ0PDhcecIQNBV4xo9+7dIIRg9+7dADiZqlQjSVJWhsaf/5z5ENbLsHUCWLJkCTO7bv78+bqZqgKAy8FOFrj88su1kzMIMec1kb28OgDEE6JK0IqOjmYX4tIZjU6n39JkZETX1NQYasfCsriMNN+UQB8j85iplxQxIwONgwahtrZWkRhSW1vb48wZYaO4TIORHbOX0z6EGSCVwQlgGNTV9SkpKRg2bJghCqEGsN+Y9GoprCgfAf52Cvnw19wIgoANGzYgPz8f99xzjyF3ndYbqta8HejKzKuurkZERETw+I1kFGkpD9WaMV5SPuL8vhFLVbrZWS8aPd1HKNzhAHAK/gchne156tQpdiPJ3NygIhBmzEDCu+8iMzNT8bXMzEz22sropgiAD9avZ7oZ9+zZo3tefQD2w58ckJKSAq/Xi5SUFHR0dGD//v2GXqaY21nXA9WP6zjUzVHb29tx/PhxQ1yFrWCz2re2turWoDbAb3HJKZs8Hg+ampoMvYSVtLUpXu4J8TOlqFhOGP3P5nR2MjOD58yZYzNnMAN1VKBQdDiC7PBLly4loiiSpUuXEsAYO7wIf1IBZAFGZpIBIUyW5CEaQUq9sYupsURRJPn5+exgOGfucnAr1ik5n05wVa/iX4yPJ0sjIhRy0tobmbcPIG63WyHvdrvJoUOH2POXBZKzs7OJy+VSyLpcLnYCAGNcU6wfAQzY5AzW9UitC/d6ZLApTImLI0AXU4aUgDNlyhT1tSG7pkSAXCsIzPNihB3eaoeIVQCJjo5WyEVHR5NVq1ZprpkPIBdxxr3ooot0k0qksVkJLXpjE4CMgp85Q0qG8Hq9xOVykREjRqhlGc/NwsCa6zKlMBI7igWB+dxkXisyWL2X+q/iok6ePMPF5XIFT570ENZqLSICZAq6sqfM3GwEIN4hQ4IURCkpKcTr9ZKUlBQCMFp8MC5aAoOUKxryhsA47lCzmST5AkqOSevDkTWVDUVI8MaRnx/6jzk+Y2xe+4xrr72Wu/YDVnHJHkims0UlyK7jKVOmGL+XZIrPBxB3bCxzbGZWISOz2LACoa4J08qHWjNT2b0M5TGFI6/3/PEBxB3IhpRayUgvg0YyMYP3YkBZSX+G6b2o7FPd50AAF25WIc0Y7nSiEQjGWSTXlcvlQmNjY/dS+VCFeEJzM4YF/l9TU4OIiIhgHYuRDDvAIOUKwKx8J/HKXCoSMPlVoNaMCIK5pBT5GPB3uWUVqhYVFRl217GSabi+eVlWk0QfRLcb16MVkjCNt30ab88ABpVosJvj4pFiXgrIXH0AIMyYgU+io4MZolLssqCgAJ988oluQX1CRATcVEKT2+1mM2fQ92FTE0ZxfnvUqFG692Ezb3szY49szQQAMQzmjeTkZDZzBiOzj3DWnHkfUfG1CkKYrt2tW7caTlaSiHUlMOm9KFchPB7gHK/6rWfRfxUX4+Q7ExJwhkrjPXPmjKGg7idASDcb4F/E7wCmX3/nzp3d6+uleiWVFBWh6M47Ff7poqIiNgu3rB0Cgb9r7ajJk5nDMG90SmmWB44rFOVBEPDNezz6vnnp4RjoJSTMmIFZZ86goKAA69evhyAIWL9+PQoKCjBr1izDzBkspXmhM2c4ACQIAtzUNet2u9mlBlRaOT76CMKePVi/fr3ia9J5UoGSL2tqwnSK6Xz69Ol47bXXdOdOwCYRAPxxMsU1RaekOxwYy5FlMrzLlIcI4JgoqmjfamtrcezYMV32CpKZiROcZ8SJEyfUyot69jni41ERp8xrrqioMPzcIQCKiooU2wy9fAoCkhyOYExeQn5+PpKSknr2PtK0x8IERru2eocMYcY99ApS5W4OuawZd50PfiZouXxmZqY6XsMZ2zBk5roImIsnyOYgxcVycnKCxbfSX0ZGBrtbLbXmxdHRpCAyUhGb4xaqMo45R+beIKSrU66KCVyja6scRs8Xy8Up/Wm5OAasq5CxPkupdTETu+QVluvJa7nsMzIyLHVpUHXGZsy7GOw4N/N6ptx1ptyMjFhxKie2xyxgplyNPkFQFW5zY8WMNTMVJpH/BiFk9erVzDj16tWr1XIyXLgxLkYVt0TtT8e4VMqLc7MZ8vMSwvStZ3IuvKuuukrTt25acVHB7Mmcan+mb11CTg5pDsRyTMWKGMFZ0eFQfMWM8uC1z+DGuCQFxhnLzNimb1ZyYSiuHJSROJQRoIykph4nOTkiyc19mwAgkyZlk5wcMXgacnIIyUEZeRWLCAHIeaSQHJSRKEp+1qxXAw/xSUz5bbiREIBU4UrV2KmpxwlwA8nIyCAHDqhlc1BG9mAaIQDZhWkEAXnlXyaJiIgg773nVcnmoIwcxZWEAOT/4cbAeF2yqanHyYkT/uvhzTfVY+egjJxHCkkDSAQWKWQFYSeJjv6YtLT4l/mPf1TL5qAsqPxGjHhGIR8bu4/MnNl1LT7+uFr+FmwNKq1hw/6b5OSIJDZ2HwHKyKBB5eT227vkCwsJycGOwJ9f/h68FLwXxo7dI1vzMhIXV0EKCrrkf4EtqrkPwxNBZXXLLV1jx8buIzk5Inn88a7LbNasrvWzei+FBVdhSKA40pwJCYhtbARSUoLuwTNnzmD48OGIjY3VdBcSKLuArlu3DsuWLcOGDRsAcPy9FL4MmNWZmZk4cOAAJkyYgMrKSnz77beWD1UB2t9uplZCikXs3IlYACQ3F/jqK+PydFquz6dKITbqHhAArAOAQLdVaa0LCwtx77336v5OSUkJ6uvrg+eGEOPNDgUAswBMoVyNAHrexdEPQKd1k8C1ferUKVx5JbXTGQFEDe6innE6QQLXSX29/948cuQIAF5HbQFwOAHRf14mAPgyMNapU6cAADfddDOeeOKH6OjQdwFr7tchIXg5MG5q6iW44orR+PLLr3Dq1Ek89tj/h5dfLuaOIMLPVUrf6YSIiIiIDLgKtVuTLAdw+vR38K+AHy0tzTh69CgISefOXYCfd9MF4M4770R5uYCJEyfi448/hs/nZbv7AaDBH24QRgzHJz/IRpHbjdLSQ5AaJaWmphpieI+GP6/A4/HA43kHQAoiIiIREeGE/hmxAE21FibQ1L5QpoPTbkEmXxbHTSC3sCQLwCg/W05CgqJpnM/nI5mZmWq3lxWuMEredAYYY97Fo0Yx3TpG3X2GrUWOLMs9y30bk7kI6ZRdzWaHGmPLoZUFRciFYXGZdqMy1pXnNmeWGTDuQ1am6gMPPKAra8plx5h3Avwp6FLjxs7OThITE0MSEhJ018zqfbg6Lc24y40a25TnQsPtTt+HKjCyCgs5WYVcb1UAF25WoSybSZB9ZlWfG0EJlJaV1MKa+fZOsQVgxw7kFhUhOztbIZ+dnY3JdPIDxfel+mwSpoompXnn5KDl2muBHTtQ/PXXTHkmLx3VSkb12SQIgGXLlim26bGcAP5jS0hIgNvtVjQ7lBIIDFt9RjM5BzooDsr1gUaScsgTYXiy0meH04mKigrF14wkCxD4rQdWpqqRvngOgMtgc++992qO74O/oWtbWxtGjBgRbIfU1taGyMhI3eLlT3nbP+Xt6QIhBA0+HzNRSY/JRfJcsBp/rly50tA1TQLeCjmY96H0/JDG3rkT8XfdxVxvFTdkd0NTrYUJjDBa0/EPTVixHBhjy988dN96rMybkG6PcZmyXBgJMYT1NsoCbe2YeVO0+qZn9XwHMGAtLkayQEFkpL61JYdsTVevXs1MkjJiPfCsvYULFxpKzjDM0s64HryBedLzNuK1WY2uGLuZY5b+iqk11kx0orw2xYLALBg3YqVq3Yd6BcispBDpT550xcKFG+MKIwiCgHWjRwNz5+rHa6z2w5H1LwIADB6sjnvpYccOfFtVhTGCgMTExGBcT7IyAU5DRXpsow3oGJBS0llj6/EkCoKAZy+/HDuHDlVwPLrdbjz77LMXruUUKmS1UARAUWMj0+oBGGnt1Fr7BAGb0MVhKcWZq6ursWnTJqxevZrrBRHgjxUVQGl1FRQUwOtlxGsoiOB3Iq+trVX31KLghL98Rt57z0g5jQj4SXLhvwYrKiowceJEeDwedg8yq5ATXQP4ByEqi2/jxo245pprQAjRpY3j3YeqZwBldToIwWwAJwO1sxK4hMrdCU21FiYIO4uLE6cyFK+xOjZlcU1xOhVvxIbS4YnyjcdwrKcbLa7gGx9jbNWaMSyCwqgo2+IKAUZiXKYyLhnrms2xXIzQcfHYWFQ92jiyhhlRGPO2YnGNApt2adSoUYbmHeqaa8X1UlJSDDFn8O5DM/Om/8w8f1gYuDGuMAMhBv3EViErfBQAzBJFFAAhFeJKMBzrYbRzMW3thTI2zZDQ2IiEjg5mAbGZGBd9brr9XPVDSBmXVq6pnQCTCGCnZKlzQODP7i2FkjC2tLQUa9eu1Y99wt+eiEUEwOwvJ4MPwHAgyLYj73YwfPhwXaLbm+G3uB588EEQQvDggw+iuroaN998s6HrSvqGnAAB0L8mBQCXcvalpqb2eLyXN7sev5c01VqYIOwsLup3tGJF3R7jslJLJYPeGw8TvbTmZiyuULMKi2Eii5Q3LwoDweKSv03LYaYY31RNJOO85MssAIlw2mi8xnBGJOM+GkVZWGatJpp70+gxW83kfBQgKQxra8mSJYbOl+FnH4NjMZRCfkIu5AJkCU4nEc2kkxNiXXFx0spZD0LVDWfF3cYZ25TiC6DXFZeJMgDV3Eyst5E0flMBaa15Uei3iquXkm6MvFTkACQT0C8rYcj6ArKsB6kqHZ9zPUqp8BLoz7yxpWOXj2mWeYeVDq9XQuADSCznmAcNGmTYVWgIDLkcsEsfmMw7Mly4iqsH4i2GYYKCyMhDOBTFY7r+i0KvKy4TskbXLFTmjOAD1qhVwJsXhX6ruDjrI4eZtS2GCWuWeghL7WZopnOXy2XoISzVYsnPK7MWqxvnLa2XlU4JoVpcXoBEcGSdTqdhujtDsDpvGeyswlAgZQlJfmuzzPFSLYPERBH4bIhFQpJNTFR+NgsLtV8hg3PcfQUrdVgSI7aUAQoYY0i5EFACoB7AOkJMs5JI8oRRE2mE/HgegI3oYjqXMHPmTENZhZcB8DAaUaanp0MURW6mG4H/mOVsORJ7TmFhIQjhZ+cR+LsNlKOr3q2oqAilpaUoLy83RNTNa3Wr1wRXAJAC4Cxj30AmjO6/yRkSS3pCAnxxcSrWdE0wmOUVn3sDdAdns6CKAXsVdHuDfgiC0IqfBzqCD3B0rYf0AK+vrze8PqG8VEjp8BnU9oyMDENJNwIAdg90ICoqSjct/CD8qdzyQl6Xy4WDBw/qjn0y8K+0PtK/J0+e5EgocSrwL90FXaK94sEB4Cr4qZfkiI6OxqWXXmqcIZ46r0bOc7B8oUBZrl5QUGCzwxOiYzYmJBBvXJy5H7TqKjSBkFxyvQBL8wrFvWnCtatH+WQaVrq+GplXAAPFVWgl0cD0vUSNy2N3HzZsWI8WIPsAc8W01Lim0sIZ87aSDs+L66kIvjnny7CLlCFrqhOADHY6fH09jhtsHtjtoJroDXhQfbFMHX8PpNIbBkWrJWRlITEtTVV0WVhY2K/dK6IoYvXq1bjtttuQl5eHEydOhPQ7QQJkGXrDjeoDUMPZV19fr0u7BAA/523/OW+PHw4A+8Gmi9q/f7+u5TKFt30Kb08XrPYDPMPZLi8K5kHuIjVrYRMEyhdKS1XlCz3uvdBUa2ECPe1r2nqwmtknwYAFMKAsrr7KKrQKq8kdBufV1xbX+++/T1asWEEIIeTgwYPk/vvvV32nv1pcQ4cO1bVcTFlNBo5Z89gZY9OWj1Fi4eAcjBDdMtaMZ2UasVKDx24kUcmKtUbBtrhCAR1fMhtvsmJ59GfISHqRk2MuzhYOcUUKA41kt6KiAllZWQD8xdiHDh0y/RvSW/QGKIuA5W/kPQUBwD1gx7jmzp1rqBN5PNhWU3x8vKbVJOjI6rVEWQagktpeWVlpeM0IQiScBnA3gExqe2ZmpqE1k35DonmSYNTCLoEJcvJuxIWZVShvNc76bKP70Y08hzbYaG5uxhDZy4DT6YTX61Vw7wFAVVWV4nN64F8h8JcAP3PGvffei6NHj+Lee+9FbW0tOjs7cfToUa4s4H8AA8BRagwe5PKS2+oz6jufffYZxo4diyNHjigepvTYIvwP8F2Ui6y6uhppaWkKeVrWB2B74Lvp6enYunUr5s6di6NHj+Ivf/kLbrvtNoXio+V5uX9lZWWoqqrSnfcyAKUbNiAvLw8rV67E2rVrsWHDBtTW1qpY3llrVgkoZLds2YKrr75ad80kE+vORYsU8160aJHmuJIsANU1YQRtbW2q69AUNO2xMIHtKux+9Nm8DKx1b7kKzSLcXYVPPPEE+fvf/x78nJWVpfqO0d52Zt2opl2EDHkf/H2zIHOzScWtTN49amxRFMmUKVMUCQJm2tDn5OQoXIpSHZlmMW1g3qbHpcYuLi4273KT3HUcWSbbCGPNTHWIoOYdKvp9HZcoiigpKcGxY8cQFRWFNWvWYNSoUT07qJQ2L72JGk2jt2EdfWFphVn9WU9hwoQJKCsrw09/+lN4PB5cqWpXbBx94UZ1AEhPT8fw4cNx4MABOByOYDfxyMhI3QQJQRAwa9YsTJkyJaTO1jt27FDUejkcDkN9xATA0riAv6M3IUTlcrMia8QSEsx2iAgT9Lni+uc//4mOjg689dZb8Hg8WLt2LZ5//vneGdxqjGWAPgB7FPaa9Rh+/OMfY8+ePZg/fz4IIXjiiSfM/QAhOFpVhTGhDG419hWQ3wGolMeBAwdw7NgxQ2OzHuLMBpicedNKSrcOKrBmJWPGmBuXMbbplwWZvBVZ00qzJ7MFDaLPFVd3BJRDhm1pXVgY4ErT4XDg8ccf7+tpWIZp5UGhr5Ju+nOyT3+be58rrlADynJYDvT1IMJ1buE6LyB85xau87Jh40JDnyuuIUOGoKWlJfhZFEWV0gKAMWP4DoyqqirN/X2JcJ1buM4LCN+56c2roqKiF2djw8aFC8GfJNJ3eP/991FWVoa1a9fC4/Hgueeew8svv6z4jv1AsNFfMHHixL6egibse8lGf4HWvdTnikvKKjx+/HgwoDx69Oi+nJINGzZs2Ahj9LnismHDhg0bNszgwqR8smHDhg0b/RZ9npxhBnrFym+//TbefPNNREREYMmSJZgxY0avzKuzsxOPPPIITp06hY6ODixZsgQ33HBDcP+rr76KrVu3Ijk5GQDw2GOP4fLLL++VuQHAzTffjLi4OADAJZdcgieffDK4r6/W7J133sG7774LAGhvb0dVVRX27NmD+Ph4AMCaNWtw4MABxMbGAgA2bdoUPIaeRGVlJZ5++mls2bIFJ06cCNLe/OAHP0BxcbEiNbutrQ0PPfQQampqEBsbi6eeeip4jsMZ4XofAeF9L4XjfQSE573U4/eRJd6OXoYW+/W5c+fIjTfeSNrb20ljY2Pw/72BrVu3kjVr1hBCCKmtrVVRxDz44IPk888/75W50GhrayM33XQTc19frpkcJSUl5M0331Rsmz9/PqmpqenVebz00kvkxhtvJPPmzSOEEHLfffeRvXv3EkIIefTRR8kHH3yg+P4rr7xCSktLCSGE/O///i/53e9+16vzDRXheh8REr73Un+4jwgJj3upN+6jfuUq1CpW/uyzzzB+/HhERUUhLi4OaWlpIZE/hoJZs2ahsLAw+FnechwADh8+jJdeegkLFizAiy++2CtzknD06FG0trbirrvuwh133AGPrHNxX66ZhM8//xxffvklbrvttuA2URRx4sQJrF69GvPnz8fWrVt7ZS5paWnYuHFj8PPhw4cxefJkAEB2djY+/vhjxffl12N2djY++eSTXpmnVYTrfQSE770U7vcRED73Um/cR/3KVahVrNzc3Kwwf2NjY9HcS40KJRO8ubkZBQUFKCoqUuz/2c9+httvvx1DhgzB0qVLUVZW1muuhJiYGNx9992YN28evvnmG/zyl7/Ee++91+drJuHFF1/Er371K8W277//HgsXLsTixYvh8/lwxx13YNy4cUhPT+f8Svdg5syZilbrREaDExsbi6amJsX35evH2h+uCNf7SBpPmmM43Uvhfh8B4XMv9cZ91K8sLq1iZXpfS0tLr8REJJw+fRp33HEHbrrpJsyePTu4nRCCRYsWITk5GVFRUcjJycGRI0d6bV6XXXYZ5syZA0EQcNlllyExMRHnz58H0Pdr1tjYiH/961+YOnWqYvugQYNwxx13YNCgQRgyZAimTp3aJ2+wcj98S0tLMGYgQb5+rP3hinC+j4DwvJfC+T4Cwvte6on7qF8prgkTJmDXrl0AoGK/zsjIQEVFBdrb29HU1ISvvvrKEju2GVRXV+Ouu+7CQw89hLlz5yr2NTc348Ybb0RLSwsIISgvL8e4ceN6ZV4AsHXrVqxduxYAcPbsWTQ3N2Po0KEA+nbNAGDfvn247rrrVNu/+eYb3H777fD5fOjs7MSBAwcwduzYXpuXhKuvvhrl5eUAgF27dmHSpEmK/RMmTMDOQH+xXbt2hX3xsYRwvY+A8L2Xwvk+AsL7XuqJ+6hf1XGxipV37dqFtLQ03HDDDXj77bfx1ltvgRCC++67DzNnzuyVea1Zswb/+Mc/FNlN8+bNQ2trK2677Tb87W9/w5YtWxAVFYVp06ahoKCgV+YFAB0dHfjNb36D7777DoIg4Ne//jUqKyv7fM0A4OWXX0ZERFj2Qm4AAAYuSURBVATuvPNOAP6MMWlemzdvxnvvvYfIyEjcdNNNWLBgQa/M6eTJk1i+fDnefvttfP3113j00UfR2dmJyy+/HGvWrIHT6cRdd92FF154AT6fDytWrMD58+cRGRmJZ555JvgwC2eE630EhO+9FM73ERB+91JP30f9SnHZsGHDhg0b/cpVaMOGDRs2bNiKy4YNGzZs9CvYisuGDRs2bPQr2IrLhg0bNmz0K9iKy4YNGzZs9CvYisvGgMHJkydx6623AgCOHTuGffv29eh448aNQ15enuLv7NmzzO++8847+PDDDwEAr7/+uuEx/u///k/1m+Xl5Zg2bRry8vKwcOFCzJ8/H1999VXoB8KBfD1t2Agn9CvKJxs2jOKDDz6Ay+XCtdde22NjJCQkYMuWLYa++/Of/zz4/+effx4LFy40JPfnP/8ZJSUluOiiixTbp06dinXr1gEAPvroI/zhD3/odR5MGzb6CrbisjHgcPbsWbz77ruIjIzE2LFj0dbWhnXr1sHpdGLkyJF4/PHHsX37dpSVlaGtrQ3nz5/HHXfcgQ8//BBffPEFHn74YfzoRz/CypUr8e2336K9vR133303fvrTnxoa/6mnnkJkZCSKioqwePFiLF68GJ9//jlcLhfq6+vR0NCAkpISlJSUBGWOHz+OtWvXQhRFNDY2YtWqVWhsbERVVRVWrFiBN954A1FRUczxGhsbkZqaCgDIy8tDUlISGhsbsXHjRqxatQpNTU2oq6vDvHnzcPvttyMvLw/p6en44osv0NzcjA0bNiA1NRWbNm3CP//5T/h8PixYsADTp09HbW0tHnjgAZw/fx5XXXUV1qxZY/n82LBhFbbisjHgcNFFF+GWW26By+XCNddcg1mzZuGNN95ASkoK1q9fj3fffRcRERFoaWnBK6+8gr///e/405/+hLfffhvl5eX485//jKlTp6K8vBx//etfAQB79uxRjdPQ0IC8vLzg52HDhuGZZ57B8uXL8Ytf/AIrVqxARkYGcnNz8fnnnwMAlixZgtdff12htADgyy+/xIoVK3DVVVdh+/bteOedd7BmzRqMGTMGJSUlKqW1d+9e5OXloaOjA8eOHVNYW7Nnz8aPf/xjHD58GD/72c/wH//xHzh79izy8vJw++23A/DTFP32t7/FunXr8Pe//x3Tp0/Hrl278Je//AUdHR145plncP3116O5uRlPPvkk4uLi8OMf/xg1NTVISUnplvNkw0aosBWXjQGN2tpanDt3Lsgy3tbWhuuvvx5paWkYM2YMACAuLg6jR4+GIAhISEhAe3s7hgwZgkcffRSPPvoompubMWfOHNVv81yFkZGRWLRoEVasWIGysjJD8xw2bBg2bdqEmJgYtLS0KNjbWZC7Cv/1r39h/vz5Qf7Byy67DADgcrnw2muv4YMPPsCQIUPg9XqD8ldffTUAYPjw4aiursbXX3+NjIwMOJ1ODBo0CKtWrcLJkycxcuRIJCQkAABSUlLQ2tpq6Hhs2OhJ2MkZNgYkBEGAKIpISkrC8OHDsWnTJmzZsgX3338/pkyZEvwOD+fOncPhw4fxxz/+ES+99BL+67/+S/Hg10JDQwNeeOEFrFy5Eo8++qhqP4tl7fe//z0KCgrw1FNP4corrwx+RxAE5vflcLlcis/Scb3yyitwu914+umnMWvWLM3fufzyy3HkyBGIoojOzk4sXrwYHR0dmmtkw0Zfwba4bAxIjBs3Dn/4wx8wevRo/Pa3v8W9994LQghiY2Pxhz/8AadPn9aUHzp0KM6fP4+bb74ZgwcPxl133RVs/SGBdhUCwPLly/Hf//3fuOeee3DTTTfh0KFD+POf/6z4zujRo/HrX/8aTz/9dHDbnDlz8MADDyAlJQXDhw9HXV0dAGD8+PF4+OGH8corryAxMTH4fclV6HA40NLSgpUrVyImJkYxzowZM1BSUoLt27cjMTERTqcTHR0dzOMdM2YMsrKysGDBAoiiiAULFnBjajZs9DVskl0bNmzYsNGvYLsKbdiwYcNGv4KtuGzYsGHDRr+Crbhs2LBhw0a/gq24bNiwYcNGv4KtuGzYsGHDRr+Crbhs2LBhw0a/gq24bNiwYcNGv8L/D2CmpqPu2WnnAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 4 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Branch Name</th>\n",
       "      <th>Predictions</th>\n",
       "      <th>test_accuracy</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>E_Threshold</th>\n",
       "      <th>acceptance_rate</th>\n",
       "      <th>accepted_correct</th>\n",
       "      <th>accepted_incorrect</th>\n",
       "      <th>accepted_accuracy</th>\n",
       "      <th>overlap_adjusted_accuracy</th>\n",
       "      <th>M(T) B(F)</th>\n",
       "      <th>M(F) B(T)</th>\n",
       "      <th>M(F) B(F) overlap</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>branch_1</td>\n",
       "      <td>10000</td>\n",
       "      <td>0.4424</td>\n",
       "      <td>0.442400</td>\n",
       "      <td>1.929964</td>\n",
       "      <td>0.264400</td>\n",
       "      <td>1745</td>\n",
       "      <td>899</td>\n",
       "      <td>0.659985</td>\n",
       "      <td>0.686838</td>\n",
       "      <td>828</td>\n",
       "      <td>32</td>\n",
       "      <td>71</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>branch_2</td>\n",
       "      <td>7356</td>\n",
       "      <td>0.4920</td>\n",
       "      <td>0.423192</td>\n",
       "      <td>1.873610</td>\n",
       "      <td>0.233687</td>\n",
       "      <td>1029</td>\n",
       "      <td>690</td>\n",
       "      <td>0.598604</td>\n",
       "      <td>0.630017</td>\n",
       "      <td>636</td>\n",
       "      <td>27</td>\n",
       "      <td>54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>branch_3</td>\n",
       "      <td>5637</td>\n",
       "      <td>0.6350</td>\n",
       "      <td>0.558453</td>\n",
       "      <td>1.499394</td>\n",
       "      <td>0.338123</td>\n",
       "      <td>1488</td>\n",
       "      <td>418</td>\n",
       "      <td>0.780693</td>\n",
       "      <td>0.808499</td>\n",
       "      <td>365</td>\n",
       "      <td>26</td>\n",
       "      <td>53</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Main_Exit</td>\n",
       "      <td>3731</td>\n",
       "      <td>0.9427</td>\n",
       "      <td>0.916912</td>\n",
       "      <td>0.420736</td>\n",
       "      <td>0.776199</td>\n",
       "      <td>2840</td>\n",
       "      <td>56</td>\n",
       "      <td>0.980663</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>56</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Branch Name  Predictions  test_accuracy  Accuracy  E_Threshold  \\\n",
       "0    branch_1        10000         0.4424  0.442400     1.929964   \n",
       "1    branch_2         7356         0.4920  0.423192     1.873610   \n",
       "2    branch_3         5637         0.6350  0.558453     1.499394   \n",
       "3   Main_Exit         3731         0.9427  0.916912     0.420736   \n",
       "\n",
       "   acceptance_rate  accepted_correct  accepted_incorrect  accepted_accuracy  \\\n",
       "0         0.264400              1745                 899           0.659985   \n",
       "1         0.233687              1029                 690           0.598604   \n",
       "2         0.338123              1488                 418           0.780693   \n",
       "3         0.776199              2840                  56           0.980663   \n",
       "\n",
       "   overlap_adjusted_accuracy  M(T) B(F)  M(F) B(T)  M(F) B(F) overlap  \n",
       "0                   0.686838        828         32                 71  \n",
       "1                   0.630017        636         27                 54  \n",
       "2                   0.808499        365         26                 53  \n",
       "3                   1.000000          0          0                 56  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Experiencing connection interruptions. Will try to reestablish communication with Neptune.\n",
      "Experiencing connection interruptions. Will try to reestablish communication with Neptune.\n",
      "Communication with Neptune restored!\n",
      "Communication with Neptune restored!\n"
     ]
    }
   ],
   "source": [
    "display(displayEvidence_cascade(test_Outputs, Evidence = False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''     overlap\n",
    "        if zero, both match, if else they don't match\n",
    "        TT 1-1 =0\n",
    "        TF 1-0 =1\n",
    "\n",
    "        FT 0-1 = -1\n",
    "        FF 0-0 =0\n",
    "        \n",
    "        '''\n",
    "# print(Outputs[1])\n",
    "displayEvidence(Outputs, Evidence = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 227,227 sigmoid evidence cifar10 v2\n",
    "# print(Outputs[0])\n",
    "\n",
    "displayEvidence(Outputs, Evidence = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 227,227 sigmoid evidence cifar10\n",
    "displayEvidence(Predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 227,227 sigmoid cifar10\n",
    "displayEvidence(Predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 32,32 crossEvidence cifar10\n",
    "displayEvidence(Predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "displayEvidence(Predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "displayEvidence(Predictions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_2 = tf.keras.models.load_model('alexNetv6_evidence_test.hdf5',\n",
    "    custom_objects={\"CrossEntropyEndpoint\":CrossEntropyEndpoint,\"crossEntropy_loss\":loss_function()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Predictions_2 = collectEvidence(model_2,test_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "displayEvidence(Predictions_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collectEntropy(model,test_ds):\n",
    "    num_outputs = len(model.outputs) # the number of output layers for the purpose of providing labels\n",
    "#     train_ds, test_ds, validation_ds = (dataset)\n",
    "    predictions = []\n",
    "    labels = []\n",
    "    iterator = iter(test_ds)\n",
    "    print(len(test_ds))\n",
    "    item = iterator.get_next()\n",
    "#     print(item)\n",
    "\n",
    "    pClass = []\n",
    "    predictions=[]\n",
    "    pEvidence = []\n",
    "    pUncertainty=[]\n",
    "    Outputs = pd.DataFrame()\n",
    "    output_names=[\"mainExit\"]\n",
    "    pAcc=[]\n",
    "    for i, (x,y) in enumerate(test_ds):\n",
    "#     for i in range(100):\n",
    "        print(\"prediction: {} of {}\".format(i,len(test_ds)),end='\\r')\n",
    "        result = model.predict(x)\n",
    "        pClass.append(tf.argmax(y,1).numpy()[0])\n",
    "        pred= (tf.nn.softmax(result)[0])\n",
    "\n",
    "        pEvidence.append(calcEntropy_Tensors(pred).numpy())\n",
    "        if np.argmax(pred) == np.argmax(y):\n",
    "            pAcc.append(1)       \n",
    "        else:\n",
    "            pAcc.append(0)\n",
    "    Predictions = pd.DataFrame({\"label\":pClass,\"evidence\":pEvidence,\"Acc\":pAcc,\"overlap\":0})\n",
    "    return Predictions\n",
    "\n",
    "def displayEntropy(Predictions):\n",
    "    output_names=[\"mainExit\"]\n",
    "    Outputs=pd.DataFrame()\n",
    "    Predictions[\"Acc\"]=Predictions[\"Acc\"].astype('bool')\n",
    "    acc = Predictions[\"Acc\"].value_counts()\n",
    "    print(acc)\n",
    "    print((acc.loc[True] , acc.loc[False]))\n",
    "    mean = Predictions.loc[(Predictions['Acc'] == False)].groupby(\"Acc\")[\"evidence\"].mean().iloc[0]\n",
    "    std = Predictions.loc[(Predictions['Acc'] == False)].groupby(\"Acc\")[\"evidence\"].std().iloc[0]\n",
    "    E_threshold = mean - std\n",
    "    correct_rows = Predictions.loc[Predictions['Acc'] == True]\n",
    "    incorrect_rows = Predictions.loc[Predictions['Acc'] == False]\n",
    "    # Incorrects_overlap = Predictions.loc[(Predictions['Acc'] == False)  & (Predictions[\"evidence\"] > E_threshold) & (Predictions[\"overlap\"] == 0)].count().iloc[0]\n",
    "    print(acc)\n",
    "    for i,name in enumerate(output_names):\n",
    "        Outputs = Outputs.append(pd.DataFrame({\"Branch Name\":output_names[i],\n",
    "                \"Accuracy\":(acc.loc[True] /  (acc.loc[True] + acc.loc[False])),\n",
    "                \"E_Threshold\":E_threshold,\n",
    "                # \"Overlap_Threshold\":non_overlapping_incorrects_threshold,\n",
    "                \"acceptance_rate\":Predictions.loc[(Predictions[\"evidence\"] < E_threshold)].sort_values(\"evidence\").shape[0]/(Predictions.count().iloc[0]),\n",
    "                \"accepted_correct\":Predictions.loc[(Predictions['Acc'] == True)  & (Predictions[\"evidence\"] < E_threshold)].sort_values(\"evidence\").shape[0],\n",
    "                \"accepted_incorrect\":Predictions.loc[(Predictions['Acc'] == False)  & (Predictions[\"evidence\"] < E_threshold)].sort_values(\"evidence\").shape[0],\n",
    "                \"accepted_accuracy\":(Predictions.loc[(Predictions['Acc'] == True)  & (Predictions[\"evidence\"] < E_threshold)].sort_values(\"evidence\").shape[0])/ Predictions.loc[(Predictions[\"evidence\"] < E_threshold)].count()[0],\n",
    "                # \"overlap_adjusted_accuracy\":(Predictions.loc[(Predictions['Acc'] == True)  & (Predictions[\"evidence\"] > E_threshold)].sort_values(\"evidence\").shape[0] + Incorrects_overlap) / Predictions.loc[(Predictions[\"evidence\"] > E_threshold)].count()[0],\n",
    "                \"rejected_correct\":Predictions.loc[(Predictions['Acc'] == True)  & (Predictions[\"evidence\"] > E_threshold)].sort_values(\"evidence\").shape[0],\n",
    "                \"rejected_incorrect\":Predictions.loc[(Predictions['Acc'] == False)  & (Predictions[\"evidence\"] > E_threshold)].sort_values(\"evidence\").shape[0],\n",
    "                # \"Incorrects_overlap\":Incorrects_overlap,\n",
    "                },index=[i]))\n",
    "\n",
    "    # fig, axs = plt.subplots(1, 2)\n",
    "    plt.suptitle('Horizontally stacked subplots')\n",
    "    plt.scatter(correct_rows['label'],correct_rows['evidence'],c ='r',marker='+')\n",
    "    plt.scatter(incorrect_rows['label']+.3,incorrect_rows['evidence'],c ='k',marker='x')\n",
    "    plt.plot(np.repeat(E_threshold,11),'b--')\n",
    "    plt.title(\"evidence\")\n",
    "\n",
    "\n",
    "    # fig.set_size_inches(10, 10)\n",
    "    plt.show()\n",
    "    return Outputs\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entropy_predictions = collectEntropy(model_2,test_ds)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "displayEntropy(Entropy_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "(train_images, train_labels), (test_images, test_labels) = tf.keras.datasets.cifar10.load_data()\n",
    "test_images = test_images.reshape(10000, 32,32,3).astype(\"float32\") / 255\n",
    "\n",
    "# print(y_train)\n",
    "K= 10 # number of classes\n",
    "test_labels = tf.keras.utils.to_categorical(test_labels,10)\n",
    "shuffle_size = 22500\n",
    "batch_size=1\n",
    "test_ds = tf.data.Dataset.from_tensor_slices((test_images, test_labels))\n",
    "\n",
    "def augment_images(image, label,input_size=(227,227), channel_first = False):\n",
    "            image = tf.image.resize(image,input_size)\n",
    "            if channel_first:\n",
    "                image = tf.transpose(image, [2, 0, 1])\n",
    "            return image, label\n",
    "test_ds_size = len(list(test_ds))\n",
    "# test_ds = (test_ds.map(augment_images))\n",
    "t_target = tf.data.Dataset.from_tensor_slices((test_labels))\n",
    "test_ds = tf.data.Dataset.zip((test_ds,t_target))\n",
    "test_ds = (test_ds.batch(batch_size=1, drop_remainder=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Predictions_mse = collectEvidence(model,test_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "displayEvidence(Predictions_mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#sum entropy \n",
    "import pandas as pd\n",
    "def entropy(x):\n",
    "    return -(x * math.log(x))\n",
    "# Data for plotting\n",
    "t = np.arange(0.00001, 1, 0.01)\n",
    "print(t.shape)\n",
    "t_ = np.full((100,), .1)\n",
    "df = pd.DataFrame([t,t,t_,t,t])\n",
    "# print(df.transpose())\n",
    "p = df.apply(calcEntropy,axis=0)\n",
    "# print(p)\n",
    "# print(p)\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(t, p)\n",
    "ax.set(xlabel='Probability of Outcome',ylabel='Entropy of event')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = [0]\n",
    "y_pred = [.99,.01, .01, .0, .01, .01]\n",
    "# Using 'auto'/'sum_over_batch_size' reduction type.  \n",
    "scce = tf.keras.losses.SparseCategoricalCrossentropy()\n",
    "crossE = scce(y_true, y_pred).numpy()\n",
    "print(\"crossE: \",crossE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = [0,1,0,0,0,0]\n",
    "y_pred = [.99,.01, .01, .0, .01, .01]\n",
    "# Using 'auto'/'sum_over_batch_size' reduction type.  \n",
    "scce = tf.keras.losses.CategoricalCrossentropy()\n",
    "crossE = scce(y_true, y_pred).numpy()\n",
    "print(\"crossE: \",crossE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = [0]\n",
    "y_pred = [.0,.01, .9, .0, .0, .0]\n",
    "ent = calcEntropy(y_pred)\n",
    "print(\"Entropy: \",ent)\n",
    "loss = ent *1\n",
    "print(\"Loss: \",loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = [[0],[0],[0]]\n",
    "y_pred = [[.9,.5, .6, .5, .5, .5],[.8,.5, .6, .5, .5, .5],[.7,.5, .6, .5, .5, .5]]\n",
    "y_pred = [[.9,.0,.0,.0,.0,.0,],[.8,.5, .6, .5, .5, .5],[.7,.5, .6, .5, .5, .5]]\n",
    "# y_pred = [.1,.1, .1, .1, .1, .1]\n",
    "# Using 'auto'/'sum_over_batch_size' reduction type.  \n",
    "scce = tf.keras.losses.SparseCategoricalCrossentropy()\n",
    "crossE = scce(y_true, y_pred).numpy()\n",
    "print(\"crossE: \",crossE)\n",
    "\n",
    "''' When the answer is correct, CrossE goes down\n",
    "    When \n",
    "    When its wrong, Entropy High\n",
    "    When its right, Entropy Low\n",
    "    \n",
    "    so penalize being right with low entropy and reward being right with high entropy\n",
    "    \n",
    "    \n",
    "    OORRRR train a second model for a branch to determine if you are going to get it right or not?\n",
    "    Isn't that what ResNet Did? you calculate if the blocks will contribute, was it block drop?\n",
    "    Binary classification,\n",
    "    could be done at the branch end as a separate evaulator, using the entropy score and the input to the branch as inputs?\n",
    "'''\n",
    "\n",
    "\n",
    "ent = calcEntropy(y_pred)\n",
    "print(\"Entropy: \",ent)\n",
    "loss = crossE + ent\n",
    "print(\"combined Loss: \",loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = [[.1,.1, .675, .1091, .4311, .1875,.121,.143,.2,.5]]\n",
    "x = tf.constant([[2, 20, 30, 3, 6], [3, 11, 16, 1, 8],\n",
    "                 [14, 45, 23, 5, 27]])\n",
    "\n",
    "print(list(map(np.argmax,np.array(x))))\n",
    "def foo(y_pred):\n",
    "    y_pred = y_pred.numpy()\n",
    "    pred_label = list(map(np.argmax,np.array(y_pred)))\n",
    "    return pred_label\n",
    "%timeit foo(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "y_true = tf.constant([[2],[2],[0]])\n",
    "A = tf.constant([.1,.1, .675, .1091, .4311, .1875,.121,.143,.2,.5])\n",
    "B = tf.constant([[2, 20, 30, 3, 6], [3, 11, 16, 1, 8],\n",
    "                 [14, 45, 23, 5, 27]])\n",
    "\n",
    "y_pred = tf.constant([[.9,.5, .6, .5, .5, .5],[.8,.5, .6, .5, .5, .5],[.7,.5, .6, .5, .5, .5]])\n",
    "\n",
    "# new_list = new_list = [list(range(10)) for _ in range(10)]\n",
    "\n",
    "print(tf.math.argmax(y_pred,1))\n",
    "pred_labels = tf.math.argmax(y_pred,1)\n",
    "print(tf.reshape(y_true,pred_labels.shape))\n",
    "indexes = tf.where(tf.math.equal(pred_labels, tf.cast(tf.reshape(y_true,pred_labels.shape),'int64')))\n",
    "indexes = tf.reshape(indexes,[-1])\n",
    "# print(tf.gather(B,indexes))\n",
    "CorrectE = tf.gather(y_pred,indexes)\n",
    "print(CorrectE)\n",
    "# print(calcEntropy(CorrectE[0]))\n",
    "\n",
    "\n",
    "results = tf.map_fn(calcEntropy,tf.cast(CorrectE,'float'))\n",
    "print(\"results: \",results)\n",
    "\n",
    "\n",
    "\n",
    "%timeit tf.map_fn(calcEntropy,tf.cast(CorrectE,'float'))\n",
    "\n",
    "\n",
    "crossE = tf.keras.losses.SparseCategoricalCrossentropy()\n",
    "crossE(y_true,y_pred)\n",
    "%timeit crossE(y_true,y_pred)\n",
    "# [\n",
    "#     [\n",
    "#         [ 2 20 30  3  6]\n",
    "#     ]\n",
    "#     [\n",
    "#         [ 3 11 16  1  8]\n",
    "#     ]\n",
    "# ]\n",
    "\n",
    "def entropyAddition_noCross(y_true, y_pred):\n",
    "    crossE = tf.keras.losses.SparseCategoricalCrossentropy()\n",
    "    #Entropy is added to the CrossE divided by the len of inputs\n",
    "    pred_labels = tf.math.argmax(y_pred,1)\n",
    "    indexes = tf.where(tf.math.equal(pred_labels, tf.cast(tf.reshape(y_true,pred_labels.shape),'int64')))\n",
    "    indexes = tf.reshape(indexes,[-1])\n",
    "    entropies = tf.gather(y_pred,indexes)\n",
    "    if tf.equal(tf.size(entropies), 0):\n",
    "        correctEntropies = 0\n",
    "    else:\n",
    "        correctEntropies = tf.reduce_mean(tf.map_fn(calcEntropy_Tensors,tf.cast(entropies,'float')))\n",
    "    scce = crossE(y_true, y_pred)\n",
    "    loss = correctEntropies\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = tf.constant([[1],[2],[0]])\n",
    "y_pred = tf.constant([[0,0, .6, .5, .5, .5],[.8,.5, .6, .5, .5, .5],[.9,.3, .3, .2, .1, .32]])\n",
    "crossE = tf.keras.losses.SparseCategoricalCrossentropy()\n",
    "def entropyAddition(y_true, y_pred):\n",
    "    crossE = tf.keras.losses.SparseCategoricalCrossentropy()\n",
    "    #Entropy is added to the CrossE divided by the len of inputs\n",
    "    pred_labels = tf.math.argmax(y_pred,1)\n",
    "    indexes = tf.where(tf.math.equal(pred_labels, tf.cast(tf.reshape(y_true,pred_labels.shape),'int64')))\n",
    "    indexes = tf.reshape(indexes,[-1])\n",
    "    entropies = tf.gather(y_pred,indexes)\n",
    "    if tf.equal(tf.size(entropies), 0):\n",
    "        correctEntropies = 0\n",
    "    else:\n",
    "        correctEntropies = tf.reduce_mean(tf.map_fn(calcEntropy_Tensors,tf.cast(entropies,'float')))\n",
    "    scce = crossE(y_true, y_pred)\n",
    "    loss = scce + (correctEntropies * scce)\n",
    "    return loss\n",
    "\n",
    "\n",
    "print(\"normal CrossE: \",crossE(y_true ,y_pred))\n",
    "\n",
    "print(\"normal Entropy\",entropyAddition_noCross(y_true2,y_pred2))\n",
    "\n",
    "print(entropyAddition(y_true2, y_pred2))\n",
    "# %timeit entropyAddition(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = tf.constant([1])\n",
    "y_pred = tf.constant([0,1, 0, 0, 0, 0])\n",
    "# print(crossE(y_true,y_pred))\n",
    "\n",
    "print(tf.cast(1e-8,'float')+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = tf.constant([[1],[2],[0]])\n",
    "y_pred = tf.constant([[.9,.1, .6, .5, .5, .5],[.8,.5, .6, .5, .5, .5],[.9,.3, .3, .2, .1, .32]])\n",
    "crossE = tf.keras.losses.SparseCategoricalCrossentropy()\n",
    "def entropyMultiplication(y_true, y_pred):\n",
    "    crossE = tf.keras.losses.SparseCategoricalCrossentropy()\n",
    "    #Entropy is added to the CrossE divided by the len of inputs\n",
    "    pred_labels = tf.math.argmax(y_pred,1)\n",
    "    indexes = tf.where(tf.math.equal(pred_labels, tf.cast(tf.reshape(y_true,pred_labels.shape),'int64')))\n",
    "    indexes = tf.reshape(indexes,[-1])\n",
    "    entropies = tf.gather(y_pred,indexes)\n",
    "    if tf.equal(tf.size(entropies), 0):\n",
    "        correctEntropies = 1\n",
    "    else:\n",
    "        correctEntropies = tf.reduce_mean(tf.map_fn(calcEntropy_Tensors,tf.cast(entropies,'float')))\n",
    "    scce = crossE(y_true, y_pred)\n",
    "    loss = correctEntropies * scce\n",
    "    return loss\n",
    "\n",
    "\n",
    "print(\"normal CrossE: \",crossE(y_true,y_pred))\n",
    "print(entropyAddition(y_true, y_pred))\n",
    "# %timeit entropyAddition(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = tf.constant([[1],[2],[1]])\n",
    "y_pred = tf.constant([[.9,.1, .6, .5, .5, .5],[.8,.5, .6, .5, .5, .5],[.9,.3, .3, .2, .1, .32]])\n",
    "\n",
    "def confidenceScore(y_true, y_pred):\n",
    "        # print(y_pred)\n",
    "        # print(tf.keras.backend.get_value(y_pred))\n",
    "        \n",
    "        # y_true =y_true.numpy()\n",
    "        # y_pred = y_pred.numpy()\n",
    "        # AvgConfidence = -1\n",
    "        pred_labels = tf.math.argmax(y_pred,1)\n",
    "        # countCorrect=0\n",
    "        indexes = tf.where(tf.math.equal(pred_labels, tf.cast(tf.reshape(y_true,pred_labels.shape),'int64')))\n",
    "        indexes = tf.reshape(indexes,[-1])\n",
    "        entropies = tf.gather(y_pred,indexes)\n",
    "        if tf.equal(tf.size(entropies), 0):\n",
    "            correctEntropies = 0\n",
    "        else:\n",
    "            correctEntropies = tf.reduce_mean(tf.map_fn(calcEntropy,tf.cast(entropies,'float')))    \n",
    "        \n",
    "        return correctEntropies\n",
    "    \n",
    "print(confidenceScore(y_true,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = [[2],[2],[0]]\n",
    "y_pred = [[.9,0, .6, .5, .5, .5],[.8,.5, .6, .5, .5, .5],[.7,.5, .6, .5, .5, .5]]\n",
    "\n",
    "def foo(x, y):\n",
    "    scce = tf.keras.losses.SparseCategoricalCrossentropy()\n",
    "    crossE = scce(x, y).numpy()\n",
    "    return crossE\n",
    "\n",
    "print(foo(y_true,y_pred))\n",
    "%timeit foo(y_true,y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Python program explaining \n",
    "# where() function \n",
    "  \n",
    "import numpy as np\n",
    "  \n",
    "# a is an array of integers.\n",
    "a = np.array([[1, 2, 3], [4, 5, 6]])\n",
    "  \n",
    "print(a)\n",
    "  \n",
    "print ('Indices of elements <4')\n",
    "  \n",
    "b = np.where(a<5)\n",
    "print(b)\n",
    "  \n",
    "print(\"Elements which are <4\")\n",
    "print(a[b])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = [[0],[0],[0]]\n",
    "y_pred = [[.5,.5, .6, .5, .5, .1],[.5,.5, .6, .5, .5, .2],[.5,.5, .6, .5, .5, .3]]\n",
    "# y_pred = [[1],[1],[1]]\n",
    "# print(np.array(y_pred))\n",
    "\n",
    "####\n",
    "# Numpy confidence metric version\n",
    "y_true =np.array(y_true)\n",
    "y_pred = np.array(y_pred)\n",
    "def argmax(x):\n",
    "    return [np.argmax(x)]\n",
    "pred_labels = list(map(argmax,np.array(y_pred)))\n",
    "x = np.where(np.equal(y_true,pred_labels) ==True)\n",
    "y = y_pred[x[0]]\n",
    "results = calcEntropy(y)\n",
    "print(results)\n",
    "if not (results):\n",
    "    print(\"A\")\n",
    "print(np.median(results))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = [[2],[2],[0]]\n",
    "y_pred = [[.5,.5, .6, .5, .5, .5],[.5,.5, .6, .5, .5, .5],[.5,.5, .6, .5, .5, .5]]\n",
    "\n",
    "y_true = [[2]]\n",
    "y_pred = [[.1,.1, .15, .1, .1, .1]]\n",
    "def entropyAddition_loss():\n",
    "    #create a wrapper function that returns a function\n",
    "    crossE = tf.keras.losses.SparseCategoricalCrossentropy()\n",
    "    def entropyAddition(y_true, y_pred):\n",
    "        #Entropy is added to the CrossE divided by the len of inputs\n",
    "        pred_labels = tf.math.argmax(y_pred,1)\n",
    "        indexes = tf.where(tf.math.equal(pred_labels, tf.cast(tf.reshape(y_true,pred_labels.shape),'int64')))\n",
    "        indexes = tf.reshape(indexes,[-1])\n",
    "        entropies = tf.gather(y_pred,indexes)\n",
    "        correctEntropies = tf.reduce_mean(tf.map_fn(calcEntropy,tf.cast(entropies,'float')))\n",
    "#         print(pred_label)\n",
    "        scce = crossE(y_true, y_pred)\n",
    "        sumEntropy = 0\n",
    "        loss = correctEntropies + scce\n",
    "        return loss\n",
    "    \n",
    "    return entropyAddition\n",
    "\n",
    "def custom_loss_multi(y_true, y_pred):\n",
    "    #CrossE is multiplied by the Entropy\n",
    "    pred_label = list(map(np.argmax,np.array(y_pred)))\n",
    "    crossE = tf.keras.losses.SparseCategoricalCrossentropy()\n",
    "    sumLoss = 0\n",
    "    \n",
    "    for i in range(len(y_pred)):\n",
    "        loss = crossE(y_true[i], y_pred[i])\n",
    "#         print('crossE: ',loss)\n",
    "        if pred_label[i] == y_true[i]:\n",
    "#             print('calcEntropy ',calcEntropy(y_pred[i]))\n",
    "            loss = loss * calcEntropy(y_pred[i])\n",
    "        sumLoss += loss\n",
    "    sumLoss = sumLoss / len(y_pred)         \n",
    "    \n",
    "#     loss = crossE(y_true, y_pred)\n",
    "#     print(\"CrossE : \",loss.numpy())\n",
    "#     print(\"Loss : \",sumLoss)\n",
    "    return sumLoss\n",
    "    ### I want to reduce the entropy of correct answers\n",
    "    ### if label - pred = 0 (aka correct) then add entropy to crossE\n",
    "    \n",
    "    \n",
    "#     squared_difference = tf.square(np.array(y_true) - np.array(y_pred))\n",
    "#     return tf.reduce_mean(squared_difference, axis=-1)  # Note the `axis=-1`\n",
    "\n",
    "scce = tf.keras.losses.SparseCategoricalCrossentropy()\n",
    "crossE = scce(y_true, y_pred).numpy()\n",
    "print(\"crossEntropyLoss: \",crossE)\n",
    "\n",
    "\n",
    "crossE = custom_loss_addition(y_true, y_pred).numpy()\n",
    "print(\"customLoss_addition: \",crossE)\n",
    "\n",
    "\n",
    "crossE = custom_loss_multi(y_true, y_pred).numpy()\n",
    "print(\"customLoss_multi: \",crossE)\n",
    "\n",
    "  \n",
    "# model.compile(loss=custom_loss, optimizer='adam')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def sub(x,y):\n",
    "    if x - y == 0:\n",
    "        return 1\n",
    "%timeit sub(5,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def sub(x,y):\n",
    "    if x == y:\n",
    "        return 0\n",
    "    \n",
    "%timeit sub(5,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "7483e188a462fd4248cd8d23b24bc727a5fe7a35e4044aa57ebcc92f8fe9e445"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
