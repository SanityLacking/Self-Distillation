# Self Distillation
<hr>
Knowledge distillation within a branching model. <br>
The main exit acts as a teacher for the branch exits during branch training, improving the branch accuracy <br>
